---
title: "Data Analysis"
author: "Emma Jones"
date: "12/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# R version 3.6.1 "Action of the Toes"
suppressPackageStartupMessages(library(tidyverse))#1.3.0
suppressPackageStartupMessages(library(sf))#0.8-0
suppressPackageStartupMessages(library(spsurvey))#4.1.1
suppressPackageStartupMessages(library(DT))#0.11
suppressPackageStartupMessages(library(purrr))#0.3.3
library(magrittr)
```


This document walks you through the steps of calculating design weights, running analyses, and writing the Integrated Report chapter for the Probabilistic Monitoring Program due to EPA every two years. This document overviews the 2022 IR ProbMon chapter.


# Design Status

Starting with an organized 'design status' database that has all Probmon stations (sampled or not) organized with stream order information. This data is important to note all sites that were evaluated and whether or not they should be included in the design weight process. Environmental, chemical, landcover, etc. data is organized in a separate spreadsheet (brought in later) that is joined to the weight category results. 

Bring in appropriate datasets. Always start with VSCI (VSCI/VCPMI) first because this parameter will have the best information on whether or not to include the station for design weight calculations. 

```{r}
#### Read in master field data sampled at all sites
# Select the parameters we want to run
# rename a few parameters to work better in R
#allData <- read_csv('processedData/Wadeable_ProbMon_2001-2018_Final_Final.csv')

```


Run only parameters we want at present and get ready for CDF

```{r}
# Select the parameters we want to run
# rename a few parameters to work better in R
allData <- read_csv('processedData/Wadeable_ProbMon_2001-2020.csv')
surveyData <- select(allData, StationID_Trend,Year,DO:MetalCCU,CALCIUM:MERCURY,wshdImpPCT) %>% 
  dplyr::rename(siteID=StationID_Trend,Ortho_P= "Ortho.P")

```


## Metals Exceedances



Now time to find any exceedances of metals criteria for acute or chronic.

```{r look for metals exceedances}
# Capitalize first letter in a word easily
capwords <- function(s, strict = FALSE) {
  cap <- function(s) paste(toupper(substring(s, 1, 1)),
                           {s <- substring(s, 2); if(strict) tolower(s) else s},
                           sep = "", collapse = " " )
  sapply(strsplit(s, split = " "), cap, USE.NAMES = !is.null(names(s)))
}

metalsData <- select(surveyData, siteID,Year, CALCIUM:MERCURY)
names(metalsData)[3:22] <- capwords(tolower(names(metalsData)[3:22]))

metalsCriteriaAssessment <- function(df){
  df1 <- gather(df, Metal, Measure, Calcium:Hardness) 
  
  criteriaHardness <- ifelse(df$Hardness<25,25,ifelse(df$Hardness>400,400,df$Hardness))
  x <- data.frame(Calcium_Chronic=NA,Calcium_Acute=NA,Calcium_PWS=NA,
                  Magnesium_Chronic=NA, Magnesium_Acute=NA, Magnesium_PWS=NA,
                  Arsenic_Chronic=150,Arsenic_Acute=340,Arsenic_PWS=10,
                  Barium_Chronic=NA,Barium_Acute=NA,Barium_PWS=2000,
                  Beryllium_Chronic=NA,Beryllium_Acute=NA,Beryllium_PWS=NA,
                  Cadmium_Chronic=format((exp(0.7852*(log(criteriaHardness))-3.49)),digits=3), 
                  Cadmium_Acute= format((exp(1.128*(log(criteriaHardness))-3.828)),digits=3), Cadmium_PWS=5,
                  Chromium_Chronic=format(((exp(0.819*(log(criteriaHardness))+0.6848))*0.86),digits=3),
                  Chromium_Acute=format(((exp(0.819*(log(criteriaHardness))+3.7256))*0.316),digits=3), Chromium_PWS=100,
                  Copper_Chronic=format(((exp(0.8545*(log(criteriaHardness))-1.702))*0.96),digits=3),
                  Copper_Acute=format(((exp(0.9422*(log(criteriaHardness))-1.70))*0.96),digits=3), Copper_PWS=1300,
                  Iron_Chronic=NA, Iron_Acute=NA, Iron_PWS= 300,
                  Lead_Chronic=format(((exp(1.273*(log(criteriaHardness))-3.259))),digits=3),
                  Lead_Acute=format(((exp(1.273*(log(criteriaHardness))-1.084))),digits=3), Lead_PWS=15,
                  Manganese_Chronic=NA, Manganese_Acute=NA, Manganese_PWS=50,
                  Thallium_Chronic=NA, Thallium_Acute=NA, Thallium_PWS=0.24,
                  Nickel_Chronic=format(((exp(0.846*(log(criteriaHardness))-0.884))*0.997),digits=3),
                  Nickel_Acute=format(((exp(0.846*(log(criteriaHardness))+1.312))*0.998),digits=3),Nickel_PWS=610,
                  Silver_Chronic=NA, Silver_Acute=format(((exp(1.72*(log(criteriaHardness))-6.52))*0.85),digits=3), Silver_PWS=NA,
                  Zinc_Chronic=format(((exp(0.8473*(log(criteriaHardness))+0.884))*0.986),digits=3),
                  Zinc_Acute=format(((exp(0.8473*(log(criteriaHardness))+0.884))*0.978),digits=3), Zinc_PWS=7400,
                  Antimony_Chronic=NA, Antimony_Acute=NA, Antimony_PWS=5.6,
                  Aluminum_Chronic=NA,Aluminum_Acute=NA,Aluminum_PWS=NA,
                  Selenium_Chronic=5, Selenium_Acute= 20, Selenium_PWS=170,
                  Hardness_Chronic=NA,Hardness_Acute=NA,Hardness_PWS=NA)
  x1 <- as.data.frame(t(x)) %>%
    rownames_to_column('original') %>%
    rowwise()%>%  
    mutate(Metal=gsub("_.*","\\1",original),Criteria=gsub(".*_","\\1",original)) %>%
                            group_by(Criteria) %>%
                            mutate(id = row_number()) %>%
                            select(-original) %>%
                            spread(Criteria, V1) %>%
                            select(-id) %>%
    full_join(df1, by= 'Metal') 
  x1[,c(2:4)] %<>% lapply(function(x) as.numeric(as.character(x)))
  x2 <- mutate(x1, Acute_Exceed=ifelse(Measure>Acute,1,0),
           Chronic_Exceed=ifelse(Measure>Chronic,1,0),
           PWS_Exceed=ifelse(Measure>PWS,1,0)) %>%
      select(siteID,Year,Metal,Measure,Acute,Chronic,PWS,Acute_Exceed,Chronic_Exceed,PWS_Exceed)
  return(x2)
}

metalsAssessment <- data.frame(siteID=NA,Year=NA,Metal=NA,Measure=NA,Acute=NA,Chronic=NA,
                               PWS=NA,Acute_Exceed=NA,Chronic_Exceed=NA,PWS_Exceed=NA)
for(i in 1:nrow(metalsData)){
  metalsAssessment <- rbind(metalsAssessment,suppressWarnings(metalsCriteriaAssessment(metalsData[i,])))
}
metalsAssessment <- arrange(metalsAssessment,siteID, Year)

# Any Acute Exceedances
filter(metalsAssessment, Acute_Exceed==1)

# Any Chronic Exceedances
filter(metalsAssessment, Chronic_Exceed==1)

# IR window Acute Exceedances
filter(metalsAssessment, Year >= 2015 & Year <= 2020 & Acute_Exceed==1)

# IR Window Chronic Exceedances
filter(metalsAssessment, Year >= 2015 & Year <= 2020 & Chronic_Exceed==1)


```


## CDF analyses


And now bring in design status data (for VSCI/VCPMI). It is best to use this as the default design status dataset (and adjust TS to OT if not sampled) because it is the most complete version of the data.

THis step "widens" the panel and biopanel information from a factor to a 1/0 to make spsurvey's job easier.
```{r bringInDesignStatusData}
designStatus <- read_csv('processedData/designStatusIR2022.csv') %>%
  mutate(
    Panel1 = ifelse(Panel == "Phase1", 1, NA),
    Panel2 = ifelse(Panel == "Phase2", 1, NA),
    BioPanel1 = ifelse(BioPanel == "Phase1", 1, NA),
    BioPanel2 = ifelse(BioPanel == "Phase2", 1, NA),
    BioPanel3 = ifelse(BioPanel == "Phase3", 1, NA),
    BioPanel4 = ifelse(BioPanel == "Phase4", 1, NA)) %>% # housekeeping: recode biophase and change 1's to NA's to indicate it wasnt sampled in that window, break up Panel and BioPanel windows to separate columns for weight adjustment purposes
  dplyr::rename(siteID= sampleID ) # start playing nicely with spsurvey)


```



These are the data windows we are looking at for 2020 IR:

* Full sample window (2001-2020)
* 2022 IR (2015-2020)
* 2020 IR (2013-2018)
* 2018 IR (2011-2016)
* 2016 IR (2009-2014)
* 2014 IR (2007-2012)
* 2012 IR (2005-2010)
* 2010 IR (2003-2008)
* 2008 IR (2001-2007)
* Panels (Phase 1= 2001-2010; Phase 2= 2011-2020) Kept Phase 1 same as 2018IR to balance n samples in window
* BioPanels (Phase 1 = 2001-2005; Phase 2 = 2006-2010; Phase 3 = 2011-2015; Phase 4 = 2016-2020)



## Functions for Weight Adjustments to run all CDF data by subpopulations

The following functions run the CDF analyses for all parameters chosen from the surveyData dataframe (you can modify the selection to increase/decrease parameters run). The subpopEstimate() function runs each subpopulation and nests inside the listOfResults(), which outputs a list of all subpopulation results. The allCDFresults() **adjusts all weights according to sample window** and then calls the listOfResults() to run all the CDF, percentile, and population estimates for each subpopulation. See below for how to run each of these functions and how to view outputs.

The critical thing to update each cycle is to add the new temporal variables we want to analyze on, e.g. IR2022, update years for panel information, add new basin levels, etc. It is easy to find where these need to be updated in the CDFanalysis.R script simply by searching for "# change here each new cycle" in the find bar. 

New levels for this 2022 IR include estimate2019, estimate2020, estimatePhase1,estimatePhase2, estimateBay, estimateNonBay, estimateBayPhase1, estimateBayPhase2, estimateNonBayPhase1, estimateNonBayPhase2, estimateBioPanelPhase1, estimateBioPanelPhase2, estimateBioPanelPhase3, estimateBioPanelPhase4, estimateIR2022.

The new basins run are estimateRU, estimateJM, estimateNE, estimateRD, estimateYO, estimatePL, estimateCU, estimateRA, estimateBS, estimateTH, estimatePS, estimateJA, estimateCM, estimateTC as they are all above the magic 25-30 sample requirements to generate a CDF curve. This list could increase in the future.

```{r}
source('CDFanalysis.R')
```



Below is how one would run a single parameter and unpack the CDF and PCT data into individual data frames. To view the list structure or data, follow script examples.

```{r runIndividualParameter}
LRBS <- allCDFresults(designStatus, surveyData,'LRBS')
LRBS_CDFdf <- suppressWarnings(LRBS[3:length(LRBS)] %>% map_df(1))
LRBS_PCTdf <-  suppressWarnings(LRBS[3:length(LRBS)] %>% map_df(2))


```

Same for VSCI and how to get all CDF estimates at VSCI=60.

```{r VSCIexample}
# VLOOKUP (Excel function hack) by Julin Maloof, edited for list application by Emma Jones
vlookupEVJ <- function(table, #the table where you want to look for it; will look in first column
                       ref, #the value or values that you want to look for
                       column, #the column that you want the return data to come from,
                       range=FALSE, #if there is not an exact match, return the closest?
                       larger=FALSE) #if doing a range lookup, should the smaller or larger key be used?)
{
  # 2020 addition, make tibbles dataframes
  table <- as.data.frame(table)
  
  if(!is.numeric(column) & !column %in% colnames(table)) {
    stop(paste("can't find column",column,"in table"))
  }
  if(range) {
    if(!is.numeric(table[,1])) {
      stop(paste("The first column of table must be numeric when using range lookup"))
    }
    table <- table[order(table[,1]),] 
    index <- findInterval(ref,table[,1])
    if(larger) {
      index <- ifelse(ref %in% table[,1],index,index+1)
    }
    output <- table[index,column]
    output[!index <= dim(table)[1]] <- NA
    
  } else {
    output <- table[match(ref,table[,1]),column]
    output[!ref %in% table[,1]] <- NA #not needed?
  }
  dim(output) <- dim(ref)
  output
}


# run VSCI
VSCI <- allCDFresults(designStatus, surveyData,'VSCIVCPMI')



# find CDF results at 60 for all subpopulations
VSCI60 <- VSCI[3:length(VSCI)] %>% # start with our big 'ol list of lists
  map('CDF') %>% #extract just the 'CDF' list from each list item (subpopulation)
  map(`[`, c('Value','Estimate.P')) %>% # kinda like dplyr::select here, extract the Value and Estimate.P columns from each list item (CDF)
  map( vlookupEVJ, 60  , 2, TRUE) %>% # use the vlookup function on each of the tables to find the Estimate.P where value = 60
  tibble::enframe(value = "CDFestimateAt60") # change the results from that list into a tibble (kinda like dataframe)

View(VSCI60)
```

first make RDSarchive directory!!

### Run all parameters

```{r runEverything}
startTime <- Sys.time()
# probably a cooler way to do this with purrr, but here is my one for loop
for(i in 3:length(surveyData)){
  assign(names(surveyData)[i], suppressWarnings(allCDFresults(designStatus, surveyData,names(surveyData)[i])))
   saveRDS(get(names(surveyData)[i]),paste('processedData/RDSarchive/',names(surveyData)[i],'.RDS',sep=''))
}


# Then unpack each parameter to get a long df of CDF and PCT data
# Needs to be in separate loop so names of each object to be called will be in the environment
allCDF <- data.frame(Type=NA,Subpopulation=NA,Indicator=NA,Value=NA,NResp=NA,Estimate.P=NA,StdError.P=NA,LCB95Pct.P=NA,UCB95Pct.P=NA,   
                     Estimate.U=NA,StdError.U=NA,LCB95Pct.U=NA,UCB95Pct.U=NA)
allPCT <- data.frame(Type=NA,Subpopulation=NA,Indicator=NA,Statistic=NA,NResp=NA,Estimate=NA,StdError=NA,LCB95Pct=NA,UCB95Pct=NA)

# okay, one last one, I promise
for(i in 3:length(surveyData)){
  assign(paste(names(surveyData)[i],"CDFdf",sep="_"),
         suppressWarnings(get(names(surveyData)[i])[3:length(get(names(surveyData)[i]))] %>% 
                            map_df(1)))
  assign(paste(names(surveyData)[i],"PCTdf",sep="_"),
         suppressWarnings(get(names(surveyData)[i])[3:length(get(names(surveyData)[i]))] %>%
                            map_df(2)))
  allCDF <- rbind(allCDF,get(paste(names(surveyData)[i],"CDFdf",sep="_")))
  allPCT <- rbind(allPCT,get(paste(names(surveyData)[i],"PCTdf",sep="_")))
  
}

write.csv(allCDF,'processedData/allCDF.csv',row.names = F)
write.csv(allPCT,'processedData/allPCT.csv',row.names = F)

howLong <- Sys.time()-startTime

```




### Relative Risk

Now run relative risk analyses. First, get final weights from VSCIVCPMI all years to use for relative risk analyses.

```{r relativeRiskgetWeights}
weights <- readRDS('processedData/RDSarchive/VSCIVCPMI.RDS') 
weights <- weights[["dataAnalyzed"]] %>% select(siteID, Year, finalweight_all)
```


```{r relativeRiskOrganization}
####Need to remove the fair....need to run code at home to check results
library(magrittr)
toNumeric <- names(select(surveyData,DO:wshdImpPCT))
surveyData[,toNumeric] %<>% lapply(function(x) as.numeric(as.character(x)))
rrisk <- left_join(surveyData,select(designStatus,siteID, Year,`Longitude-DD`,`Latitude-DD`)
                   ,by=c('siteID','Year')) %>%
  select(siteID,Year,`Longitude-DD`,`Latitude-DD`,everything()) %>%
  left_join(weights, by=c('siteID','Year')) %>%
  select(siteID:`Latitude-DD`,finalweight_all,TN,TP,TDS,LRBS,TotHab,VSCIVCPMI,MetalCCU)

rrisk$VSCIstatus <- cut(rrisk$VSCIVCPMI,c(0,50,60,100),labels=c('Poor','Fair','Good'))  
rrisk$TotHabstatus <- cut(rrisk$TotHab, c(0,120,150,200),labels=c('Poor','Fair','Good'))
rrisk$TNstatus <- cut(rrisk$TN, c(0,1,2,100),labels=c('Good','Fair','Poor'))
rrisk$TPstatus <- cut(rrisk$TP, c(0,0.02,0.05,100),labels=c('Good','Fair','Poor'))
rrisk$TDSstatus <- cut(rrisk$TDS, c(0,100,350,20000),labels=c('Good','Fair','Poor'))
rrisk$MetalCCUstatus <- cut(rrisk$MetalCCU, c(0,1,2,100),labels=c('Good','Fair','Poor'))
rrisk$LRBSstatus <- cut(rrisk$LRBS, c(-10,-1,-0.5,0.5,10),labels=c('Poor','Fair','Good','Fair2'))
names(rrisk)
rrisk$VSCIstatus[ rrisk$VSCIstatus == "Fair" ] = NA
rrisk$TotHabstatus[ rrisk$TotHabstatus == "Fair" ] = NA
rrisk$TNstatus[ rrisk$TNstatus == "Fair" ] = NA
rrisk$TPstatus[ rrisk$TPstatus == "Fair" ] = NA
rrisk$TDSstatus[ rrisk$TDSstatus == "Fair" ] = NA
rrisk$MetalCCUstatus[ rrisk$MetalCCUstatus == "Fair" ] = NA
rrisk$LRBSstatus[ rrisk$LRBSstatus == "Fair" ] = NA
rrisk$LRBSstatus[ rrisk$LRBSstatus == "Fair2" ] = NA

```

```{r rriskPrep}

# Create a variable that contains only the name of the response variable.
resp.var<-"VSCIstatus"


#########.
# For stressor variables, we will initially select a few stressor variables 
# All of these must be condition class variables.

stres.vars<- c("TotHabstatus","TNstatus","TPstatus","TDSstatus","MetalCCUstatus","LRBSstatus");

# Create a vector containing the names of all selected stressor variables
#stres.vars<-c("PTL_COND","NTL_COND","TURB_COND","ANC_COND","SALINITY_COND",
#              "RDIS_COND", "RVEG_COND", "LITCVR_COND","LITRIPCVR_COND");


# First, set up the 4 data frames, using columns in ProbMetrics

# 4.1) The sites data frame:
sites.va.rr<-data.frame(siteID=rrisk$siteID, Use=rep(TRUE, nrow(rrisk)))

# ii) The subpopulation data frame. RR and AR estimates have high uncertainties
# and require large sample sizes. Thus, we will estimate RR and AR only for our largest sample size, 
# which is the whole state basis.
subpop.va.rr <- data.frame(siteID=rrisk$siteID, all.virginia=rep("All_of_VA", nrow(rrisk)))


# iii) The design data frame is the same as for extent estimation. However, to be safe,  
#      let's rebuild this data frame, since we are now working from texas.dat.rr;

# add marinus projection coordinates
#tmp <- marinus(rrisk$LatitudeDD,-rrisk$LongitudeDD)
#tmp$xmarinus <- tmp[,'x']
#tmp$ymarinus <- tmp[,'y']
####Noticed USEPA using albers ver marinus projection now (using geodalbers function spsurvey)


albers.cord.rr<-geodalbers(lon=rrisk$`Longitude-DD`,lat=rrisk$`Latitude-DD`)
design.va.rr<-data.frame(siteID=rrisk$siteID, 
                         xcoord=albers.cord.rr$xcoord,ycoord=albers.cord.rr$ycoord, 
                         wgt=rrisk$finalweight_all)


# iv) The data.cat data frame should contain siteID, plus all stressor
# and response variables;
data.cat.va.rr<-subset(rrisk, select=c("siteID",resp.var, stres.vars),drop=T);
names(data.cat.va.rr)[1]<-"siteID";

```

```{r relativeRiskAnalysis}
#Finally, we implement relrisk.analysis().

relrisk.estimates<-relrisk.analysis(sites=sites.va.rr,subpop=subpop.va.rr,
                                    design=design.va.rr,data.rr=as.data.frame(data.cat.va.rr),
                                    response.var=rep(resp.var,length(stres.vars)),
                                    stressor.var=stres.vars,
                                    response.levels=rep(list(c("Poor","Good")),length(stres.vars)), 
                                    stressor.levels=rep(list(c("Poor","Good")),length(stres.vars)))


write.csv(relrisk.estimates, file = "processedData/relriskIR2022.csv", row.names = FALSE)
write.csv(rrisk, file = "processedData/rrisk.csv", row.names = FALSE)

```


### Maps

Now time to make maps of sites sampled across the state for the introduction part of chapter. First you need to make shapefiles of all sites (wadeable and nonwadeable) ever sampled, then split by IR window, then pull watersheds from IR window from Landcover GIS database. I created the GIS project "MapDataOrganization.mxd" to overview the process.

```{r, GISwrangling- wadeable}
allProb <- left_join(surveyData,designStatus,by=c('siteID','Year')) %>%
  select(siteID, Year, `strahler order`:`Latitude-DD`,Basin:IR2022) %>%
  mutate(StationID = gsub("_.*$", "", siteID)) %>% # make stationID column with no trend years
  select(StationID, everything())%>%
  rename(Longitude=`Longitude-DD`,Latitude=`Latitude-DD`) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), 
               remove = F, # don't remove these lat/lon cols from df
               crs = 4326)  # add projection, needs to be geographic for now bc entering lat/lng

#st_write(allProb, 'processedData/GIS/allProb2022.shp')

#st_layers('C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/Watersheds.gdb')

# Bring in watersheds to match to point files
allWshd <- st_read('C:/HardDriveBackup/GIS/ProbMonGIS/DelineatedWatersheds/Watersheds.gdb',
                   'AllWatersheds_through2020') %>% 
  mutate(StationID = sub("\r\n" ,"",StationID) ) # Get rid of any stray spaces after StationID in attribute table


allWshdforProb <- filter(allWshd, StationID %in% allProb$StationID)
#allWshdforProb <- spTransform(allWshdforProb,CRS( "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Now save it
#st_write(allWshdforProb, "processedData/GIS/allProbWshd2022.shp")
```



```{r  GISwrangling- non wadeable}
#bring in old boatable sites and add these to that dataset
boatableSites <- st_read('C:/HardDriveBackup/R/GitHub/FreshwaterProbMonIntegratedReports/2020ProbChapter/processedData/boatableSites2008_2018.shp') %>% 
  dplyr::select(StationID)

# new boatable sites 
boatableSitesNew <- filter(designStatus, status == 'NT' ) %>% 
  filter(str_detect(comment, 'Large') | str_detect(comment, 'Boat')) %>% 
  filter(! str_detect(comment, 'Lake')) %>% 
  filter(! str_detect(comment, 'not sampled')) %>% 
  filter(! str_detect(comment, 'dry')) %>% 
  filter(! str_detect(comment, 'tidal')) %>% 
                          # NT isn't perfect filter but if there is a boatable watershed then it was sampled
                          mutate(StationID = gsub("_.*$", "", siteID)) %>% 
  st_as_sf(coords = c("Longitude-DD", "Latitude-DD"), 
               remove = T, # don't remove these lat/lon cols from df
               crs = 4326) %>%   # add projection, needs to be geographic for now bc entering lat/lng
  dplyr::select(StationID = siteID)

boatableSitesNew <- rbind(boatableSites, boatableSitesNew) %>% 
  #distinct(StationID) %>% # distinct is not working
  group_by(StationID) %>% 
  mutate(n = n()) %>% 
  filter(n == 1)

boatableWshdStationID <- boatableSitesNew %>% 
   pull(StationID)
boatableWshd <- filter(allWshd, StationID %in% boatableWshdStationID) %>% 
  dplyr::select(StationID) %>% 
  st_transform(st_crs(boatableWshd)) %>% 
  rename(geometry= Shape)


st_write(boatableSitesNew, "processedData/GIS/boatableSites2006_2020.shp")
st_write(boatableWshd, "processedData/GIS/boatableWshd2006_2020.shp")

```



### Micromap Section

This section makes micromaps from CDF data and shapefiles

```{r}
library(micromap)

# VLOOKUP (Excel function hack) by Julin Maloof
vlookup <- function(ref, #the value or values that you want to look for
                    table, #the table where you want to look for it; will look in first column
                    column, #the column that you want the return data to come from,
                    range=FALSE, #if there is not an exact match, return the closest?
                    larger=FALSE) #if doing a range lookup, should the smaller or larger key be used?)
{
  # 2020 addition, make tibbles dataframes
  table <- as.data.frame(table)
  
  if(!is.numeric(column) & !column %in% colnames(table)) {
    stop(paste("can't find column",column,"in table"))
  }
  if(range) {
    if(!is.numeric(table[,1])) {
      stop(paste("The first column of table must be numeric when using range lookup"))
    }
    table <- table[order(table[,1]),] 
    index <- findInterval(ref,table[,1])
    if(larger) {
      index <- ifelse(ref %in% table[,1],index,index+1)
    }
    output <- table[index,column]
    output[!index <= dim(table)[1]] <- NA
    
  } else {
    output <- table[match(ref,table[,1]),column]
    output[!ref %in% table[,1]] <- NA #not needed?
  }
  dim(output) <- dim(ref)
  output
}

# Stats lookup function (gets data in correct format for micromaps)
statslookup <- function(indicator,measure,category,revOrder){
  chowan <-  filter(dat,Subpopulation=='Chowan'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  rappahannock <-  filter(dat,Subpopulation=='Rappahannock'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  york <-  filter(dat,Subpopulation=='York'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  potomac <-  filter(dat,Subpopulation=='Potomac'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  shenandoah <-  filter(dat,Subpopulation=='Shenandoah'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  roanoke <-  filter(dat,Subpopulation=='Roanoke Basin'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  james <-  filter(dat,Subpopulation=='James Basin'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  new <-  filter(dat,Subpopulation=='New'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  bigsandy <-  filter(dat,Subpopulation=='Big Sandy'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  cp <-  filter(dat,Subpopulation=='Clinch-Powell'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  holston <-  filter(dat,Subpopulation=='Holston'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  Virginia <-  filter(dat,Subpopulation=='Virginia'&Indicator==indicator)%>%
    select(Value,Estimate.P,LCB95Pct.P,UCB95Pct.P,NResp)
  
  x <- data.frame(Subpopulation=c('Chowan','Rappahannock','York','Potomac','Shenandoah','Roanoke','James',
                                  'New','Big Sandy','Clinch-Powell','Holston','Virginia'),
                  Category=category,NResp=c(max(chowan$NResp),max(rappahannock$NResp),max(york$NResp),
                                            max(potomac$NResp),max(shenandoah$NResp),max(roanoke$NResp),
                                            max(james$NResp),max(new$NResp),max(bigsandy$NResp),max(cp$NResp),
                                            max(holston$NResp),max(Virginia$NResp)),
                  matchingdfname=c('chowan','rappahannock','york','potomac','shenandoah','roanoke','james','new','bigsandy','cp','holston','Virginia'))
  y=data.frame(Estimate.P=NA,LCB95Pct.P=NA, UCB95Pct.P=NA)
  for(i in 1:nrow(x)){
    y[i,1] <- vlookup(measure,get(as.character(x[i,4])),2,TRUE)
    y[i,2] <- vlookup(measure,get(as.character(x[i,4])),3,TRUE)
    y[i,3] <- vlookup(measure,get(as.character(x[i,4])),4,TRUE)
    y[is.na(y)] <- 0
    y[y>100] <- 100
  }
  y2 <- mutate(y,Estimate.P2=100-Estimate.P,LCB95Pct.P2=Estimate.P2-(Estimate.P-LCB95Pct.P),
               UCB95Pct.P2=Estimate.P2+(UCB95Pct.P-Estimate.P))%>%select(Estimate.P2,LCB95Pct.P2,UCB95Pct.P2)
  y2[y2>100] <- 100
  names(y) <- c(paste(indicator,"Estimate.P",sep=""),paste(indicator,"LCB95Pct.P",sep=""),paste(indicator,"UCB95Pct.P",sep=""))
  names(y2) <- c(paste(indicator,"Estimate.P",sep=""),paste(indicator,"LCB95Pct.P",sep=""),paste(indicator,"UCB95Pct.P",sep=""))
  if(revOrder==FALSE){z <- cbind(x,y)}else{z<-cbind(x,y2)}
  return(z)
}

```

```{r VSCIstatusMicromap} 
basinssmooth <- readOGR('originalData','VAbasins_smoothNoChesPeeDee') # have to use sp for micromaps
map.table <- create_map_table(basinssmooth,'BASIN')

dat <- read_csv('processedData/allCDF.csv')

# for other VSCI graphic
chowan <-  filter(dat, Subpopulation == 'Chowan' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
rappahannock <-  filter(dat, Subpopulation == 'Rappahannock' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
york <-  filter(dat, Subpopulation == 'York' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
potomac <-  filter(dat, Subpopulation == 'Potomac' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
shenandoah <-  filter(dat, Subpopulation == 'Shenandoah' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
roanoke <-  filter(dat, Subpopulation == 'Roanoke Basin' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
james <-  filter(dat, Subpopulation == 'James Basin' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
new <-  filter(dat, Subpopulation == 'New' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
bigsandy <-  filter(dat, Subpopulation == 'Big Sandy' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
cp <-  filter(dat, Subpopulation == 'Clinch-Powell' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())
holston <-  filter(dat, Subpopulation == 'Holston' & Indicator == 'VSCIVCPMI') %>%
  select(Value:UCB95Pct.P,everything())

# pull median VSCI scores and upper/lower bounds for micromap
stats <- data.frame(Basin=c('Chowan','Rappahannock','York','Potomac','Shenandoah','Roanoke','James','New',
                            'Big Sandy','Clinch-Powell', 'Holston'),Indicator='VSCIVCPMI')

# Switch lookup column to search for median
chowan <- select(chowan,Estimate.P,everything())
rappahannock <- select(rappahannock,Estimate.P,everything())
york <- select(york,Estimate.P,everything())
potomac <- select(potomac,Estimate.P,everything())
shenandoah <- select(shenandoah,Estimate.P,everything())
roanoke <- select(roanoke,Estimate.P,everything())
james <- select(james,Estimate.P,everything())
new <- select(new,Estimate.P,everything())
bigsandy <- select(bigsandy,Estimate.P,everything())
cp <- select(cp,Estimate.P,everything())
holston <- select(holston,Estimate.P,everything())

#statssuperB <- suppressWarnings(
#  rbind(data.frame(vlookup(50,chowan,2:4,TRUE)),data.frame(vlookup(50,rapYork,2:4,TRUE)),
#        data.frame(vlookup(50,potShen,2:4,TRUE)),data.frame(vlookup(50,roanoke,2:4,TRUE)),
#        data.frame(vlookup(50,tennessee,2:4,TRUE)),data.frame(vlookup(50,james,2:4,TRUE)),
#        data.frame(vlookup(50,new,2:4,TRUE))))

statsbasin <- rbind(
  data.frame(x25=vlookup(25,chowan,2,TRUE),x50=vlookup(50,chowan,2,TRUE),
             x75=vlookup(75,chowan,2,TRUE),n=max(chowan$NResp)),
  data.frame(x25=vlookup(25,rappahannock,2,TRUE),x50=vlookup(50,rappahannock,2,TRUE),
             x75=vlookup(75,rappahannock,2,TRUE),n=max(rappahannock$NResp)),
  data.frame(x25=vlookup(25,york,2,TRUE),x50=vlookup(50,york,2,TRUE),
             x75=vlookup(75,york,2,TRUE),n=max(york$NResp)),
  data.frame(x25=vlookup(25,potomac,2,TRUE),x50=vlookup(50,potomac,2,TRUE),
             x75=vlookup(75,potomac,2,TRUE),n=max(potomac$NResp)),
  data.frame(x25=vlookup(25,shenandoah,2,TRUE),x50=vlookup(50,shenandoah,2,TRUE),
             x75=vlookup(75,shenandoah,2,TRUE),n=max(shenandoah$NResp)),
  data.frame(x25=vlookup(25,roanoke,2,TRUE),x50=vlookup(50,roanoke,2,TRUE),
             x75=vlookup(75,roanoke,2,TRUE),n=max(roanoke$NResp)),
  data.frame(x25=vlookup(25,james,2,TRUE),x50=vlookup(50,james,2,TRUE),
             x75=vlookup(75,james,2,TRUE),n=max(james$NResp)),
  data.frame(x25=vlookup(25,new,2,TRUE),x50=vlookup(50,new,2,TRUE),
             x75=vlookup(75,new,2,TRUE),n=max(new$NResp)),
  data.frame(x25=vlookup(25,bigsandy,2,TRUE),x50=vlookup(50,bigsandy,2,TRUE),
             x75=vlookup(75,bigsandy,2,TRUE),n=max(bigsandy$NResp)),
  data.frame(x25=vlookup(25,cp,2,TRUE),x50=vlookup(50,cp,2,TRUE),
             x75=vlookup(75,cp,2,TRUE),n=max(cp$NResp)),
  data.frame(x25=vlookup(25,holston,2,TRUE),x50=vlookup(50,holston,2,TRUE),
             x75=vlookup(75,holston,2,TRUE),n=max(holston$NResp)))

stats <- cbind(stats,statsbasin)

#mmplot(stat.data=stats,
#       map.data=map.table,
#       panel.types=c('dot_legend', 'labels', 'dot_cl', 'map'),
#       panel.data=list(NA,'Basin',list('x50', 'x25', 'x75'),NA),
#       ord.by='x50', grouping=4,
#       median.row=F,
#       map.link=c("Basin", "ID"))



suppressWarnings(
  mmplot(stat.data=stats,
       map.data=map.table,
       map.link=c("Basin", "ID"),
       panel.types=c('dot_legend', 'labels','labels', 'dot_cl', 'map'),
       panel.data=list(NA,'Basin','n',list('x50', 'x25', 'x75'),NA),
       ord.by='x50',
       grouping=4,
       median.row=F,
       plot.height=7,
       plot.width=8,
       colors=brewer.pal(7, "Spectral"),
       rev.ord=T,
       panel.att=list(list(1, point.type=20, point.border=TRUE, point.size=2),
                      list(2, header='Basin', panel.width=.4, 
                           align='left', text.size=.9),
                      list(3,header='n',panel.width=.2,align='left',text.size=.9),
                      list(4, header='Estimated Median VSCI Score and \nAssociated Interquartile Range',
                           graph.bgcolor='lightgray', point.size=1.5,
                           xaxis.ticks=list(40,50,60,70,80), xaxis.labels=list(40,50,60,70,80)
                           ,add.line=60,add.line.col='black',add.line.typ='dashed',
                           xaxis.title='VSCI Score'),
                      list(5, header='Light Gray Means\nPreviously Displayed',
                           map.all=TRUE, fill.regions='aggregate',
                           active.border.color='black', active.border.size=1.0,
                           inactive.border.color=gray(.7), inactive.border.size=1, 
                           panel.width=1.0))) )

 


```




```{r}
TotHabstats <- statslookup('TotHab',120,'SubOptimal',FALSE)%>%select(-c(matchingdfname))

VSCIstats <- statslookup('VSCIVCPMI',60,'SubOptimal',FALSE)%>%select(-c(matchingdfname))


TotHaball <- merge(VSCIstats,TotHabstats,by=c('Subpopulation','Category'))
TotHabsummary <- TotHaball%>%
  filter(Subpopulation!='Virginia')%>% # get rid of Virginia to make mmplot work
  arrange(desc(TotHabEstimate.P))

map.table <- create_map_table(basinssmooth,'BASIN')
suppressWarnings(
  mmplot(stat.data=TotHabsummary,
       map.data=map.table,
       map.link=c("Subpopulation", "ID"),
       panel.types=c('map','labels', 'bar_cl', 'bar_cl'),
       panel.data=list(NA,'Subpopulation',
                       list( "TotHabEstimate.P","TotHabLCB95Pct.P","TotHabUCB95Pct.P"),
                       list("VSCIVCPMIEstimate.P","VSCIVCPMILCB95Pct.P","VSCIVCPMIUCB95Pct.P")),
       ord.by='TotHabEstimate.P',
       grouping=3,
       median.row=F,
       plot.height=7,
       plot.width=8,
       colors=brewer.pal(3, "Spectral"),
       rev.ord=T,
       panel.att=list(list(1,header='Light Gray Means\nPreviously Displayed',
                           map.all=TRUE, fill.regions='aggregate',
                           active.border.color='black', active.border.size=1.0,
                           inactive.border.color=gray(.7), inactive.border.size=1, 
                           panel.width=1.0),
                      list(2, header='Basin', panel.width=.4, 
                           align='left', text.size=.9),
                      list(3,header='Percent of Stream Miles with \nSuboptimal Habitat Disturbance',
                           graph.bgcolor='lightgray',
                           graph.bar.size = .4,
                           xaxis.ticks=list(0,20,40,60,80,100), xaxis.labels=list(0,20,40,60,80,100)
                           ,add.line=16.7393531 ,add.line.col='black',add.line.typ='dashed',
                           xaxis.title='Percent of Stream Miles'),
                      list(4,header='Percent of Stream Miles below \n VSCI/VCPMI Assessment Threshold',
                           graph.bgcolor='lightgray',
                           graph.bar.size = .4,
                           add.line=45.41627,add.line.col='black',add.line.typ='dashed',
                           xaxis.title='Percent of Stream Miles',
                           xaxis.ticks = c(0,20,40,60,80,100),
                           xaxis.labels = c(0,20,40,60,80,100)))))


```




