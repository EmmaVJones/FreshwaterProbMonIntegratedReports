---
title: "Data Acquisition"
author: "Emma Jones"
date: "10/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(config)
library(sf)
library(lubridate)
library(pool)
library(pins)
library(sqldf)
library(dbplyr)
library(readxl)

```

## Background

This script connects to the ODS environment, pulls data for assessment window, and summarizes results. Historically, field and analyte information has been reduced to one record per sample year, even if the station was sampled more than once (e.g. spring and fall). The summary statistic used is the median for this report.

## Connect to ODS

Make sure pulling from production environment and not test since those are not exact copies.

```{r connect to ODS}

# Server connection things
conn <- config::get("connectionSettings") # get configuration settings


board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## Connect to ODS production
pool <- dbPool(
 drv = odbc::odbc(),
 Driver = "ODBC Driver 11 for SQL Server",#"SQL Server Native Client 11.0",
 Server= "DEQ-SQLODS-PROD,50000",
 dbname = "ODS",
 trusted_connection = "yes"
)
```

## Bring in final data format to match

Below is the final dataset for the 2020 IR that we will aim to match for 2022 IR.

```{r prob 2020}
prob2020 <- read_csv('originalData/Wadeable_ProbMon_2001-2018_Final_Final.csv')
```

Generally speaking, we need to hit a few data sources to acquire all the data we need to publish for final chapter:
 - Field Data (Wqm_Field_Data_View) for DO, pH, SpCond
 - Analyte Data (Wqm_Analyte_Data_View) for TN, TP, TDS, NH4, NO3, TKN, Ortho-P, Turb, TSS, Na, K, Cl, Sf, 70331VFine, SSCCOARSE, SSCFINE, SSCTOTAL
 - PHAB database (still in Access) for LRBS, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, BL_CB_GR_Embed_PCT, Embed_PCT
 - EDAS (pinned (pre calculated) habitat data on server) for TotHab
 - EDAS (pinned (pre calculated) benthic data on server) for TotTaxa, EPTTax, VEphem, VPTHydropsychidae, VScrap, VChiro, V2Dom, HBI, VHapto, EPTInd, VSCIVCPMI (calculated conversion)
 - Analyte Data (Wqm_Analyte_Data_View) for dissolved metals MetalCCU (calculated), ARSENICppm, BERYLLIUMppm, CADMIUMppm, CHROMIUMppm, COPPERppm, LEADppm, MANGppm, NICKELppm, SILVERppm, ZINCppm, ANTIMONYppm, ALUMINUMppm, SELENIUMppm, IRONppm, MERCURYppm, THALLIUMppm, CALCIUM, MAGNESIUM, ARSENIC, BARIUM, BERYLLIUM, CADMIUM, CHROMIUM, COPPER, IRON, LEAD, MANGANESE, THALLIUM, NICKEL, SILVER, ZINC, ANTIMONY, ALUMINUM, SELENIUM, HARDNESS, MERCURY, Hg-C
 - Special GIS dataset for watershed spatial info 


## Bring in sites

```{r 2020 final sites}
WQM_Stations_Filter <- read_excel('originalData/Wadeable_ProbMon_2019-2020_Final.xlsx', sheet = 'All20192020StationInfo') %>% 
  filter(status == 'TS') # keep only wadeable sites that were sampled for data querying
  # read_excel('originalData/20192020FP.xlsx', sheet = '2019') %>% 
  #       dplyr::select(StationID = Sta_Id) %>% 
  #       bind_rows(
  #               read_excel('originalData/20192020FP.xlsx', sheet = '2020') %>% 
  #                       dplyr::select(StationID = Sta_Id)       ) # %>% 
  #filter(StationID == '3-MTN021.11')#1AXOR000.47')#2-BGC008.10')


```


## Query data

Using the current build of the conventionals function to query and organize field, analyte, and metals data consistently. Field and analyte methods pulled from CEDS WQM data query tool.

### Query Terms

These presets allow correct data retrieval. 

```{r query terms}

# Basic station info for conventionals
multiStationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()
multiStationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()

# make sure all stations are in CEDS
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationInfo$Sta_Id]
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationGIS_View$Station_Id]

```


### Field data

```{r field data}
#dateRange_multistation <- c(as.Date('2020-01-01'), as.Date('2020-12-31'))# c(as.Date('2019-02-05'), as.Date('2019-02-08'))# 
x2019Sites <- filter(WQM_Stations_Filter, Year == 2019)$StationID
x2020Sites <- filter(WQM_Stations_Filter, Year == 2020)$StationID

# do as two steps to make sure only bring back data from desired window for each site
multistationFieldData <- bind_rows(
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2019Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2019-01-01'), as.Date('2019-12-31'))) %>% # & # x >= left & x <= right
    as_tibble(),
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2020Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2020-01-01'), as.Date('2020-12-31'))) %>% # & # x >= left & x <= right
    as_tibble()) %>% 
  filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")

  # filter(Fdt_Sta_Id %in% !! WQM_Stations_Filter$StationID &
  #          between(as.Date(Fdt_Date_Time), !! dateRange_multistation[1], !! dateRange_multistation[2])) %>% # & # x >= left & x <= right
           #Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>%  # don't drop QA failure on SQL part bc also drops any is.na(Ssc_Description)
  # as_tibble() %>% 
  # filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")
```

### Analyte Data

```{r analyte data}
# can do this in one step since we are searching for Fdt_Id's from appropriate field data windows
multistationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! multistationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange_multistation[1], !! dateRange_multistation[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>% 
  as_tibble() %>%
  left_join(dplyr::select(multistationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

```


### Organize by Conventionals logic

To consistently organize field, analyte, and metals data it is prudent to use the "conventionals" data rules.

```{r conventionals organization of raw data}
source('C:/HardDriveBackup/R/GitHub/WQMdataQueryTool/conventionalsFunction1232021.R')

conventionalsList <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                                          stationFieldDataUserFilter= multistationFieldData, 
                                          stationAnalyteDataUserFilter = multistationAnalyteData, 
                                          stationInfo = multiStationInfo,
                                          stationGIS_View = multiStationGIS_View,
                                          dropCodes = c('QF'),
                                          assessmentUse = F) 
conventionals <- conventionalsList$More %>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)

#write.csv(conventionals, '2020conventionals2.csv', row.names = F, na = "")
```

### QA data with Lucy

In order to make sure we are pulling the same information from Logi and R-ODS connection, the following script walks users through verification. Lucy pulled 2020 data using Logi.

```{r convetionals QA}
source('conventionalsQA.R')
```

Looks good for parameters matching what logi pulls. 

Now quick QA test for EB information. What we want is to make sure that none of the blanks have too high values, indicating too much uncertainty with respect to units and potentially exceeding standards.

```{r QA conventionals EB}
QAconventionals <- conventionals %>% 
  filter(Ana_Sam_Mrs_Container_Id_Desc == 'EB') %>% 
  dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, NITROGEN_mg_L:RMK_82079) %>% 
  dplyr::select(!contains('LEVEL_'))

# two steps bc can't combine character and numeric entries in a pivot longer
QAconventionals <- left_join(
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, ! contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols =- c(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'Parameter', values_to = 'Measure', values_drop_na = T),
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc,  contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols =- c(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = '', values_to = 'Measure', values_drop_na = T) ,
  by = c("FDT_STA_ID", "FDT_DATE_TIME", "FDT_SPG_CODE", "Ana_Sam_Mrs_Container_Id_Desc"))
  
  QAconventionalsChar <-  

group_by(FDT_STA_ID, FDT_DATE_TIME) %>% 
  mutate(n = n()) %>% 
  filter(n > 1) %>% 
  dplyr::select(n,FDT_STA_ID,	FDT_DATE_TIME,	FDT_DEPTH,	FDT_SPG_CODE,	Ana_Sam_Mrs_Container_Id_Desc:RMK_82079, everything()) %>% 
  arrange(FDT_STA_ID,	FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc)

# QAconventionals <- conventionals %>% 
#   group_by(FDT_STA_ID, FDT_DATE_TIME) %>% 
#   mutate(n = n()) %>% 
#   filter(n > 1) %>% 
#   dplyr::select(n,FDT_STA_ID,	FDT_DATE_TIME,	FDT_DEPTH,	FDT_SPG_CODE,	Ana_Sam_Mrs_Container_Id_Desc:RMK_82079, everything()) %>% 
#   arrange(FDT_STA_ID,	FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc)

#write.csv(QAconventionals, 'test.csv', row.names = F, na="")
```


For reporting purposes, we don't want to use the raw data (even though Roger has confidence in those numbers). We want to back lab reported values back to 0.001 if they fall below that. This is the first report we are pulling uncensored data. We need to repull and organize all years of data with uncensored and censored and rerun statistics for both (some programs e.g. permitting) may prefer censored data but uncensored could offer smoother CDF curves (remove blocky detection limits that have improved over last 20+ years).

Jason and I have determined that using uncensored this round is okay because we are only cleaning up the lower end of the dataset and are not coming close to potential standards violations.

```{r back to positive}
conventionals2 <- conventionals
conventionals2[conventionals2 < 0] <- 0.001  
# double check things worked
summary(conventionals)
summary(conventionals2) # make sure columns with negative mins have gone to 0.001

#Change longitude back to real numbers
conventionals2 <- left_join(conventionals2,
                            dplyr::select(conventionals, FDT_STA_ID, Longitude) %>% 
                              distinct(FDT_STA_ID, .keep_all = T),
                            by = 'FDT_STA_ID') %>% 
  mutate(Longitude.x = Longitude.y) %>% 
  rename(Longitude = Longitude.x) %>% 
  dplyr::select(-Longitude.y)
```

Next we need to change the data for manual QA to the probmon format. We will also drop EB's at this point.


```{r prob data format}

probData <- conventionals2 %>%
  filter(Ana_Sam_Mrs_Container_Id_Desc != 'EB') %>% 
  mutate(StationID = FDT_STA_ID, 
         Year = year(FDT_DATE_TIME),
              DO = DO_mg_L, 
              pH = FDT_FIELD_PH,
              SpCond = FDT_SPECIFIC_CONDUCTANCE, 
              TN = NITROGEN_mg_L, 
              TP = PHOSPHORUS_mg_L, 
              TDS = TDS_mg_L,
              NH4 = AMMONIA_mg_L, 
              NO3 = NITRATE_mg_L, 
              TKN = NITROGEN_KJELDAHL_TOTAL_00625_mg_L,
              `Ortho-P` = PHOSPHORUS_TOTAL_ORTHOPHOSPHATE_70507_mg_L,
              Turb = `TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU`,
              TSS = TSS_mg_L, 
              Na = `SODIUM, DISSOLVED (MG/L AS NA)`, 
              K = `POTASSIUM, DISSOLVED (MG/L AS K)`,
              Cl = CHLORIDE_mg_L,
              Sf = SULFATE_mg_L,
              `70331VFine` = `SSC%Finer`,
              SSCCOARSE = SSC_COARSE,
              SSCFINE =  SSC_FINE, 
              SSCTOTAL = SSC_TOTAL,
              # sediment ppm data hasn't been collected since 2011
              ARSENICppm = as.numeric(NA),
              BERYLLIUMppm = as.numeric(NA),
              CADMIUMppm = as.numeric(NA),
              CHROMIUMppm = as.numeric(NA),
              COPPERppm = as.numeric(NA),
              LEADppm = as.numeric(NA),
              MANGppm = as.numeric(NA),
              NICKELppm = as.numeric(NA),
              SILVERppm = as.numeric(NA),
              ZINCppm = as.numeric(NA),
              ANTIMONYppm = as.numeric(NA),
              ALUMINUMppm = as.numeric(NA),
              SELENIUMppm = as.numeric(NA),
              IRONppm = as.numeric(NA),
              MERCURYppm = as.numeric(NA),
              THALLIUMppm = as.numeric(NA),
              CALCIUM = `CALCIUM, DISSOLVED (MG/L AS CA)`,
              MAGNESIUM = `MAGNESIUM, DISSOLVED (MG/L AS MG)`,
              ARSENIC = `ARSENIC, DISSOLVED  (UG/L AS AS)`,
              BARIUM = `BARIUM, DISSOLVED (UG/L AS BA)`,
              BERYLLIUM = `BERYLLIUM, DISSOLVED (UG/L AS BE)`,
              CADMIUM = `CADMIUM, DISSOLVED (UG/L AS CD)`,
              CHROMIUM = `CHROMIUM, DISSOLVED (UG/L AS CR)`,
              COPPER = `COPPER, DISSOLVED (UG/L AS CU)`,
              IRON = `IRON, DISSOLVED (UG/L AS FE)`,
              LEAD = `LEAD, DISSOLVED (UG/L AS PB)`,
              MANGANESE = `MANGANESE, DISSOLVED (UG/L AS MN)`,
              THALLIUM = `THALLIUM, DISSOLVED (UG/L AS TL)`,
              NICKEL = `NICKEL, DISSOLVED (UG/L AS NI)`,
              SILVER = `SILVER, DISSOLVED (UG/L AS AG)`,
              ZINC = `ZINC, DISSOLVED (UG/L AS ZN)`,
              ANTIMONY = `ANTIMONY, DISSOLVED (UG/L AS SB)`,
              ALUMINUM = `ALUMINUM, DISSOLVED (UG/L AS AL)`,
              SELENIUM = `SELENIUM, DISSOLVED (UG/L AS SE)`,
              HARDNESS = `HARDNESS, CA MG CALCULATED (MG/L AS CACO3) AS DISSOLVED`,
              MERCURY = `MERCURY-TL,FILTERED WATER,ULTRATRACE METHOD NG/L`,
              `Hg-C` = `RMK_50091`) %>% 
  dplyr::select(FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, Ana_Sam_Mrs_Container_Id_Desc, #other helpful info
                any_of(names(prob2020))) %>% 
  dplyr::select(StationID, Year, FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, 
                Ana_Sam_Mrs_Container_Id_Desc, everything()) %>% 
  arrange(Year, StationID, FDT_DATE_TIME)

#clean up workspace
rm(multistationAnalyteData);rm(multistationFieldData);rm(multiStationGIS_View);rm(multiStationInfo)
rm(conventionalsList);rm(conventionals);rm(conventionals2)
```


### Metals CCU

This is a good point to run the Metals CCU analysis before QA and yearly consolidation.

MetalsCCU calculation function from BSA tool. This version is the correct version with Larry's bonus metals commented out.

```{r metalsCCUfunction}

# Metals CCU Calculation, with Larry Bonus metals commented out
metalsCCUcalc <- function(Hardness,Aluminum,Arsenic,Cadmium,Chromium,Copper,Lead,Nickel,Selenium,Zinc){
  criteriaHardness <- ifelse(Hardness<25,25,ifelse(Hardness>400,400,Hardness))
  #AluminumEPAChronic <- Aluminum/150 # Larry Bonus Metal
  ArsenicChronic <- Arsenic/150
  #CadmiumChronic <- Cadmium/(exp(0.7852*(log(criteriaHardness))-3.49)) # Larry Bonus Metal
  ChromiumChronic <- Chromium/((exp(0.819*(log(criteriaHardness))+0.6848))*0.86)
  CopperChronic <- Copper/((exp(0.8545*(log(criteriaHardness))-1.702))*0.96)
  LeadChronic <- Lead/((exp(1.273*(log(criteriaHardness))-3.259))*(1.46203-(log(criteriaHardness)*(0.145712))))
  NickelChronic <- Nickel/((exp(0.846*(log(criteriaHardness))-0.884))*0.997)
  #SeleniumChronic <- Selenium/5 # Larry Bonus Metal
  ZincChronic <- Zinc/((exp(0.8473*(log(criteriaHardness))+0.884))*0.986)
  #return(sum(AluminumEPAChronic,ArsenicChronic,CadmiumChronic,ChromiumChronic,CopperChronic,LeadChronic,
  #           NickelChronic,SeleniumChronic,ZincChronic))
  return(sum(ArsenicChronic,ChromiumChronic,CopperChronic,LeadChronic,
             NickelChronic,ZincChronic))
}


```

Just get necessary data to run function.

```{r metalsCCUcalculation}
metalsCCUresults <- select(probData,StationID, Year,  FDT_DATE_TIME, Ana_Sam_Mrs_Container_Id_Desc,
                           HARDNESS, ALUMINUM, ARSENIC, CADMIUM, 
                           CHROMIUM, COPPER, LEAD, NICKEL, SELENIUM, ZINC)

metalsCCUresults <- metalsCCUresults %>% 
  rowwise() %>% 
  mutate( MetalCCU = round(metalsCCUcalc(Hardness = HARDNESS, Aluminum = ALUMINUM, 
                               Arsenic = ARSENIC,Cadmium = CADMIUM,
                               Chromium = CHROMIUM,Copper = COPPER, 
                               Lead = LEAD,Nickel =NICKEL,
                               Selenium = SELENIUM,Zinc = ZINC), 
                            digits=4))  %>% 
  dplyr::select(StationID:Ana_Sam_Mrs_Container_Id_Desc, MetalCCU)


probData <- left_join(probData, metalsCCUresults,
                       by = c("StationID", "Year", "FDT_DATE_TIME", "Ana_Sam_Mrs_Container_Id_Desc"))

``` 




Last, we need to remove extra samples not associated with FP/bio sampling events but not drop too much in case other programs grabbed samples on a different date. This is still best done manually. For 2022IR, Lucy manually screened data for just spring/fall sample information (using excel).

To do this, we need benthic data.

### Benthic Data

Use pinned benthic data to pull appropriate SCI information for sites.

```{r get ecoregion for SCI}
ecoregion <- pin_get("ejones/WQM-Stations-Spatial", board = "rsconnect") %>% 
        filter(StationID %in% WQM_Stations_Filter$StationID) %>% 
  dplyr::select(StationID, US_L3CODE, Basin)
```

Get all SCI information from pinned data. 

```{r SCI pins}
VSCIresults <- pin_get("ejones/VSCIresults", board = "rsconnect")
VCPMI63results <- pin_get("ejones/VCPMI63results", board = "rsconnect")
VCPMI65results <- pin_get("ejones/VCPMI65results", board = "rsconnect")
```

Bring in correct BenSampID info for sites.

```{r}
x2019Sites <- filter(WQM_Stations_Filter, Year == 2019)$StationID
x2020Sites <- filter(WQM_Stations_Filter, Year == 2020)$StationID

bioData2019 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2019Sites & year(`Collection Date`) == 2019) 
bioData2020 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2020Sites & year(`Collection Date`) == 2020) 

# make sure no sites are missing biological data (our key to using a site)
x2019Sites[! x2019Sites %in% bioData2019$StationID]
x2020Sites[! x2020Sites %in% bioData2020$StationID]

benSamps_Filter_fin <- bind_rows(bioData2019, bioData2020) %>% 
  filter(str_detect(BenSampID, 'R110')) %>% # only keep rarified data
  filter(RepNum == 1) %>% # only use Rep1
  left_join(ecoregion, by = 'StationID')
```

Get correct SCI information. Make sure to grab the VSCI metrics for all stations not run with VSCI and have that ready for final data smashing. We report the VSCI metrics for VCPMI sites for continuity.

```{r}
SCI_filter <- filter(VSCIresults, BenSampID %in% filter(benSamps_Filter_fin, ! US_L3CODE %in% c(63,65))$BenSampID) %>%
  bind_rows(
    filter(VCPMI63results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(63) | str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  bind_rows(
    filter(VCPMI65results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(65) & !str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  mutate(SeasonGradient = as.factor(paste0(Season, " (",Gradient,")"))) %>%
  left_join(dplyr::select(benSamps_Filter_fin, StationID, BenSampID, RepNum, `Collection Date`) ,
              by = c('StationID', 'BenSampID', 'RepNum', 'Collection Date')) %>%

  # make year variable to make review easier
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, Season, BenSampID, RepNum, SCI, `SCI Score`, `SCI Threshold`,
                Gradient, `Target Count`,  `Sample Comments`, `Collected By`:`Entered Date`, everything()) %>% 
  arrange(Year, StationID, `Collection Date`)


# after speaking with PRO and NRO regional bios, the following SCI overrides are suggested
SCI_filter <- filter(SCI_filter, StationID != "1AXOR000.47") %>% 
  bind_rows(
    filter(VSCIresults, BenSampID %in% filter(benSamps_Filter_fin, StationID == "1AXOR000.47")$BenSampID) %>%
  mutate(SeasonGradient = as.factor(paste0(Season, " (",Gradient,")"))) %>%
  left_join(dplyr::select(benSamps_Filter_fin, StationID, BenSampID, RepNum, `Collection Date`) ,
              by = c('StationID', 'BenSampID', 'RepNum', 'Collection Date')) %>%

  # make year variable to make review easier
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, Season, BenSampID, RepNum, SCI, `SCI Score`, `SCI Threshold`,
                Gradient, `Target Count`,  `Sample Comments`, `Collected By`:`Entered Date`, everything()) %>% 
  arrange(Year, StationID, `Collection Date`)
  ) %>% 
  dplyr::select(Year, StationID, `Collection Date`, BenSampID, SCI, `SCI Score`) %>% # only actually need these metrics
  arrange(Year, StationID, `Collection Date`) 

# We report VSCI metrics regardless of SCI used for continuity 
SCImetrics <- filter(VSCIresults, BenSampID %in% SCI_filter$BenSampID) %>% 
  dplyr::select(BenSampID:`Fam %MFBI Score`)

SCIfinal <- left_join(SCI_filter, SCImetrics, by = c('BenSampID'))
```


While we are pulling data, NRO has requested we ensure the Po River trend site is only using VCPMI65. We will repull all the data for this site, average it, and smash it in to the final dataset later, overwriting previous benthic information.

```{r po river fix}

po <- filter(VCPMI65results, StationID == '8-POR015.70'& `Target Count` == 110) %>% 
  mutate(Year = year(`Collection Date`)) %>% 
  filter(Year %in% c(2004,2011,2013,2015,2017))#,2019))# just the samples associated with prob
# but we only report the VSCI metrics so grab those for these bensamps
poMetrics <- filter(VSCIresults, BenSampID %in% po$BenSampID)


# clean up workspace
rm(VSCIresults);rm(VCPMI63results);rm(VCPMI65results); rm(bioData2019);rm(bioData2020); rm(benSamps_Filter_fin)
rm(SCI_filter)
```


Now we can save both of these objects out and manually delete the erroneous rows in probData (things not close to benthic collections). We will average the station data in R after extra data is dropped.

```{r save for manual review}
#write.csv(probData, 'processedData/prob20192020data.csv', row.names = F, na = '')
#write.csv(SCI_filter, 'processedData/bio20192020data.csv', row.names = F, na = '')
```

### Habitat Data

Next we need to get the total habitat scores for each benthic sample.

```{r tot hab}
habSamps <- bind_rows(pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% x2019Sites & year(`Collection Date`) == 2019) ,
                      pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% x2020Sites & year(`Collection Date`) == 2020)) 
# make sure this is close to nrow SCI_filter
habValues <- pin_get('ejones/habValues', board = 'rsconnect') %>% 
  filter(HabSampID %in% habSamps$HabSampID) %>% 
  group_by(HabSampID) %>% 
  summarise(TotHab = sum(HabValue))

totHab <- left_join(habSamps, habValues, by = "HabSampID") %>% 
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, TotHab)
# we will do the join to benthic data and average by year farther down the script

#write.csv(totHab, 'processedData/totHab.csv', row.names = F, na = '')

# clean up workspace
rm(habSamps); rm(habValues)
```

### LRBS data

This analysis is run using C:\HardDriveBackup\R\GitHub\PhysicalHabitat\PHAB2021\Dec2021run.R and the TMDLsummary_2021-12-09.csv was copy/pasted into this project's processed Data directory.

Data is summarized and QAed below.

```{r lrbs qa}
names(prob2020) # get the variables we need to match

lrbs <- read.csv('processedData/MasterSummary_2021-12-09.csv') %>% #TMDLsummary_2021-12-09.csv') %>% 
  dplyr::select(StationID, Date, LRBS = LRBS2, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, 
                BL_CB_GR_Embed_PCT = BL_CB_GRmeanEmbed,
                Embed_PCT = Xembed ) %>% 
  mutate(Year = year(Date)) %>% 
  group_by(StationID, Year) %>% 
  summarise_at(vars(c(LRBS:Embed_PCT)), mean, na.rm = TRUE) 

# see what stations are missing data from this latest two years
View(left_join(WQM_Stations_Filter, lrbs, by = c('StationID', 'Year')))
# cool only the expected 2019 sites don't have lrbs

#write.csv(lrbs, 'processedData/FinalLRBS.csv', row.names = F, na = '')
```


### Geospatial data 

Watershed-specific landcover information is provided for all wadeable probmon sites each IR publication. In order to expedite the process of organizing and analyzing this geospatial dataset, automated scripts have been developed. This project calls these scripts. 

#### Tiger roads organization

Each year a new tiger road file is required to best compare sample year to roads in the watershed. Follow instructions in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsWebScrapingScript.R in order to scrape the FTP and organize each year into a single shapefile for analyses.

*** Remember: you need to first create a new directory named the year you are scraping (e.g. 2019) in the local C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsPull directory for downloadTigerRoadsByYear() and in unzipped directory to work properly.

#### Delineate new watersheds

The C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\Organize20192020prob.Rmd walks users through delineating wadeable and boatable sites sampled in 2019-2020 using automated streamstats delineation methods. Manual review is conducted in either R or ArcGIS. 

#### Landcover analysis

Once all the necessary stations (wadeable) have QAed watershed information, landcover analysis scripts in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\landcoverAnalysis_20192020.R take users through the automated process of generating the ~100 landcover metrics the prob chapter reports out for wadeable dataset.



## QA data

### Summarize Field/Analyte/Metals
Lucy cleaned the field/analyte/metals datasets and produced a final version with just the samples closest to the biological samples (if exist) per site. There are still S1/2 in there and all sites need to be averaged to a single row per year. I did this in the prob20192020data_EVJEdits sheet. Where there were R, S1, and S2 samples and missing data from the R line, I used S1 data to fill the missing fields from the same sample date. Where there are only S1 and S2 information, I created a new row and named it R and copied all S1 information for that date/time to be used. These changes are all highlighted in green in the .xlsx sheet.

We still need to average parameters to one result per year for CDF analyses. These steps occur in the next chunk.


### EVJ forgot TDS for Lucy's QA, so this is a special step to get TDS data back in before averaging. This will not be necessary in the future because the code is now fixed above to let TDS pass through from conventionals to manual review

```{r TDS smash}

lucy1 <- read_excel('processedData/prob20192020data_LKSEdits.xlsx', sheet = 'prob20192020data_EVJEdits', skip = 1) %>% # skip first row
  left_join(dplyr::select(probData, StationID:Ana_Sam_Mrs_Container_Id_Desc , TDS), 
            by = c("StationID", "Year", "FDT_DATE_TIME", "FDT_DEPTH"  , "FDT_SPG_CODE", 
                   "FDT_COMMENT" , "Ana_Sam_Mrs_Container_Id_Desc")) %>% 
  
left_join(dplyr::select(probData, StationID:Ana_Sam_Mrs_Container_Id_Desc , MetalCCU), 
            by = c("StationID", "Year", "FDT_DATE_TIME", "FDT_DEPTH"  , "FDT_SPG_CODE", 
                   "FDT_COMMENT" , "Ana_Sam_Mrs_Container_Id_Desc"))


#write.csv(lucy1, 'processedData/TDSfix.csv', row.names = F, na = '')
  rm(lucy1)
```

EVJ went in to sheet = prob20192020data_EVJEdits and put correct TDS info and cleaned up R/S1 information for that parameter as well. prob20192020data_EVJEdits is good to go for final data consolidation.



```{r one row per year}
lucy <- read_excel('processedData/prob20192020data_LKSEdits.xlsx', sheet = 'prob20192020data_EVJEdits', skip = 1) %>% # skip first row
  filter(! Ana_Sam_Mrs_Container_Id_Desc %in% c('S1', 'S2')) %>% 
  # make sure only one row per date/time
  group_by(StationID, FDT_DATE_TIME) %>% 
  mutate(n = n()) %>% 
  dplyr::select(n , everything()) %>% ungroup()
  # if everything fine, proceed
  
# can do everything but MERCURY and Hg-C here
lucyAvg <- lucy %>% 
  dplyr::select(-c(n, FDT_DATE_TIME, Ana_Sam_Mrs_Container_Id_Desc, FDT_SPG_CODE, FDT_COMMENT, FDT_DEPTH)) %>% ungroup() %>% 
  # get one row per StationID
  group_by(StationID, Year) %>% 
  summarise_at(vars(DO:HARDNESS), mean, na.rm = TRUE) %>% 
  # change NaN to NA
  pivot_longer(cols =DO:HARDNESS,names_to = 'Parameter', values_to = 'Value')
lucyAvg$Value[lucyAvg$Value== 'NaN'] <- NA
lucyAvg <- pivot_wider(lucyAvg, names_from = 'Parameter', values_from = 'Value')
  

lucyHg <- dplyr::select(lucy, StationID, Year, MERCURY, `Hg-C`) %>%  
  filter(!is.na(MERCURY) | !is.na(`Hg-C`)) %>% 
  group_by(StationID, Year) %>% 
  summarise(MERCURY = mean(MERCURY, na.rm = T),
            HgCom = unique(`Hg-C`)) 
# change any NaN's back to NA and review manually to make sure no data lost
lucyHg$MERCURY[lucyHg$MERCURY== 'NaN'] <- NA


# smash lucyAvg and lucyHg together to a single, clean dataset
lucyFin <- left_join(lucyAvg, lucyHg, by = c('StationID', 'Year'))


# save and manually review one last time
#write.csv(lucyFin, 'processedData/FinalFieldAnalyteMetals.csv', row.names = F, na= '')
# went through and manually checked all blanks, 0's, and random points to make sure data looks good

# clean up workspace
rm(lucyAvg);rm(lucy);rm(lucyHg); rm(lucyFin)
```

### Summarize Bugs

Now we need to convert the VCPMI scores to VSCI scale for analyses and make one row per bug record where more than one sample were collected per year to ensure data joins cleanly and CDF analysis can run.

```{r bugAvg and hab avg}
source('VCPMItoVSCIconversion.R')

# dataset for eastern bios to review to make sure all VSCI/VCPMI results are used correctly
#write.csv(filter(SCI_filter, `Collected By` %in% c("BRETT STERN", "MIKEJ SHAVER", "JUSTIND LOYD" )), 'processedData/VSCI_VCPMIreview.csv', row.names = F, na = '')

bugAvg <- SCIfinal %>% 
  full_join(totHab, by = c('StationID', 'Collection Date', 'Year')) %>% 
  
  # QA data
  # dplyr::select(-c(`Family Total Taxa`:`Fam %MFBI Score`)) %>% 
  # group_by(StationID, `Collection Date`) %>% 
  # mutate(n = n()) 
  
  # special step bc this station had habitat collected on different date than bugs
  mutate(TotHab = case_when(StationID == '4ALOR005.92' & BenSampID == 'LOR12689R110' ~ 98,
                            TRUE ~ TotHab)) %>% 
  filter(!is.na(SCI)) %>% 
  
  group_by(StationID, Year, SCI) %>% 
  summarise_at(vars(c(`SCI Score`, `Family Total Taxa`:TotHab)), mean, na.rm = TRUE) %>% 
  mutate(VSCIVCPMI = case_when(SCI == 'VSCI' ~ `SCI Score`,
                               SCI != 'VSCI' ~ VCPMItoVSCIconversion(`SCI Score`, probRatio),
                               TRUE ~ as.numeric(NA))) %>% 
  dplyr::select(StationID, Year, TotHab,
                TotTaxa = `Family Total Taxa`,
                EPTTax = `Family EPT Taxa`,
                VEphem = `%Ephem`,
                VPTHydropsychidae = `%PT - Hydropsychidae`,
                VScrap = `%FamilyScraper`,
                VChiro = `%Chironomidae Score`,
                V2Dom = `Family %2 Dominant`,
                HBI = `Family HBI`,
                #VHapto = ????????? # dropped from this report bc could not make joins to old EDAS taxa info easily after EDAS-CEDS data updates
                EPTInd = `Family EPT Taxa`,
                VSCIVCPMI)
  


#write.csv(bugAvg, 'processedData/FinalBugHabitatMetrics.csv', row.names = F, na = '')
```

And back to the Po fix, run the VCMPI conversion on those samples and combine the benthic metrics by year.

```{r po fix2}
# first get VSCI metrics associated with site and VCPMI65 score
poFix <- left_join(dplyr::select(poMetrics, BenSampID: `Fam %MFBI Score`, StationID),
                   dplyr::select(po, BenSampID, SCI, `SCI Score`, Year, StationID),
                   by = c('BenSampID',  'StationID')) %>% 
# then average everything by year
group_by(StationID, Year, SCI) %>% 
summarise_at(vars(c(`SCI Score`, `Family Total Taxa`:`Fam %MFBI Score`)), mean, na.rm = TRUE) %>% 
# then covert average VCPMI65 to VSCIVCPMI scale
  mutate( VSCIVCPMI = case_when(SCI == 'VSCI' ~ `SCI Score`,
                               SCI != 'VSCI' ~ VCPMItoVSCIconversion(`SCI Score`, probRatio),
                               TRUE ~ as.numeric(NA))) %>% 
  dplyr::select(StationID, Year,# TotHab,
                TotTaxa = `Family Total Taxa`,
                EPTTax = `Family EPT Taxa`,
                VEphem = `%Ephem`,
                VPTHydropsychidae = `%PT - Hydropsychidae`,
                VScrap = `%FamilyScraper`,
                VChiro = `%Chironomidae Score`,
                V2Dom = `Family %2 Dominant`,
                HBI = `Family HBI`,
                #VHapto = ????????? # dropped from this report bc could not make joins to old EDAS taxa info easily after EDAS-CEDS data updates
                EPTInd = `Family EPT Taxa`,
                VSCIVCPMI) 

rm(po); rm(poMetrics)
# clean up workspace
rm(probRatio); rm(SCIfinal); rm(bugAvg); rm(SCImetrics); rm(totHab); rm(ecoregion)

```

Now overwrite these results for all previous Po results.

```{r Po Overwrite}
prob2020new <- filter(prob2020, StationID != '8-POR015.70')
prob2020Po <- filter(prob2020, StationID == '8-POR015.70') %>% 
  dplyr::select(-c(TotTaxa:VSCIVCPMI)) %>% 
  mutate(VHapto = NA) %>% # placeholder
  left_join(poFix, by = c('StationID', 'Year')) %>% 
  dplyr::select(names(prob2020new))

prob2020 <- bind_rows(prob2020new, prob2020Po) %>% 
  arrange(Year, StationID)

#write.csv(prob2020, 'prob2020PoFix.csv', row.names = F, na = '')

rm(prob2020new); rm(prob2020Po); rm(poFix)
```



# Combine data


Bring in corrected prob2020 with Po river fix.

```{r prob2020}
prob2020 <- read.csv('prob2020PoFix.csv') %>% 
   rename('Ortho-P' = 'Ortho.P',
         'Hg-C' = 'Hg.C', 
         '70331VFine' = 'X70331VFine')
```


Bring in Field, Analyte, and Metals Data

```{r field}
field <- read.csv('processedData/FinalFieldAnalyteMetals.csv') %>% 
  rename('Ortho-P' = 'Ortho.P',
         'Hg-C' = 'HgCom', 
         '70331VFine' = 'X70331VFine')

```

Bring in Bug and habitat data

```{r bug}
bugs <- read.csv('processedData/FinalBugHabitatMetrics.csv') 
```


Bring in LRBS data

```{r lrbs}
lrbs <- read.csv('processedData/FinalLRBS.csv')
```

Bring in landcover data

```{r landcover}
landcover <- read.csv('C:/HardDriveBackup/R/GitHub/LandcoverAnalysis/Results/ProbWadeable20192020/Result10.csv')
```

Smash data

```{r data smash}

# we are dropping %haptobenthos metric this cycle so get it out of the dataset we are trying to match
prob2020fin <- prob2020 %>% 
  dplyr::select(-VHapto) %>% 
  mutate(IR2022 = 1) %>% 
  dplyr::select(DataSource:IR2020, IR2022, DO:STXRD )
names(prob2020fin) # what to match
rm(prob2020)

finalSmash <- dplyr::select(WQM_Stations_Filter, DataSource:set) %>% 
  # add in population information later
  left_join(field, by = c('StationID', 'Year')) %>% 
  left_join(lrbs, by = c('StationID', 'Year')) %>% 
  left_join(bugs, by = c('StationID', 'Year')) %>% 
  left_join(landcover %>% 
              mutate(Year = YearSampled),
            by = c('StationID', 'Year'))


names(prob2020fin)[! names(prob2020fin) %in% names(finalSmash)]
    
# clean up workspace
rm(bugs);rm(field);rm(landcover);rm(lrbs);rm(metalsCCUresults);rm(probData)
```




```{r}
names(prob2020fin)[! names(prob2020fin) %in% names(finalSmash)]

```


## Attach final metadata


First, where possible, get metadata from previous years.

```{r grab previous metdata}
# get what you can from previous metadat
previousInfo <- filter(prob2020fin, StationID %in% finalSmash$StationID) %>% 
  distinct(StationID, .keep_all = T)
previousInfoMetadata <-  filter(prob2020fin, StationID %in% previousInfo$StationID) %>%  
                                  dplyr::select(StationID, Basin, SubBasin, BayShed, EcoRegion, 
                                                BioRegion, Order, BasinSize, StreamSizeCat) %>% 
                                  distinct(StationID, .keep_all = T)
```

Use pinned data to get other basic metadata.

```{r new sites}
finalSmashMetadata <- filter(finalSmash, ! StationID %in% previousInfoMetadata$StationID) %>% 
  left_join(pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect') %>%
              filter(StationID %in% finalSmash$StationID) %>% 
              dplyr::select(StationID, Basin, US_L3NAME, Basin_Code), 
            by = 'StationID') %>% 
  mutate(Basin = case_when(Basin %in% c("James River Basin" ) ~ "James",
                           Basin %in% c("Roanoke River Basin" ) ~ "Roanoke",
                           Basin %in% c( "New River Basin"  ) ~ "New",
                           Basin %in% c( "Potomac River Basin" , "Shenandoah River Basin") ~ "Potomac-Shenandoah",
                           Basin %in% c("Tennessee and Big Sandy River Basin" ) ~  "Tennessee",
                           Basin %in% c("Rappahannock River Basin", "York River Basin" ) ~ "Rappahannock-York",
                           Basin %in% c(  "Chowan and Dismal Swamp River Basin"  ) ~   "Chowan" ,
                           TRUE ~ as.character(NA)),
         EcoRegion = case_when(US_L3NAME %in% c("Ridge and Valley" ) ~ "Central Appalachian Ridges and Valleys",
                               US_L3NAME %in% c("Blue Ridge" ) ~ "Blue Ridge Mountains",
                               TRUE ~ as.character(US_L3NAME)),
         SubBasin =  case_when(Basin_Code %in% c("James-Lower", "James-Middle", "James-Upper","Appomattox"  ) ~ "James",   
                               Basin_Code %in% c("Roanoke", "Yadkin") ~ "Roanoke",
                               Basin_Code %in% c( "New"  ) ~ "New",
                               Basin_Code %in% c(  "Potomac-Lower") ~  "Potomac" ,
                               Basin_Code%in% c("Potomac-Shenandoah") ~  "Shenandoah",
                               Basin_Code %in% c("Rappahannock") ~ "Rappahannock",
                               Basin_Code%in% c("York"  ) ~  "York" ,
                               Basin_Code %in% c("Chowan-Dismal" ) ~ "Chowan",
                               Basin_Code %in% c("Tennessee-Holston" ) ~  "Holston",
                               Basin_Code  %in% c( "Tennessee-Clinch"  ) ~   "Clinch-Powell" ,
                               Basin_Code  %in% c( "Tennessee-Big Sandy") ~  "Big Sandy",
                               # hardly ever used "Cheasapeake Bay"
                               TRUE ~ as.character(NA)),
         BayShed = case_when(Basin %in% c("James River Basin","Potomac River Basin" , "Shenandoah River Basin",
                                          "Rappahannock River Basin", "York River Basin") ~ "Bay",
                             TRUE ~ as.character("NonBay")),
         BioRegion = case_when(EcoRegion %in% c("Middle Atlantic Coastal Plain", "Southeastern Plains") ~ "Coast",
                               EcoRegion %in% c("Piedmont", "Northern Piedmont") ~ "Piedmont",
                               TRUE ~ as.character('Mountain')),
         StationID_Trend = StationID,
         Order = weightcategory,
         AREA_SQ_MILES = totalArea_sqMile,
         BasinSize = case_when(AREA_SQ_MILES < 1 ~ 1,
                               between(AREA_SQ_MILES, 1, 9.99999999) ~ 2, #between is >= and <= 
                               between(AREA_SQ_MILES, 10, 49.9999999) ~ 3, #between is >= and <= 
                               AREA_SQ_MILES >= 50 ~ 4,
                               TRUE ~ as.numeric(NA)),
         StreamSizeCat = case_when(BasinSize %in% c(1,2) ~ 'Small',
                                   BasinSize == 3 ~ "Medium",
                                   BasinSize == 4 ~ "Large",
                                   TRUE ~ as.character(NA))) %>% 
  dplyr::select(DataSource:set, Basin, SubBasin, BayShed, EcoRegion, BioRegion, Order, BasinSize,
                StreamSizeCat,AREA_SQ_MILES, DO:STXRD) %>% 
  bind_rows(
     filter(finalSmash, StationID %in% previousInfoMetadata$StationID) %>% 
       left_join(previousInfoMetadata, by = 'StationID') %>% 
       mutate(AREA_SQ_MILES = totalArea_sqMile) %>% 
       dplyr::select(DataSource:set, Basin, SubBasin, BayShed, EcoRegion, BioRegion, Order, BasinSize,
                     StreamSizeCat, AREA_SQ_MILES, DO:STXRD)
     )


names(prob2020fin)[! names(prob2020fin) %in% names(finalSmashMetadata)]

```

These are the data windows we are looking at for 2022 IR:

* Full sample window (2001-2020)
* 2022 IR (2015-2020)
* 2020 IR (2013-2018)
* 2018 IR (2011-2016)
* 2016 IR (2009-2014)
* 2014 IR (2007-2012)
* 2012 IR (2005-2010)
* 2010 IR (2003-2008)
* 2008 IR (2001-2007)
* Panels (Phase 1= 2001-2009; Phase 2= 2010-2020) Kept Phase 1 same as 2018IR to balance n samples in window
* BioPanels (Phase 1 = 2001-2005; Phase 2 = 2006-2010; Phase 3 = 2011-2015; Phase 4 = 2016-2020)


```{r temporal metadata}
finalSmashMetadata <- mutate(finalSmashMetadata,
                            IR2008 = case_when(between(Year, 2001, 2007)~ 2008, TRUE ~ as.numeric(NA)),
                            IR2010 = case_when(between(Year, 2003, 2008)~ 2010, TRUE ~ as.numeric(NA)),
                            IR2012 = case_when(between(Year, 2005, 2010)~ 2012, TRUE ~ as.numeric(NA)),
                            IR2014 = case_when(between(Year, 2007, 2012)~ 2014, TRUE ~ as.numeric(NA)),
                            IR2016 = case_when(between(Year, 2009, 2014)~ 2016, TRUE ~ as.numeric(NA)),
                            IR2018 = case_when(between(Year, 2011, 2016)~ 2018, TRUE ~ as.numeric(NA)),
                            IR2020 = case_when(between(Year, 2013, 2018)~ 2020, TRUE ~ as.numeric(NA)),
                            IR2022 = case_when(between(Year, 2015, 2020)~ 2022, TRUE ~ as.numeric(NA)),
                            Panel = case_when(between(Year, 2001, 2009)~ 'Phase1',
                                              between(Year, 2010, 2020)~ 'Phase2',
                                              TRUE ~ as.character(NA)),
                            BioPanel = case_when(between(Year, 2001, 2005)~ 'Phase1',
                                              between(Year, 2006, 2010)~ 'Phase2',
                                              between(Year, 2011, 2015)~ 'Phase3',
                                              between(Year, 2016, 2020)~ 'Phase4',
                                              TRUE ~ as.character(NA)),
                            BayPanel = paste0(BayShed, Panel),
                            StreamSizeCatPhase = paste0(StreamSizeCat, Panel),
                            DataSource = "State ProbMon") %>% # change region info to State ProbMon 
  dplyr::select(names(prob2020fin))


names(prob2020fin)[! names(prob2020fin) %in% names(finalSmashMetadata)]


```
## Smash old and new data

```{r old and new}
prob2022 <- bind_rows(prob2020fin, finalSmashMetadata) %>% 
  arrange(Year, StationID)
```


## New for 2022, add VAHUSB

Some of our larger basins (looking at you James and Roanoke) are ready for refinement.

```{r VAHUSB}
# not all stations have VAHUSB info in this table??
# WQMstations <- pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect') %>% 
#   filter(StationID %in% prob2022$StationID) %>% 
#   dplyr::select(StationID, VAHUSB)


# prob2022 <- left_join(prob2022, WQMstations, by = "StationID") %>% 
#   dplyr::select(DataSource:SubBasin, VAHUSB, BayShed:STXRD)
```
 

Make the vahusb dataset

```{r}
# vahusb <- st_read('C:/HardDriveBackup/GIS/Assessment/AssessmentRegions_VA84_basins.shp') %>% 
#   group_by(VAHUSB) %>% 
#   summarise() %>% 
#   mutate(VAHUSB_NAME = case_when(VAHUSB == 'AO' ~ 'Atlantic Ocean Coastal',
#                                  VAHUSB == 'AS' ~ 'Albemarle Sound',
#                                  VAHUSB == 'BS' ~ 'Big Sandy River',
#                                  VAHUSB == 'CB' ~ 'Chesapeake Bay/Chesapeake Bay Coastal',
#                                  VAHUSB == 'CL' ~ 'Chowan River, Lower',
#                                  VAHUSB == 'CM' ~ 'Chowan River-Meherrin River',
#                                  VAHUSB == 'CU' ~ 'Chowan River, Upper',
#                                  VAHUSB == 'JA' ~ 'James River- Appomattox River',
#                                  VAHUSB == 'JL' ~ 'James River, Lower (Tidal)',
#                                  VAHUSB == 'JM' ~ 'James River, Middle (Piedmont)',
#                                  VAHUSB == 'JR' ~ 'James River- Rivanna River',
#                                  VAHUSB == 'JU' ~ 'James River, Upper (Mountain)',
#                                  VAHUSB == 'NE' ~ 'New River',
#                                  VAHUSB == 'PL' ~ 'Potomac River, Lower',
#                                  VAHUSB == 'PS' ~ 'Potomac River-Shenandoah River',
#                                  VAHUSB == 'PU' ~ 'Potomac River, Upper',
#                                  VAHUSB == 'RA' ~ 'Rappahannock River',
#                                  VAHUSB == 'RD' ~ 'Roanoke River- Dan River',
#                                  VAHUSB == 'RL' ~ 'Roanoke River, Lower',
#                                  VAHUSB == 'RU' ~ 'Roanoke River, Upper',
#                                  VAHUSB == 'TC' ~ 'Tennessee-Clinch River',
#                                  VAHUSB == 'TH' ~ 'Tennessee-Holston River',
#                                  VAHUSB == 'TP' ~ 'Tennessee-Powell River',
#                                  VAHUSB == 'YA' ~ 'Yadkin River-Ararat River',
#                                  VAHUSB == 'YO' ~ 'York River',
#                                  TRUE ~ as.character(NA)))
# 
# View(vahusb %>% st_drop_geometry())
# 
# #st_write(vahusb, 'C:/HardDriveBackup/GIS/Assessment/VAHUSB.shp')
``` 
Spatially join prob2022 to this VAHUSB dataset and 4th order watershed as well

```{r new basin analyses}
library(sf)


vahusb <- st_read('C:/HardDriveBackup/GIS/Assessment/VAHUSB.shp') %>% 
  rename(VAHUSB_NAME = VAHUSB_)
wshd4Order <- st_read("C:/HardDriveBackup/GIS/GIS_BaseLayer_Datasets.gdb",'VA_SUBBASINS_4TH_ORDER_STG')

prob2022sf <- prob2022 %>% 
    st_as_sf(coords = c("LongitudeDD", "LatitudeDD"), 
               remove = F, # don't remove these lat/lon cols from df
               crs = 4326) %>% # add projection, needs to be geographic for now bc entering lat/lng
  st_intersection(vahusb) 

# after careful review, this step was dropped because a station was lost in this intersection and the n in each category didn't prove useful for further analysis
#%>% 
  #st_intersection(dplyr::select(wshd4Order, Basin4thOrder = NAME)) 

prob2022sf$StationID[!prob2022sf$StationID %in% prob2022$StationID]

prob2022fin <- prob2022sf %>% 
  dplyr::select(DataSource:SubBasin, VAHUSB, VAHUSB_NAME,# Basin4thOrder, 
                BayShed:STXRD) %>% 
  arrange(Year, StationID) %>% 
  st_drop_geometry()
```

 

What's the breakdown of these new categories?

```{r VAHUSB summary}
prob2022fin %>% 
  group_by( VAHUSB, VAHUSB_NAME) %>% 
  count()%>% 
  arrange(desc(n))

# prob2022fin %>% 
#   group_by( Basin4thOrder) %>% 
#   count() %>% 
#   arrange(desc(n))
# # too many levels, looking for at least 25-30 sites to run cdf analyses


#prob2022fin <- dplyr::select(prob2022fin, -Basin4thOrder)
```






```{r save data for review}

write.csv(prob2022fin, 'processedData/Wadeable_ProbMon_2001-2020.csv', row.names = F, na  ='')

write.csv(prob2022fin %>% 
  group_by(Basin) %>% 
  count(), 'processedData/summaries/BasinSummary.csv', row.names = F, na  ='')


write.csv(prob2022fin %>% 
  group_by(SubBasin) %>% 
  count, 'processedData/summaries/SubBasinSummary.csv', row.names = F, na  ='')

write.csv(prob2022fin %>% 
  group_by(VAHUSB) %>% 
  count(), 'processedData/summaries/VAHUSBSummary.csv', row.names = F, na  ='')

# write.csv(prob2022fin %>% 
#   group_by(Basin4thOrder) %>% 
#   count(), 'processedData/summaries/Basin4OrderSummary.csv', row.names = F, na  ='')
```






# Design Status update

We also want to add back in the full design status sites for an updated design status dataset.

Note the sampleID field uses the StationID_Trend information to ensure each station has a unique name for CDF analysis purposes.

```{r}
designStatusOld <- read_excel('C:/HardDriveBackup/R/GitHub/FreshwaterProbMonIntegratedReports/2020ProbChapter/processedData/biology20012018final.xlsx', sheet = 'biology2020') %>% 
  mutate(IR2022 = NA, VAHUSB = NA, VAHUSB_NAME = NA,
         IR2008 = case_when(IR2008 == 2008 ~ 2008, TRUE ~ as.numeric(NA)),
         IR2010 = case_when(IR2010 == 2010 ~ 2010, TRUE ~ as.numeric(NA)),
         IR2012 = case_when(IR2012 == 2012 ~ 2012, TRUE ~ as.numeric(NA)),
         IR2014 = case_when(IR2014 == 2014 ~ 2014, TRUE ~ as.numeric(NA)),
         IR2016 = case_when(IR2016== 2016 ~ 2016, TRUE ~ as.numeric(NA)),
         IR2018 = case_when(IR2018 == 2018 ~ 2018, TRUE ~ as.numeric(NA)),
         IR2020 = case_when(IR2020 == 2020 ~ 2020, TRUE ~ as.numeric(NA)))



designStatusNew <- read_excel('originalData/Wadeable_ProbMon_2019-2020_Final.xlsx', 
                              sheet = 'All20192020StationInfo') %>% 
  dplyr::select(StationID:set) %>% 
  left_join(dplyr::select(prob2022fin, StationID, Year, #`strahler order` = Order, 
                          #`Longitude-DD` =  LongitudeDD, `Latitude-DD` = LatitudeDD, 
                          Basin:IR2022, Order, `weight category` = weightcategory) %>% 
              mutate(`strahler order` = Order),
                          # stratum, designweight, `weight category` = weightcategory,
                          # station, # will overwrite later,
                          # state:set), 
            by = c('StationID', 'Year')) %>%
  mutate(sampleID = coalesce(StationID_Trend, StationID)) %>% # use StationID_Trend where available
  rename(`Longitude-DD` =  LongitudeDD, `Latitude-DD` = LatitudeDD) %>% 
  dplyr::select(names(designStatusOld))

# add subbasin info
designStatusOldSampled <- filter(designStatusOld, status == 'TS') %>% 
  dplyr::select(-c(VAHUSB, VAHUSB_NAME)) %>% 
    st_as_sf(coords = c("Longitude-DD", "Latitude-DD"), 
               remove = F, # don't remove these lat/lon cols from df
               crs = 4326) %>% # add projection, needs to be geographic for now bc entering lat/lng
  st_intersection(vahusb) %>% 
  rename(`strahler order` =strahler.order,
         `Longitude-DD` = Longitude.DD, `Latitude-DD` = Latitude.DD,
         `weight category` = weight.category) %>% 
  arrange(Year, sampleID) %>% 
  st_drop_geometry()

designStatusFin <- bind_rows(designStatusOldSampled,
                             filter(designStatusNew, status == 'TS'),
                             filter(designStatusOld, status != 'TS'),
                             filter(designStatusNew, status != 'TS')) %>% 
  mutate(station = 1:n()) %>% 
  dplyr::select(sampleID:SubBasin, VAHUSB, VAHUSB_NAME,BayShed:IR2022)

#write.csv(designStatusFin, 'processedData/designStatusIR2022.csv', row.names = F, na = '')

```
