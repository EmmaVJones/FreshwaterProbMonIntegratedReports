---
title: "Data Acquisition"
author: "Emma Jones"
date: "10/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(config)
library(sf)
library(lubridate)
library(pool)
library(pins)
library(sqldf)
library(dbplyr)
library(readxl)

```

## Background

This script connects to the ODS environment, pulls data for assessment window, and summarizes results. Historically, field and analyte information has been reduced to one record per sample year, even if the station was sampled more than once (e.g. spring and fall). The summary statistic used is the median for this report.

## Connect to ODS

Make sure pulling from production environment and not test since those are not exact copies.

```{r connect to ODS}

# Server connection things
conn <- config::get("connectionSettings") # get configuration settings


board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## Connect to ODS production
pool <- dbPool(
 drv = odbc::odbc(),
 Driver = "ODBC Driver 11 for SQL Server",#"SQL Server Native Client 11.0",
 Server= "DEQ-SQLODS-PROD,50000",
 dbname = "ODS",
 trusted_connection = "yes"
)
```

## Bring in final data format to match

Below is the final dataset for the 2020 IR that we will aim to match for 2022 IR.

```{r prob 2020}
prob2020 <- read_csv('originalData/Wadeable_ProbMon_2001-2018_Final_Final.csv')
```

Generally speaking, we need to hit a few data sources to acquire all the data we need to publish for final chapter:
 - Field Data (Wqm_Field_Data_View) for DO, pH, SpCond
 - Analyte Data (Wqm_Analyte_Data_View) for TN, TP, TDS, NH4, NO3, TKN, Ortho-P, Turb, TSS, Na, K, Cl, Sf, 70331VFine, SSCCOARSE, SSCFINE, SSCTOTAL
 - PHAB database (still in Access) for LRBS, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, BL_CB_GR_Embed_PCT, Embed_PCT
 - EDAS (pinned (pre calculated) habitat data on server) for TotHab
 - EDAS (pinned (pre calculated) benthic data on server) for TotTaxa, EPTTax, VEphem, VPTHydropsychidae, VScrap, VChiro, V2Dom, HBI, VHapto, EPTInd, VSCIVCPMI (calculated conversion)
 - Analyte Data (Wqm_Analyte_Data_View) for dissolved metals MetalCCU (calculated), ARSENICppm, BERYLLIUMppm, CADMIUMppm, CHROMIUMppm, COPPERppm, LEADppm, MANGppm, NICKELppm, SILVERppm, ZINCppm, ANTIMONYppm, ALUMINUMppm, SELENIUMppm, IRONppm, MERCURYppm, THALLIUMppm, CALCIUM, MAGNESIUM, ARSENIC, BARIUM, BERYLLIUM, CADMIUM, CHROMIUM, COPPER, IRON, LEAD, MANGANESE, THALLIUM, NICKEL, SILVER, ZINC, ANTIMONY, ALUMINUM, SELENIUM, HARDNESS, MERCURY, Hg-C
 - Special GIS dataset for watershed spatial info 


## Bring in sites

```{r 2020 final sites}
WQM_Stations_Filter <- read_excel('originalData/Wadeable_ProbMon_2019-2020_Final.xlsx', sheet = 'All20192020StationInfo') %>% 
  filter(status == 'TS') # keep only wadeable sites that were sampled for data querying
  # read_excel('originalData/20192020FP.xlsx', sheet = '2019') %>% 
  #       dplyr::select(StationID = Sta_Id) %>% 
  #       bind_rows(
  #               read_excel('originalData/20192020FP.xlsx', sheet = '2020') %>% 
  #                       dplyr::select(StationID = Sta_Id)       ) # %>% 
  #filter(StationID == '3-MTN021.11')#1AXOR000.47')#2-BGC008.10')


```


## Query data

Using the current build of the conventionals function to query and organize field, analyte, and metals data consistently. Field and analyte methods pulled from CEDS WQM data query tool.

### Query Terms

These presets allow correct data retrieval. 

```{r query terms}

# Basic station info for conventionals
multiStationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()
multiStationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()

# make sure all stations are in CEDS
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationInfo$Sta_Id]
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationGIS_View$Station_Id]

```


### Field data

```{r field data}
#dateRange_multistation <- c(as.Date('2020-01-01'), as.Date('2020-12-31'))# c(as.Date('2019-02-05'), as.Date('2019-02-08'))# 
x2019Sites <- filter(WQM_Stations_Filter, Year == 2019)$StationID
x2020Sites <- filter(WQM_Stations_Filter, Year == 2020)$StationID

# do as two steps to make sure only bring back data from desired window for each site
multistationFieldData <- bind_rows(
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2019Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2019-01-01'), as.Date('2019-12-31'))) %>% # & # x >= left & x <= right
    as_tibble(),
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2020Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2020-01-01'), as.Date('2020-12-31'))) %>% # & # x >= left & x <= right
    as_tibble()) %>% 
  filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")

  # filter(Fdt_Sta_Id %in% !! WQM_Stations_Filter$StationID &
  #          between(as.Date(Fdt_Date_Time), !! dateRange_multistation[1], !! dateRange_multistation[2])) %>% # & # x >= left & x <= right
           #Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>%  # don't drop QA failure on SQL part bc also drops any is.na(Ssc_Description)
  # as_tibble() %>% 
  # filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")
```

### Analyte Data

```{r analyte data}
# can do this in one step since we are searching for Fdt_Id's from appropriate field data windows
multistationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! multistationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange_multistation[1], !! dateRange_multistation[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>% 
  as_tibble() %>%
  left_join(dplyr::select(multistationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

```


### Organize by Conventionals logic

To consistently organize field, analyte, and metals data it is prudent to use the "conventionals" data rules.

```{r conventionals organization of raw data}
source('C:/HardDriveBackup/R/GitHub/WQMdataQueryTool/conventionalsFunction1232020.R')

conventionalsList <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                                          stationFieldDataUserFilter= multistationFieldData, 
                                          stationAnalyteDataUserFilter = multistationAnalyteData, 
                                          stationInfo = multiStationInfo,
                                          stationGIS_View = multiStationGIS_View,
                                          dropCodes = c('QF'),
                                          assessmentUse = F) 
conventionals <- conventionalsList$More %>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)

#write.csv(conventionals, '2020conventionals2.csv', row.names = F, na = "")
```

### QA data with Lucy

In order to make sure we are pulling the same information from Logi and R-ODS connection, the following script walks users through verification. Lucy pulled 2020 data using Logi.

```{r convetionals QA}
source('conventionalsQA.R')
```

Looks good for parameters matching what logi pulls. 

Now quick QA test for EB information. What we want is to make sure that none of the blanks have too high values, indicating too much uncertainty with respect to units and potentially exceeding standards.

```{r QA conventionals EB}
QAconventionals <- conventionals %>% 
  filter(Ana_Sam_Mrs_Container_Id_Desc == 'EB') %>% 
  dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, NITROGEN_mg_L:RMK_82079) %>% 
  dplyr::select(!contains('LEVEL_'))

# two steps bc can't combine character and numeric entries in a pivot longer
QAconventionals <- left_join(
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, ! contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols =- c(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'Parameter', values_to = 'Measure', values_drop_na = T),
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc,  contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols =- c(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = '', values_to = 'Measure', values_drop_na = T) ,
  by = c("FDT_STA_ID", "FDT_DATE_TIME", "FDT_SPG_CODE", "Ana_Sam_Mrs_Container_Id_Desc"))
  
  QAconventionalsChar <-  

group_by(FDT_STA_ID, FDT_DATE_TIME) %>% 
  mutate(n = n()) %>% 
  filter(n > 1) %>% 
  dplyr::select(n,FDT_STA_ID,	FDT_DATE_TIME,	FDT_DEPTH,	FDT_SPG_CODE,	Ana_Sam_Mrs_Container_Id_Desc:RMK_82079, everything()) %>% 
  arrange(FDT_STA_ID,	FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc)

# QAconventionals <- conventionals %>% 
#   group_by(FDT_STA_ID, FDT_DATE_TIME) %>% 
#   mutate(n = n()) %>% 
#   filter(n > 1) %>% 
#   dplyr::select(n,FDT_STA_ID,	FDT_DATE_TIME,	FDT_DEPTH,	FDT_SPG_CODE,	Ana_Sam_Mrs_Container_Id_Desc:RMK_82079, everything()) %>% 
#   arrange(FDT_STA_ID,	FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc)

#write.csv(QAconventionals, 'test.csv', row.names = F, na="")
```


For reporting purposes, we don't want to use the raw data (even though Roger has confidence in those numbers). We want to back lab reported values back to 0.001 if they fall below that. This is the first report we are pulling uncensored data. We need to repull and organize all years of data with uncensored and censored and rerun statistics for both (some programs e.g. permitting) may prefer censored data but uncensored could offer smoother CDF curves (remove blocky detection limits that have improved over last 20+ years).

Jason and I have determined that using uncensored this round is okay because we are only cleaning up the lower end of the dataset and are not coming close to potential standards violations.

```{r back to positive}
conventionals2 <- conventionals
conventionals2[conventionals2 < 0] <- 0.001  
# double check things worked
summary(conventionals)
summary(conventionals2) # make sure columns with negative mins have gone to 0.001

#Change longitude back to real numbers
conventionals2 <- left_join(conventionals2,
                            dplyr::select(conventionals, FDT_STA_ID, Longitude) %>% 
                              distinct(FDT_STA_ID, .keep_all = T),
                            by = 'FDT_STA_ID') %>% 
  mutate(Longitude.x = Longitude.y) %>% 
  rename(Longitude = Longitude.x) %>% 
  dplyr::select(-Longitude.y)
```

Next we need to change the data for manual QA to the probmon format. We will also drop EB's at this point.


```{r prob data format}

probData <- conventionals2 %>%
  filter(Ana_Sam_Mrs_Container_Id_Desc != 'EB') %>% 
  mutate(StationID = FDT_STA_ID, 
         Year = year(FDT_DATE_TIME),
              DO = DO_mg_L, 
              pH = FDT_FIELD_PH,
              SpCond = FDT_SPECIFIC_CONDUCTANCE, 
              TN = NITROGEN_mg_L, 
              TP = PHOSPHORUS_mg_L, 
              NH4 = AMMONIA_mg_L, 
              NO3 = NITRATE_mg_L, 
              TKN = NITROGEN_KJELDAHL_TOTAL_00625_mg_L,
              `Ortho-P` = PHOSPHORUS_TOTAL_ORTHOPHOSPHATE_70507_mg_L,
              Turb = `TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU`,
              TSS = TSS_mg_L, 
              Na = `SODIUM, DISSOLVED (MG/L AS NA)`, 
              K = `POTASSIUM, DISSOLVED (MG/L AS K)`,
              Cl = CHLORIDE_mg_L,
              Sf = SULFATE_mg_L,
              `70331VFine` = `SSC%Finer`,
              SSCCOARSE = SSC_COARSE,
              SSCFINE =  SSC_FINE, 
              SSCTOTAL = SSC_TOTAL,
              # sediment ppm data hasn't been collected since 2011
              ARSENICppm = as.numeric(NA),
              BERYLLIUMppm = as.numeric(NA),
              CADMIUMppm = as.numeric(NA),
              CHROMIUMppm = as.numeric(NA),
              COPPERppm = as.numeric(NA),
              LEADppm = as.numeric(NA),
              MANGppm = as.numeric(NA),
              NICKELppm = as.numeric(NA),
              SILVERppm = as.numeric(NA),
              ZINCppm = as.numeric(NA),
              ANTIMONYppm = as.numeric(NA),
              ALUMINUMppm = as.numeric(NA),
              SELENIUMppm = as.numeric(NA),
              IRONppm = as.numeric(NA),
              MERCURYppm = as.numeric(NA),
              THALLIUMppm = as.numeric(NA),
              CALCIUM = `CALCIUM, DISSOLVED (MG/L AS CA)`,
              MAGNESIUM = `MAGNESIUM, DISSOLVED (MG/L AS MG)`,
              ARSENIC = `ARSENIC, DISSOLVED  (UG/L AS AS)`,
              BARIUM = `BARIUM, DISSOLVED (UG/L AS BA)`,
              BERYLLIUM = `BERYLLIUM, DISSOLVED (UG/L AS BE)`,
              CADMIUM = `CADMIUM, DISSOLVED (UG/L AS CD)`,
              CHROMIUM = `CHROMIUM, DISSOLVED (UG/L AS CR)`,
              COPPER = `COPPER, DISSOLVED (UG/L AS CU)`,
              IRON = `IRON, DISSOLVED (UG/L AS FE)`,
              LEAD = `LEAD, DISSOLVED (UG/L AS PB)`,
              MANGANESE = `MANGANESE, DISSOLVED (UG/L AS MN)`,
              THALLIUM = `THALLIUM, DISSOLVED (UG/L AS TL)`,
              NICKEL = `NICKEL, DISSOLVED (UG/L AS NI)`,
              SILVER = `SILVER, DISSOLVED (UG/L AS AG)`,
              ZINC = `ZINC, DISSOLVED (UG/L AS ZN)`,
              ANTIMONY = `ANTIMONY, DISSOLVED (UG/L AS SB)`,
              ALUMINUM = `ALUMINUM, DISSOLVED (UG/L AS AL)`,
              SELENIUM = `SELENIUM, DISSOLVED (UG/L AS SE)`,
              HARDNESS = `HARDNESS, CA MG CALCULATED (MG/L AS CACO3) AS DISSOLVED`,
              MERCURY = `MERCURY-TL,FILTERED WATER,ULTRATRACE METHOD NG/L`,
              `Hg-C` = `RMK_50091`) %>% 
  dplyr::select(FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, Ana_Sam_Mrs_Container_Id_Desc, #other helpful info
                any_of(names(prob2020))) %>% 
  dplyr::select(StationID, Year, FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, 
                Ana_Sam_Mrs_Container_Id_Desc, everything()) %>% 
  arrange(Year, StationID, FDT_DATE_TIME)

```


Last, we need to remove extra samples not associated with FP/bio sampling events but not drop too much in case other programs grabbed samples on a different date. This is still best done manually. For 2022IR, Lucy manually screened data for just spring/fall sample information (using excel).

To do this, we need benthic data.

### Benthic Data

Use pinned benthic data to pull appropriate SCI information for sites.

```{r get ecoregion for SCI}
ecoregion <- pin_get("ejones/WQM-Stations-Spatial", board = "rsconnect") %>% 
        filter(StationID %in% WQM_Stations_Filter$StationID) %>% 
  dplyr::select(StationID, US_L3CODE, Basin)
```

Get all SCI information from pinned data.

```{r SCI pins}
VSCIresults <- pin_get("ejones/VSCIresults", board = "rsconnect")
VCPMI63results <- pin_get("ejones/VCPMI63results", board = "rsconnect")
VCPMI65results <- pin_get("ejones/VCPMI65results", board = "rsconnect")
```

Bring in correct BenSampID info for sites.

```{r}
# x2019Sites <- filter(WQM_Stations_Filter, Year == 2019)$StationID
# x2020Sites <- filter(WQM_Stations_Filter, Year == 2020)$StationID

bioData2019 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2019Sites & year(`Collection Date`) == 2019) 
bioData2020 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2020Sites & year(`Collection Date`) == 2020) 

# make sure no sites are missing biological data (our key to using a site)
x2019Sites[! x2019Sites %in% bioData2019$StationID]
x2020Sites[! x2020Sites %in% bioData2020$StationID]

benSamps_Filter_fin <- bind_rows(bioData2019, bioData2020) %>% 
  filter(str_detect(BenSampID, 'R110')) %>% # only keep rarified data
  filter(RepNum == 1) %>% # only use Rep1
  left_join(ecoregion, by = 'StationID')
```

Grab raw benthic data so we can add in the % haptobenthos metric that isn't in basic SCI data.

```{r raw benthic}
rawBio <- pin_get("ejones/benthics", board = "rsconnect") %>% 
  #filter(BenSampID %in% benSamps_Filter_fin$BenSampID) %>% 
  filter(BenSampID %in% c('FAMACO4564')) %>% #'ABR3814', 'ABR3814R110')) %>% 
  
  # join in master taxa list to get habit info
  left_join(pin_get('ejones/masterTaxaGenus', board = 'rsconnect') %>% 
              dplyr::select(FinalID, FamHabit),
            by = 'FinalID') %>% 
  filter(FamHabit %in% c('Climber', 'Clinger')) %>% 
  group_by(StationID, BenSampID) %>% 
  summarise(`%Hapto` = sum(Individuals))
```


Get correct SCI information.

```{r}
SCI_filter <- filter(VSCIresults, BenSampID %in% filter(benSamps_Filter_fin, ! US_L3CODE %in% c(63,65))$BenSampID) %>%
  bind_rows(
    filter(VCPMI63results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(63) | str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  bind_rows(
    filter(VCPMI65results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(65) & !str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  mutate(SeasonGradient = as.factor(paste0(Season, " (",Gradient,")"))) %>%
  left_join(dplyr::select(benSamps_Filter_fin, StationID, BenSampID, RepNum, `Collection Date`) ,
              by = c('StationID', 'BenSampID', 'RepNum', 'Collection Date')) %>%

  # make year variable to make review easier
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, Season, BenSampID, RepNum, SCI, `SCI Score`, `SCI Threshold`,
                Gradient, `Target Count`,  `Sample Comments`, `Collected By`:`Entered Date`, everything()) %>% 
  arrange(Year, StationID, `Collection Date`)

# clean up workspace
rm(VSCIresults);rm(VCPMI63results);rm(VCPMI65results); rm(bioData2019);rm(bioData2020); rm(benSamps_Filter_fin)
```


Now we can save both of these objects out and manually delete the erroneous rows in probData (things not close to benthic collections). We will average the station data in R after extra data is dropped.

```{r save for manual review}
#write.csv(probData, 'processedData/prob20192020data.csv', row.names = F, na = '')
#write.csv(SCI_filter, 'processedData/bio20192020data.csv', row.names = F, na = '')
```

### Habitat Data

Next we need to get the total habitat scores for each benthic sample.

```{r tot hab}
habSamps <- bind_rows(pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% SCI_filter$StationID & year(`Collection Date`) == 2019) ,
                      pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% SCI_filter$StationID & year(`Collection Date`) == 2020)) 
# make sure this is close to nrow SCI_filter
habValues <- pin_get('ejones/habValues', board = 'rsconnect') %>% 
  filter(HabSampID %in% habSamps$HabSampID) %>% 
  group_by(HabSampID) %>% 
  summarise(TotHab = sum(HabValue))

totHab <- left_join(habSamps, habValues, by = "HabSampID") %>% 
  dplyr::select(StationID, `Collection Date`, TotHab)

#write.csv(totHab, 'processedData/totHab.csv', row.names = F, na = '')
```



### Geospatial data 

Watershed-specific landcover information is provided for all wadeable probmon sites each IR publication. In order to expedite the process of organizing and analyzing this geospatial dataset, automated scripts have been developed. This project calls these scripts. 

#### Tiger roads organization

Each year a new tiger road file is required to best compare sample year to roads in the watershed. Follow instructions in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsWebScrapingScript.R in order to scrape the FTP and organize each year into a single shapefile for analyses.

*** Remember: you need to first create a new directory named the year you are scraping (e.g. 2019) in the local C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsPull directory for downloadTigerRoadsByYear() and in unzipped directory to work properly.

#### Delineate new watersheds

The C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\Organize20192020prob.Rmd walks users through delineating wadeable and boatable sites sampled in 2019-2020 using automated streamstats delineation methods. Manual review is conducted in either R or ArcGIS. 

#### Landcover analysis

Once all the necessary stations (wadeable) have QAed watershed information, landcover analysis scripts in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\landcoverAnalysis_20192020.R take users through the automated process of generating the ~100 landcover metrics the prob chapter reports out for wadeable dataset.



## QA data

### Summarize Field/Analyte/Metals
Lucy cleaned the field/analyte/metals datasets and produced a final version with just the samples closest to the biological samples (if exist) per site. There are still S1/2 in there and all sites need to be averaged to a single row per year. I did this in the prob20192020data_EVJEdits sheet. Where there were R, S1, and S2 samples and missing data from the R line, I used S1 data to fill the missing fields from the same sample date. Where there are only S1 and S2 information, I created a new row and named it R and copied all S1 information for that date/time to be used. These changes are all highlighted in green in the .xlsx sheet.

We still need to average parameters to one result per year for CDF analyses. These steps occur in the next chunk.


```{r one row per year}
lucy <- read_excel('processedData/prob20192020data_LKSEdits.xlsx', sheet = 'prob20192020data_EVJEdits', skip = 1) %>% # skip first row
  filter(! Ana_Sam_Mrs_Container_Id_Desc %in% c('S1', 'S2')) %>% 
  # make sure only one row per date/time
  group_by(StationID, FDT_DATE_TIME) %>% 
  mutate(n = n()) %>% 
  dplyr::select(n , everything()) %>% ungroup()
  # if everything fine, proceed
  
# can do everything but MERCURY and Hg-C here
lucyAvg <- lucy %>% 
  dplyr::select(-c(n, FDT_DATE_TIME, Ana_Sam_Mrs_Container_Id_Desc, FDT_SPG_CODE, FDT_COMMENT, FDT_DEPTH)) %>% ungroup() %>% 
  # get one row per StationID
  group_by(StationID, Year) %>% 
  summarise_at(vars(DO:HARDNESS), mean, na.rm = TRUE) %>% 
  # change NaN to NA
  pivot_longer(cols =DO:HARDNESS,names_to = 'Parameter', values_to = 'Value')
lucyAvg$Value[lucyAvg$Value== 'NaN'] <- NA
lucyAvg <- pivot_wider(lucyAvg, names_from = 'Parameter', values_from = 'Value')
  

lucyHg <- dplyr::select(lucy, StationID, Year, MERCURY, `Hg-C`) %>%  
  filter(!is.na(MERCURY) | !is.na(`Hg-C`)) %>% 
  group_by(StationID, Year) %>% 
  summarise(MERCURY = mean(MERCURY, na.rm = T),
            HgCom = unique(`Hg-C`)) 
# change any NaN's back to NA and review manually to make sure no data lost
lucyHg$MERCURY[lucyHg$MERCURY== 'NaN'] <- NA


# smash lucyAvg and lucyHg together to a single, clean dataset
lucyFin <- left_join(lucyAvg, lucyHg, by = c('StationID', 'Year'))

# save and manually review one last time
#write.csv(lucyFin, 'processedData/FinalFieldAnalyteMetals.csv', row.names = F, na= '')'
# went through and manually checked all blanks, 0's, and random points to make sure data looks good

# clean up workspace
rm(lucyAvg);rm(lucy);rm(lucyHg); rm(lucyFin)
```

### Summarize Bugs

Now we need to convert the VCPMI scores to VSCI scale for analyses and make one row per bug record where more than one sample were collected per year to ensure data joins cleanly and CDF analysis can run.

```{r bugAvg}
source('VCPMItoVSCIconversion.R')

# dataset for eastern bios to review to make sure all VSCI/VCPMI results are used correctly
#write.csv(filter(SCI_filter, `Collected By` %in% c("BRETT STERN", "MIKEJ SHAVER", "JUSTIND LOYD" )), 'processedData/VSCI_VCPMIreview.csv', row.names = F, na = '')

SCI_filterAdjust <- SCI_filter %>% 
  filter(SCI != 'VSCI') %>% 
  mutate(VSCIVCPMI = VCPMItoVSCIconversion(`SCI Score`, probRatio))

bugAvg <- SCI_filter %>% 
  group_by(StationID, Year, SCI) %>% 
  summarise_at(vars(c(`SCI Score`, `Family Total Taxa`:PctIntol)), mean, na.rm = TRUE) %>% 
  mutate(VSCIVCPMI = case_when(SCI == 'VSCI' ~ `SCI Score`,
                               SCI != 'VSCI' ~ VCPMItoVSCIconversion(`SCI Score`, probRatio),
                               TRUE ~ as.numeric(NA))) %>% 
  mutate(TotTaxa = `Family Total Taxa`,
         EPTTax = `Family EPT Taxa`,
         VEphem = `%Ephem`,
         VPTHydropsychidae = `%PT - Hydropsychidae`,
         VScrap = case_when(SCI == 'VSCI' ~ `%FamilyScraper`, 
                                   SCI != 'VSCI' ~ `%Scrap`,
                                    TRUE ~ as.numeric(NA)),
         VChiro = case_when(SCI == 'VSCI' ~ `%Chironomidae Score`, 
                            SCI != 'VSCI' ~ ???????,
                             TRUE ~ as.numeric(NA)),
         V2Dom = case_when(SCI == 'VSCI' ~ `Family %2 Dominant`, 
                            SCI != 'VSCI' ~ ??????,
                             TRUE ~ as.numeric(NA)),
         HBI = `Family HBI`,
         VHapto = ?????????
         EPTInd = `Family EPT Taxa`)
  
  
  
  dplyr::select(StationID, Year, TotTaxa = `Family Total Taxa`, EPTTax = `Family EPT Taxa`,
                VEphem = `%Ephem`, VPTHydropsychidae = `%PT - Hydropsychidae`,
                VScrap = `%FamilyScraper`, VChiro = `%Chironomidae Score`, 
                V2Dom, HBI = `Family HBI`, VHapto
                EPTInd
                VSCIVCPMI
names(prob2020)
names(bugAvg)
```





## Combine data


Bring in Field, Analyte, and Metals Data

```{r field}
field <- read.csv('processedData/FinalFieldAnalyteMetals.csv')

```

Bring in Bug data

```{r bug}

```

Bring in Habitat data

```{r habitat}

```

Bring in LRBS data

```{r lrbs}

```

Bring in landcover data

```{r landcover}
landcover <- read.csv('C:/HardDriveBackup/R/GitHub/LandcoverAnalysis/Results/ProbWadeable20192020/Result10.csv')
```

Smash data

```{r data smash}

```

