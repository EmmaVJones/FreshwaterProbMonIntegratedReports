---
title: "Data Acquisition"
author: "Emma Jones"
date: "2/6/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(config)
library(sf)
library(lubridate)
library(pool)
library(pins)
library(sqldf)
library(dbplyr)
library(readxl)

```



## Background

This script connects to the ODS environment, pulls data for assessment window, and summarizes results. Historically, field and analyte information has been reduced to one record per sample year, even if the station was sampled more than once (e.g. spring and fall). The summary statistic used is the median for this report.

## Connect to ODS

Make sure pulling from production environment and not test since those are not exact copies.

```{r connect to ODS}

# Server connection things
conn <- config::get("connectionSettings") # get configuration settings


board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## Connect to ODS production
pool <- dbPool(
 drv = odbc::odbc(),
 Driver = "ODBC Driver 11 for SQL Server",#"SQL Server Native Client 11.0",
 Server= "DEQ-SQLODS-PROD,50000",
 dbname = "ODS",
 trusted_connection = "yes"
)
```

## Bring in final data format to match

Below is the final dataset for the 2022 IR that we will aim to match for 2024 IR.

This version (_EVJ) was used because it includes the transition of the NRO site naming issue 1AQUA004.14 to 1AQUA004.35. The original data published online for IR2022 is still available in 'originalData/originalOriginal/Wadeable_ProbMon_2001-2020.csv' but this version represents the best data to move forward with for IR2024. Since Lucy is awesome, she combined these data in previous reports and kept the ProbMon name 1AQUA004.14 to represent the station. Emma updated that station in the _EVJ spreadsheet on 2/6/2023 so when carried forward, this station will me known as 1AQUA004.35.

```{r prob 2022}
prob2022 <- read_csv('originalData/Wadeable_ProbMon_2001-2020_EVJ.csv') 
```

Generally speaking, we need to hit a few data sources to acquire all the data we need to publish for final chapter:
 - Field Data (Wqm_Field_Data_View) for DO, pH, SpCond
 - Analyte Data (Wqm_Analyte_Data_View) for TN, TP, TDS, NH4, NO3, TKN, Ortho-P, Turb, TSS, Na, K, Cl, Sf, 70331VFine, SSCCOARSE, SSCFINE, SSCTOTAL
 - PHAB database (still in Access) for LRBS, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, BL_CB_GR_Embed_PCT, Embed_PCT
 - EDAS (pinned (pre calculated) habitat data on server) for TotHab
 - EDAS (pinned (pre calculated) benthic data on server) for TotTaxa, EPTTax, VEphem, VPTHydropsychidae, VScrap, VChiro, V2Dom, HBI, VHapto, EPTInd, VSCIVCPMI (calculated conversion)
 - Analyte Data (Wqm_Analyte_Data_View) for dissolved metals MetalCCU (calculated), ARSENICppm, BERYLLIUMppm, CADMIUMppm, CHROMIUMppm, COPPERppm, LEADppm, MANGppm, NICKELppm, SILVERppm, ZINCppm, ANTIMONYppm, ALUMINUMppm, SELENIUMppm, IRONppm, MERCURYppm, THALLIUMppm, CALCIUM, MAGNESIUM, ARSENIC, BARIUM, BERYLLIUM, CADMIUM, CHROMIUM, COPPER, IRON, LEAD, MANGANESE, THALLIUM, NICKEL, SILVER, ZINC, ANTIMONY, ALUMINUM, SELENIUM, HARDNESS, MERCURY, Hg-C
 - Special GIS dataset for watershed spatial info 


## Bring in sites

Now we need to bring in the two years worth of new sites to pull the data

```{r new sites}
WQM_Stations_Filter <- read_excel('originalData/Wadeable_ProbMon2021-2022.xlsx', 
                                  sheet = '20212022') %>% 
  filter(status == 'TS') # keep only wadeable sites that were sampled for data querying
```




## Query data

Using the current build of the conventionals function to query and organize field, analyte, and metals data consistently. Field and analyte methods pulled from CEDS WQM data query tool.

### Query Terms

These presets allow correct data retrieval. 

```{r query terms}

# Basic station info for conventionals
multiStationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()
multiStationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()

# make sure all stations are in CEDS
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationInfo$Sta_Id]
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationGIS_View$Station_Id]
```




### Field data

```{r field data}
#dateRange_multistation <- c(as.Date('2020-01-01'), as.Date('2020-12-31'))# c(as.Date('2019-02-05'), as.Date('2019-02-08'))# 
x2021Sites <- filter(WQM_Stations_Filter, Year == 2021)$StationID
x2022Sites <- filter(WQM_Stations_Filter, Year == 2022)$StationID

# do as two steps to make sure only bring back data from desired window for each site
multistationFieldData <- bind_rows(
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2021Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2021-01-01'), as.Date('2021-12-31'))) %>% # & # x >= left & x <= right
    as_tibble(),
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2022Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2022-01-01'), as.Date('2022-12-31'))) %>% # & # x >= left & x <= right
    as_tibble()) %>% 
  filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")

  # filter(Fdt_Sta_Id %in% !! WQM_Stations_Filter$StationID &
  #          between(as.Date(Fdt_Date_Time), !! dateRange_multistation[1], !! dateRange_multistation[2])) %>% # & # x >= left & x <= right
           #Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>%  # don't drop QA failure on SQL part bc also drops any is.na(Ssc_Description)
  # as_tibble() %>% 
  # filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")
```

### Analyte Data

```{r analyte data}
# can do this in one step since we are searching for Fdt_Id's from appropriate field data windows
multistationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! multistationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange_multistation[1], !! dateRange_multistation[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>% 
  as_tibble() %>%
  left_join(dplyr::select(multistationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

analyteNames <- multistationAnalyteData %>% 
  dplyr::select(Me_Meth_Short_Name, Pg_Storet_Code) %>% 
  distinct(Pg_Storet_Code, .keep_all = T)
```




### Organize by Conventionals logic

To consistently organize field, analyte, and metals data it is prudent to use the "conventionals" data rules.

```{r conventionals organization of raw data}
source('C:/HardDriveBackup/R/GitHub/WQMdataQueryTool/conventionalsFunction10212022.R')

conventionalsList <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                                          stationFieldDataUserFilter= multistationFieldData, 
                                          stationAnalyteDataUserFilter = multistationAnalyteData, 
                                          stationInfo = multiStationInfo,
                                          stationGIS_View = multiStationGIS_View,
                                          dropCodes = c('QF'),
                                          assessmentUse = F,
                                          overwriteUncensoredZeros = TRUE) 
conventionals <- conventionalsList$More %>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)

#write.csv(conventionals, 'processedData/2022conventionals.csv', row.names = F, na = "")
```
### QA data with Lucy

In order to make sure we are pulling the same information from Logi and R-ODS connection, the following script walks users through verification. Lucy pulled 2020 data using Logi.

```{r convetionals QA}
source('conventionalsQA.R')
```

Looks good for parameters matching what logi pulls. 

Now quick QA test for EB information. What we want is to make sure that none of the blanks have too high values, indicating too much uncertainty with respect to units and potentially exceeding standards.

```{r QA conventionals EB}
QAconventionals <- conventionals %>% 
  filter(Ana_Sam_Mrs_Container_Id_Desc == 'EB') %>% 
  dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, NITROGEN_mg_L:RMK_82079) %>% 
  dplyr::select(!contains('LEVEL_'))

# make a lookuptable quick
allNames = c(names(QAconventionals %>% dplyr::select(-c(FDT_STA_ID:Ana_Sam_Mrs_Container_Id_Desc))))
parameters = x[seq(1, length(x), 2)]  
remarks = x[! x %in% y]  

lookupTable <- tibble(Parameter = parameters,
       RemarkName = remarks) %>% 
  mutate(link = 1:n())

# two steps bc can't combine character and numeric entries in a pivot longer
QAconventionals <- left_join(
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, ! contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols = -c(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'Parameter', values_to = 'Measure', values_drop_na = T) %>% 
    left_join(dplyr::select(lookupTable, -RemarkName), by = 'Parameter'),
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc,  contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols = -c(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'RemarkName', values_to = 'Remark', values_drop_na = T) %>% 
    left_join(dplyr::select(lookupTable, -Parameter), by = 'RemarkName'),
  by = c("FDT_STA_ID", "FDT_DATE_TIME", "FDT_SPG_CODE", "Ana_Sam_Mrs_Container_Id_Desc", "link")) %>% 
  dplyr::select(-link)
  
QAconventionalsChar <-  conventionals %>% 

group_by(FDT_STA_ID, FDT_DATE_TIME) %>% 
  mutate(n = n()) %>% 
  filter(n > 1) %>% 
  dplyr::select(n,FDT_STA_ID,	FDT_DATE_TIME,	FDT_DEPTH,	FDT_SPG_CODE,	Ana_Sam_Mrs_Container_Id_Desc:RMK_82079, everything()) %>% 
  arrange(FDT_STA_ID,	FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc)

# QAconventionals <- conventionals %>% 
#   group_by(FDT_STA_ID, FDT_DATE_TIME) %>% 
#   mutate(n = n()) %>% 
#   filter(n > 1) %>% 
#   dplyr::select(n,FDT_STA_ID,	FDT_DATE_TIME,	FDT_DEPTH,	FDT_SPG_CODE,	Ana_Sam_Mrs_Container_Id_Desc:RMK_82079, everything()) %>% 
#   arrange(FDT_STA_ID,	FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc)

#write.csv(QAconventionals, 'test.csv', row.names = F, na="")
```

T	= Value reported is less than the criteria of detection
QQ =	Analyte detected above the MDL but below the method quantification limit.


For reporting purposes, we don't want to use the raw data (even though Roger has confidence in those numbers). We want to back lab reported values back to 0.001 if they fall below that. This is the first report we are pulling uncensored data. We need to repull and organize all years of data with uncensored and censored and rerun statistics for both (some programs e.g. permitting) may prefer censored data but uncensored could offer smoother CDF curves (remove blocky detection limits that have improved over last 20+ years).

Jason and I have determined that using uncensored this round is okay because we are only cleaning up the lower end of the dataset and are not coming close to potential standards violations.

```{r back to positive}
conventionals2 <- conventionals %>% 
  mutate_if(is.numeric, ~1 * (. < 0))
conventionals2[conventionals2 < 0] <- 0.001  
# double check things worked
summary(conventionals)
summary(conventionals2) # make sure columns with negative mins have gone to 0.001

#Change longitude back to real numbers
conventionals2 <- left_join(conventionals2,
                            dplyr::select(conventionals, FDT_STA_ID, Longitude) %>% 
                              distinct(FDT_STA_ID, .keep_all = T),
                            by = 'FDT_STA_ID') %>% 
  mutate(Longitude.x = Longitude.y) %>% 
  rename(Longitude = Longitude.x) %>% 
  dplyr::select(-Longitude.y)
```




Next we need to change the data for manual QA to the probmon format. We will also drop EB's at this point.


```{r prob data format}

probData <- conventionals2 %>%
  filter(Ana_Sam_Mrs_Container_Id_Desc != 'EB') %>% 
  mutate(StationID = FDT_STA_ID, 
         Year = year(FDT_DATE_TIME),
              DO = DO_mg_L, 
              pH = FDT_FIELD_PH,
              SpCond = FDT_SPECIFIC_CONDUCTANCE, 
              TN = NITROGEN_mg_L, 
              TP = PHOSPHORUS_mg_L, 
              TDS = TDS_mg_L,
              NH4 = AMMONIA_mg_L, 
              NO3 = NITRATE_mg_L, 
              TKN = NITROGEN_KJELDAHL_TOTAL_00625_mg_L,
              `Ortho-P` = PHOSPHORUS_TOTAL_ORTHOPHOSPHATE_70507_mg_L,
              Turb = `TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU`,
              TSS = TSS_mg_L, 
              Na = `SODIUM, DISSOLVED (MG/L AS NA)`, 
              K = `POTASSIUM, DISSOLVED (MG/L AS K)`,
              Cl = CHLORIDE_mg_L,
              Sf = SULFATE_mg_L,
              `70331VFine` = `SSC%Finer`,
              SSCCOARSE = SSC_COARSE,
              SSCFINE =  SSC_FINE, 
              SSCTOTAL = SSC_TOTAL,
              # sediment ppm data hasn't been collected since 2011
              ARSENICppm = as.numeric(NA),
              BERYLLIUMppm = as.numeric(NA),
              CADMIUMppm = as.numeric(NA),
              CHROMIUMppm = as.numeric(NA),
              COPPERppm = as.numeric(NA),
              LEADppm = as.numeric(NA),
              MANGppm = as.numeric(NA),
              NICKELppm = as.numeric(NA),
              SILVERppm = as.numeric(NA),
              ZINCppm = as.numeric(NA),
              ANTIMONYppm = as.numeric(NA),
              ALUMINUMppm = as.numeric(NA),
              SELENIUMppm = as.numeric(NA),
              IRONppm = as.numeric(NA),
              MERCURYppm = as.numeric(NA),
              THALLIUMppm = as.numeric(NA),
              CALCIUM = `CALCIUM, DISSOLVED (MG/L AS CA)`,
              MAGNESIUM = `MAGNESIUM, DISSOLVED (MG/L AS MG)`,
              ARSENIC = `ARSENIC, DISSOLVED  (UG/L AS AS)`,
              BARIUM = `BARIUM, DISSOLVED (UG/L AS BA)`,
              BERYLLIUM = `BERYLLIUM, DISSOLVED (UG/L AS BE)`,
              CADMIUM = `CADMIUM, DISSOLVED (UG/L AS CD)`,
              CHROMIUM = `CHROMIUM, DISSOLVED (UG/L AS CR)`,
              COPPER = `COPPER, DISSOLVED (UG/L AS CU)`,
              IRON = `IRON, DISSOLVED (UG/L AS FE)`,
              LEAD = `LEAD, DISSOLVED (UG/L AS PB)`,
              MANGANESE = `MANGANESE, DISSOLVED (UG/L AS MN)`,
              THALLIUM = `THALLIUM, DISSOLVED (UG/L AS TL)`,
              NICKEL = `NICKEL, DISSOLVED (UG/L AS NI)`,
              SILVER = `SILVER, DISSOLVED (UG/L AS AG)`,
              ZINC = `ZINC, DISSOLVED (UG/L AS ZN)`,
              ANTIMONY = `ANTIMONY, DISSOLVED (UG/L AS SB)`,
              ALUMINUM = `ALUMINUM, DISSOLVED (UG/L AS AL)`,
              SELENIUM = `SELENIUM, DISSOLVED (UG/L AS SE)`,
              HARDNESS = `HARDNESS, CA MG CALCULATED (MG/L AS CACO3) AS DISSOLVED`,
              MERCURY = `MERCURY-TL,FILTERED WATER,ULTRATRACE METHOD NG/L`,
              `Hg-C` = `RMK_50091`) %>% 
  dplyr::select(FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, Ana_Sam_Mrs_Container_Id_Desc, #other helpful info
                any_of(names(prob2020))) %>% 
  dplyr::select(StationID, Year, FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, 
                Ana_Sam_Mrs_Container_Id_Desc, everything()) %>% 
  arrange(Year, StationID, FDT_DATE_TIME)

#clean up workspace
rm(multistationAnalyteData);rm(multistationFieldData);rm(multiStationGIS_View);rm(multiStationInfo)
rm(conventionalsList);rm(conventionals);rm(conventionals2)
```


### Metals CCU

This is a good point to run the Metals CCU analysis before QA and yearly consolidation.

MetalsCCU calculation function from BSA tool. This version is the correct version with Larry's bonus metals commented out.

```{r metalsCCUfunction}

# Metals CCU Calculation, with Larry Bonus metals commented out
metalsCCUcalc <- function(Hardness,Aluminum,Arsenic,Cadmium,Chromium,Copper,Lead,Nickel,Selenium,Zinc){
  criteriaHardness <- ifelse(Hardness<25,25,ifelse(Hardness>400,400,Hardness))
  #AluminumEPAChronic <- Aluminum/150 # Larry Bonus Metal
  ArsenicChronic <- Arsenic/150
  #CadmiumChronic <- Cadmium/(exp(0.7852*(log(criteriaHardness))-3.49)) # Larry Bonus Metal
  ChromiumChronic <- Chromium/((exp(0.819*(log(criteriaHardness))+0.6848))*0.86)
  CopperChronic <- Copper/((exp(0.8545*(log(criteriaHardness))-1.702))*0.96)
  LeadChronic <- Lead/((exp(1.273*(log(criteriaHardness))-3.259))*(1.46203-(log(criteriaHardness)*(0.145712))))
  NickelChronic <- Nickel/((exp(0.846*(log(criteriaHardness))-0.884))*0.997)
  #SeleniumChronic <- Selenium/5 # Larry Bonus Metal
  ZincChronic <- Zinc/((exp(0.8473*(log(criteriaHardness))+0.884))*0.986)
  #return(sum(AluminumEPAChronic,ArsenicChronic,CadmiumChronic,ChromiumChronic,CopperChronic,LeadChronic,
  #           NickelChronic,SeleniumChronic,ZincChronic))
  return(sum(ArsenicChronic,ChromiumChronic,CopperChronic,LeadChronic,
             NickelChronic,ZincChronic))
}


```

Just get necessary data to run function.

```{r metalsCCUcalculation}
metalsCCUresults <- select(probData,StationID, Year,  FDT_DATE_TIME, Ana_Sam_Mrs_Container_Id_Desc,
                           HARDNESS, ALUMINUM, ARSENIC, CADMIUM, 
                           CHROMIUM, COPPER, LEAD, NICKEL, SELENIUM, ZINC)

metalsCCUresults <- metalsCCUresults %>% 
  rowwise() %>% 
  mutate( MetalCCU = round(metalsCCUcalc(Hardness = HARDNESS, Aluminum = ALUMINUM, 
                               Arsenic = ARSENIC,Cadmium = CADMIUM,
                               Chromium = CHROMIUM,Copper = COPPER, 
                               Lead = LEAD,Nickel =NICKEL,
                               Selenium = SELENIUM,Zinc = ZINC), 
                            digits=4))  %>% 
  dplyr::select(StationID:Ana_Sam_Mrs_Container_Id_Desc, MetalCCU)


probData <- left_join(probData, metalsCCUresults,
                       by = c("StationID", "Year", "FDT_DATE_TIME", "Ana_Sam_Mrs_Container_Id_Desc"))

``` 




Last, we need to remove extra samples not associated with FP/bio sampling events but not drop too much in case other programs grabbed samples on a different date. This is still best done manually. For 2022IR, Lucy manually screened data for just spring/fall sample information (using excel).

To do this, we need benthic data.


### Benthic Data