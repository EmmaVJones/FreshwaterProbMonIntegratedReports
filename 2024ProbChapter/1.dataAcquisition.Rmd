---
title: "Data Acquisition"
author: "Emma Jones"
date: "2/6/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(config)
library(sf)
library(lubridate)
library(pool)
library(pins)
library(sqldf)
library(dbplyr)
library(readxl)

```



## Background

This script connects to the ODS environment, pulls data for assessment window, and summarizes results. Historically, field and analyte information has been reduced to one record per sample year, even if the station was sampled more than once (e.g. spring and fall). The summary statistic used is the median for this report.

## Connect to ODS

Make sure pulling from production environment and not test since those are not exact copies.

```{r connect to ODS}

# Server connection things
conn <- config::get("connectionSettings") # get configuration settings


board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## Connect to ODS production
pool <- dbPool(
 drv = odbc::odbc(),
 Driver = "ODBC Driver 11 for SQL Server",#"SQL Server Native Client 11.0",
 Server= "DEQ-SQLODS-PROD,50000",
 dbname = "ODS",
 trusted_connection = "yes"
)
```

## Bring in final data format to match

Below is the final dataset for the 2022 IR that we will aim to match for 2024 IR.

This version (_EVJ_fixedAreaAndUpdatedPopulation) was used because it includes the transition of the NRO site naming issue 1AQUA004.14 to 1AQUA004.35. It also contains the fixed landcover metrics where watershed areas were incorrect, rerun 2019-2020 landcover data with NLCD2019, and population metrics based off 2020 census for all sites. This work is documented in `C:\HardDriveBackup\R\GitHub\LandcoverAnalysis/2023watershedQAwork.Rmd`. The original data published online for IR2022 is still available in 'originalData/originalOriginal/Wadeable_ProbMon_2001-2020.csv' but this version represents the best data to move forward with for IR2024. Since Lucy is awesome, she combined these data in previous reports and kept the ProbMon name 1AQUA004.14 to represent the station. Emma updated that station in the _EVJ spreadsheet on 2/6/2023 so when carried forward, this station will me known as 1AQUA004.35.

```{r prob 2022}
prob2022 <- read_csv('originalData/Wadeable_ProbMon_2001-2020_EVJ_fixedAreaAndUpdatedPopulation.csv') %>% 
  rename(`Ortho-P` = 'Ortho P',
         'Hg-C' = 'Hg C', 
         '70331VFine' = 'X70331VFine')
```

Generally speaking, we need to hit a few data sources to acquire all the data we need to publish for final chapter:
 - Field Data (Wqm_Field_Data_View) for DO, pH, SpCond
 - Analyte Data (Wqm_Analyte_Data_View) for TN, TP, TDS, NH4, NO3, TKN, Ortho-P, Turb, TSS, Na, K, Cl, Sf, 70331VFine, SSCCOARSE, SSCFINE, SSCTOTAL
 - PHAB database (still in Access) for LRBS, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, BL_CB_GR_Embed_PCT, Embed_PCT
 - EDAS (pinned (pre calculated) habitat data on server) for TotHab
 - EDAS (pinned (pre calculated) benthic data on server) for TotTaxa, EPTTax, VEphem, VPTHydropsychidae, VScrap, VChiro, V2Dom, HBI, VHapto, EPTInd, VSCIVCPMI (calculated conversion)
 - Analyte Data (Wqm_Analyte_Data_View) for dissolved metals MetalCCU (calculated), ARSENICppm, BERYLLIUMppm, CADMIUMppm, CHROMIUMppm, COPPERppm, LEADppm, MANGppm, NICKELppm, SILVERppm, ZINCppm, ANTIMONYppm, ALUMINUMppm, SELENIUMppm, IRONppm, MERCURYppm, THALLIUMppm, CALCIUM, MAGNESIUM, ARSENIC, BARIUM, BERYLLIUM, CADMIUM, CHROMIUM, COPPER, IRON, LEAD, MANGANESE, THALLIUM, NICKEL, SILVER, ZINC, ANTIMONY, ALUMINUM, SELENIUM, HARDNESS, MERCURY, Hg-C
 - Special GIS dataset for watershed spatial info 


## Bring in sites

Now we need to bring in the two years worth of new sites to pull the data

```{r new sites}
WQM_Stations_Filter <- read_excel('C:/Users/wmu43954/OneDrive - Commonwealth of Virginia/Freshwater ProbMon/IR2024/Wadeable_ProbMon2021-2022.xlsx',
                                  #'originalData/Wadeable_ProbMon2021-2022.xlsx', # or from here for local archiving
                                  sheet = 'Sheet1') %>% 
  filter(status == 'TS') # keep only wadeable sites that were sampled for data querying
```




## Query data

Using the current build of the conventionals function to query and organize field, analyte, and metals data consistently. Field and analyte methods pulled from CEDS WQM data query tool.

### Query Terms

These presets allow correct data retrieval. 

```{r query terms}

# Basic station info for conventionals
multiStationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()
multiStationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()

# make sure all stations are in CEDS
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationInfo$Sta_Id]
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationGIS_View$Station_Id]
```




### Field data

```{r field data}
#dateRange_multistation <- c(as.Date('2020-01-01'), as.Date('2020-12-31'))# c(as.Date('2019-02-05'), as.Date('2019-02-08'))# 
x2021Sites <- filter(WQM_Stations_Filter, Year == 2021)$StationID
x2022Sites <- filter(WQM_Stations_Filter, Year == 2022)$StationID

# do as two steps to make sure only bring back data from desired window for each site
multistationFieldData <- bind_rows(
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2021Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2021-01-01'), as.Date('2021-12-31'))) %>% # & # x >= left & x <= right
    as_tibble(),
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2022Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2022-01-01'), as.Date('2022-12-31'))) %>% # & # x >= left & x <= right
    as_tibble()) %>% 
  filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")

  # filter(Fdt_Sta_Id %in% !! WQM_Stations_Filter$StationID &
  #          between(as.Date(Fdt_Date_Time), !! dateRange_multistation[1], !! dateRange_multistation[2])) %>% # & # x >= left & x <= right
           #Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>%  # don't drop QA failure on SQL part bc also drops any is.na(Ssc_Description)
  # as_tibble() %>% 
  # filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")
```

### Analyte Data

```{r analyte data}
# can do this in one step since we are searching for Fdt_Id's from appropriate field data windows
multistationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! multistationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange_multistation[1], !! dateRange_multistation[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>% 
  as_tibble() %>%
  left_join(dplyr::select(multistationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

analyteNames <- multistationAnalyteData %>% 
  dplyr::select(Me_Meth_Short_Name, Pg_Storet_Code) %>% 
  distinct(Pg_Storet_Code, .keep_all = T)
```




### Organize by Conventionals logic

To consistently organize field, analyte, and metals data it is prudent to use the "conventionals" data rules.

```{r conventionals organization of raw data}
source('C:/HardDriveBackup/R/GitHub/WQMdataQueryTool/conventionalsFunction10212022.R')

conventionalsList <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                                          stationFieldDataUserFilter= multistationFieldData, 
                                          stationAnalyteDataUserFilter = multistationAnalyteData, 
                                          stationInfo = multiStationInfo,
                                          stationGIS_View = multiStationGIS_View,
                                          dropCodes = c('QF'),
                                          assessmentUse = F,
                                          overwriteUncensoredZeros = TRUE) 
conventionals <- conventionalsList$More %>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)

#write.csv(conventionals, 'processedData/2024conventionals.csv', row.names = F, na = "")
```

### QA data

In previous cycles, we had a Logi-proficient staff member pull the pre-R queries for the same data using Logi and then ran analyses to double check both methods resulted in the same answers. For IR2024, we will subsample 5% of all CEDS field and analyte data points and manually review to ensure the data are correct. 

```{r QA data}
conventionalsLong <- conventionals %>% 
  dplyr::select(-c(STA_DESC:Data_Source, FDT_DEPTH_DESC:FDT_COMMENT, Ana_Sam_Mrs_Container_Id_Desc, ENTEROCOCCI )) %>% 
  dplyr::select(! contains(c('LEVEL_', 'RMK'))) %>% # drop all level columns since all DEQ data
  pivot_longer(-c('FDT_STA_ID', 'FDT_DATE_TIME', 'FDT_DEPTH'), 
               names_to = 'Parameter', values_to = 'Value',  values_drop_na = TRUE)  # drop NA values so QAing 10% of real data

# how many rows do we need to QA?
nrow(conventionalsLong) * .05
  
conventionalsLongQA <- conventionalsLong %>% 
  sample_n(331, replace = FALSE) %>%  #randomly selects 335 rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)

#331 is still a lot, subsample again to 5% and see if we need to dig in more
conventionalsLongQAminiEmma <- conventionalsLongQA %>% 
  sample_n(17, replace = FALSE) %>%  #randomly selects 5% of rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)
# write.csv(conventionalsLongQAminiEmma, 'processedData/QA/conventionalsLongQAminiEmma.csv')
# This dataset checked out 100%


conventionalsLongQAminiJason <- conventionalsLongQA %>% 
  sample_n(17, replace = FALSE) %>%  #randomly selects 5% of rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)
# write.csv(conventionalsLongQAminiJason, 'processedData/QA/conventionalsLongQAminiJason.csv')


conventionalsLongQAminiScott <- conventionalsLongQA %>% 
  sample_n(17, replace = FALSE) %>%  #randomly selects 5% of rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)
# write.csv(conventionalsLongQAminiScott, 'processedData/QA/conventionalsLongQAminiScott.csv')
rm(conventionalsLongQA, conventionalsLongQAmini, conventionalsLongQAminiEmma, conventionalsLongQAminiJason, conventionalsLongQAminiScott) # clean up workspace
```



#### QA Equipment Blank Data


Now quick QA test for EB information. What we want is to make sure that none of the blanks have too high values, indicating too much uncertainty with respect to units and potentially exceeding standards.

```{r QA conventionals EB}
QAconventionals <- conventionals %>% 
  filter(Ana_Sam_Mrs_Container_Id_Desc == 'EB') %>% 
  dplyr::select(FDT_STA_ID, FDT_DATE_TIME,STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, NITROGEN_mg_L:RMK_82079) %>% 
  dplyr::select(!contains('LEVEL_'))

# No SWRO QA in 2021-2022?

# make a lookuptable quick
allNames = c(names(QAconventionals %>% dplyr::select(-c(FDT_STA_ID:Ana_Sam_Mrs_Container_Id_Desc))))
parameters = allNames[seq(1, length(allNames), 2)]  
remarks = allNames[! allNames %in% parameters]  

lookupTable <- tibble(Parameter = parameters,
       RemarkName = remarks) %>% 
  mutate(link = 1:n())

# two steps bc can't combine character and numeric entries in a pivot longer
QAconventionals <- left_join(
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE,FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, ! contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols = -c(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'Parameter', values_to = 'Measure', values_drop_na = T) %>% 
    left_join(dplyr::select(lookupTable, -RemarkName), by = 'Parameter'),
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc,  contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols = -c(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'RemarkName', values_to = 'Remark', values_drop_na = T) %>% 
    left_join(dplyr::select(lookupTable, -Parameter), by = 'RemarkName'),
  by = c("FDT_STA_ID", "FDT_DATE_TIME", "STA_REC_CODE", "FDT_SPG_CODE", "Ana_Sam_Mrs_Container_Id_Desc", "link")) %>% 
  dplyr::select(-link)
  

#write.csv(QAconventionals, 'test.csv', row.names = F, na="")
```


T	= Value reported is less than the criteria of detection
QQ =	Analyte detected above the MDL but below the method quantification limit.

Since all equipment blanks have those two flags, our sample data are deemed within spec and we are good to proceed.


For reporting purposes, we don't want to use the raw data (even though Roger has confidence in those numbers). We want to back lab reported values back to 0.001 if they fall below that. This is the second report we are pulling uncensored data. We need to repull and organize all years of data with uncensored and censored and rerun statistics for both (some programs e.g. permitting) may prefer censored data but uncensored could offer smoother CDF curves (remove blocky detection limits that have improved over last 20+ years).

Jason and I have determined that using uncensored this round is okay because we are only cleaning up the lower end of the dataset and are not coming close to potential standards violations.

```{r back to positive}
below0 <- function(x)(if(all(!is.na(x) & x < 0 & is.numeric(x))){return(0.001)}else{x})
#below0(conventionals$`CADMIUM, DISSOLVED (UG/L AS CD)`)

conventionals2 <- conventionals %>% 
  rowwise() %>% 
 # mutate_if(is.numeric, ~0.001 * (. < 0))
  mutate_at(vars(FDT_FIELD_PH:SSC_TOTAL), below0)

# double check things worked
#summary(conventionals)
#summary(conventionals2) # make sure columns with negative mins have gone to 0.001
```




Next we need to change the data for manual QA to the probmon format. We will also drop EB's at this point.


```{r prob data format}

probData <- conventionals2 %>%
  filter(Ana_Sam_Mrs_Container_Id_Desc != 'EB') %>% 
  mutate(StationID = FDT_STA_ID, 
         Year = year(FDT_DATE_TIME),
              DO = DO_mg_L, 
              pH = FDT_FIELD_PH,
              SpCond = FDT_SPECIFIC_CONDUCTANCE, 
              TN = NITROGEN_TOTAL_00600_mg_L,
              TP = PHOSPHORUS_TOTAL_00665_mg_L, 
              TDS = TDS_mg_L, #this is fine, just a recode of storet 70300
              NH4 = AMMONIA_mg_L, 
              NO3 = NITRATE_mg_L, 
              TKN = NITROGEN_KJELDAHL_TOTAL_00625_mg_L,
              `Ortho-P` = PHOSPHORUS_TOTAL_ORTHOPHOSPHATE_70507_mg_L,
              Turb = `TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU`,
              TSS = TSS_mg_L, 
              Na = `SODIUM, DISSOLVED (MG/L AS NA)`, 
              K = `POTASSIUM, DISSOLVED (MG/L AS K)`,
              Cl = CHLORIDE_DISSOLVED_00941_mg_L, #CHLORIDE_mg_L,
              Sf = SULFATE_DISS_mg_L, #SULFATE_mg_L,
              `70331VFine` = `SSC%Finer`,
              SSCCOARSE = SSC_COARSE,
              SSCFINE =  SSC_FINE, 
              SSCTOTAL = SSC_TOTAL,
              # sediment ppm data hasn't been collected since 2011
              ARSENICppm = as.numeric(NA),
              BERYLLIUMppm = as.numeric(NA),
              CADMIUMppm = as.numeric(NA),
              CHROMIUMppm = as.numeric(NA),
              COPPERppm = as.numeric(NA),
              LEADppm = as.numeric(NA),
              MANGppm = as.numeric(NA),
              NICKELppm = as.numeric(NA),
              SILVERppm = as.numeric(NA),
              ZINCppm = as.numeric(NA),
              ANTIMONYppm = as.numeric(NA),
              ALUMINUMppm = as.numeric(NA),
              SELENIUMppm = as.numeric(NA),
              IRONppm = as.numeric(NA),
              MERCURYppm = as.numeric(NA),
              THALLIUMppm = as.numeric(NA),
              CALCIUM = `CALCIUM, DISSOLVED (MG/L AS CA)`,
              MAGNESIUM = `MAGNESIUM, DISSOLVED (MG/L AS MG)`,
              ARSENIC = `ARSENIC, DISSOLVED  (UG/L AS AS)`,
              BARIUM = `BARIUM, DISSOLVED (UG/L AS BA)`,
              BERYLLIUM = `BERYLLIUM, DISSOLVED (UG/L AS BE)`,
              CADMIUM = `CADMIUM, DISSOLVED (UG/L AS CD)`,
              CHROMIUM = `CHROMIUM, DISSOLVED (UG/L AS CR)`,
              COPPER = `COPPER, DISSOLVED (UG/L AS CU)`,
              IRON = `IRON, DISSOLVED (UG/L AS FE)`,
              LEAD = `LEAD, DISSOLVED (UG/L AS PB)`,
              MANGANESE = `MANGANESE, DISSOLVED (UG/L AS MN)`,
              THALLIUM = `THALLIUM, DISSOLVED (UG/L AS TL)`,
              NICKEL = `NICKEL, DISSOLVED (UG/L AS NI)`,
              SILVER = `SILVER, DISSOLVED (UG/L AS AG)`,
              ZINC = `ZINC, DISSOLVED (UG/L AS ZN)`,
              ANTIMONY = `ANTIMONY, DISSOLVED (UG/L AS SB)`,
              ALUMINUM = `ALUMINUM, DISSOLVED (UG/L AS AL)`,
              SELENIUM = `SELENIUM, DISSOLVED (UG/L AS SE)`,
              HARDNESS = `HARDNESS, CA MG CALCULATED (MG/L AS CACO3) AS DISSOLVED`,
              MERCURY = `MERCURY-TL,FILTERED WATER,ULTRATRACE METHOD NG/L`,
              `Hg-C` = `RMK_50091`) %>% 
  dplyr::select(FDT_DATE_TIME, FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, Ana_Sam_Mrs_Container_Id_Desc, #other helpful info
                any_of(names(prob2022))) %>% 
  dplyr::select(StationID, FDT_DATE_TIME, Year, FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, 
                Ana_Sam_Mrs_Container_Id_Desc, everything()) %>% 
  arrange(Year, StationID, FDT_DATE_TIME)

#clean up workspace
rm(multistationAnalyteData, multistationFieldData, multiStationGIS_View, multiStationInfo, 
   conventionalsList, conventionals, conventionals2)
```


At this point we know there are still too many samples at certain stations that get repeat visits for other sample program codes. We will correct for this in future steps once we have the benthic data to choose the chem results that occurred closest to the bug data.


### Metals CCU

This is a good point to run the Metals CCU analysis before QA and yearly consolidation.

MetalsCCU calculation function from BSA tool. This version is the correct version with Larry's bonus metals commented out.

```{r metalsCCUfunction}

# Metals CCU Calculation, with Larry Bonus metals commented out
metalsCCUcalc <- function(Hardness,Aluminum,Arsenic,Cadmium,Chromium,Copper,Lead,Nickel,Selenium,Zinc){
  criteriaHardness <- ifelse(Hardness<25,25,ifelse(Hardness>400,400,Hardness))
  #AluminumEPAChronic <- Aluminum/150 # Larry Bonus Metal
  ArsenicChronic <- Arsenic/150
  #CadmiumChronic <- Cadmium/(exp(0.7852*(log(criteriaHardness))-3.49)) # Larry Bonus Metal
  ChromiumChronic <- Chromium/((exp(0.819*(log(criteriaHardness))+0.6848))*0.86)
  CopperChronic <- Copper/((exp(0.8545*(log(criteriaHardness))-1.702))*0.96)
  LeadChronic <- Lead/((exp(1.273*(log(criteriaHardness))-3.259))*(1.46203-(log(criteriaHardness)*(0.145712))))
  NickelChronic <- Nickel/((exp(0.846*(log(criteriaHardness))-0.884))*0.997)
  #SeleniumChronic <- Selenium/5 # Larry Bonus Metal
  ZincChronic <- Zinc/((exp(0.8473*(log(criteriaHardness))+0.884))*0.986)
  #return(sum(AluminumEPAChronic,ArsenicChronic,CadmiumChronic,ChromiumChronic,CopperChronic,LeadChronic,
  #           NickelChronic,SeleniumChronic,ZincChronic))
  return(sum(ArsenicChronic,ChromiumChronic,CopperChronic,LeadChronic,
             NickelChronic,ZincChronic))
}


```

Just get necessary data to run function.

```{r metalsCCUcalculation}
metalsCCUresults <- select(probData,StationID, Year,  FDT_DATE_TIME, Ana_Sam_Mrs_Container_Id_Desc,
                           HARDNESS, ALUMINUM, ARSENIC, CADMIUM, 
                           CHROMIUM, COPPER, LEAD, NICKEL, SELENIUM, ZINC)

metalsCCUresults <- metalsCCUresults %>% 
  rowwise() %>% 
  mutate( MetalCCU = round(metalsCCUcalc(Hardness = HARDNESS, Aluminum = ALUMINUM, 
                               Arsenic = ARSENIC,Cadmium = CADMIUM,
                               Chromium = CHROMIUM,Copper = COPPER, 
                               Lead = LEAD,Nickel =NICKEL,
                               Selenium = SELENIUM,Zinc = ZINC), 
                            digits=4))  %>% 
  dplyr::select(StationID:Ana_Sam_Mrs_Container_Id_Desc, MetalCCU)


probData <- left_join(probData, metalsCCUresults,
                       by = c("StationID", "Year", "FDT_DATE_TIME", "Ana_Sam_Mrs_Container_Id_Desc"))

``` 

Sorting MCCU decreasing shows none of the values are above the "no risk to aquatic life" BSA threshold of 0.75 CCU, great news.



Last, we need to remove extra samples not associated with FP/bio sampling events but not drop too much in case other programs grabbed samples on a different date. This is still best done manually. For 2022IR, Lucy manually screened data for just spring/fall sample information (using excel).

To do this, we need benthic data.



### Benthic Data

Use pinned benthic data to pull appropriate SCI information for sites.

```{r get ecoregion for SCI}
ecoregion <- pin_get("ejones/WQM-Stations-Spatial", board = "rsconnect") %>% 
        filter(StationID %in% WQM_Stations_Filter$StationID) %>% 
  dplyr::select(StationID, US_L3CODE, Basin) %>% 
  distinct(StationID, .keep_all = T)
```

Get all SCI information from pinned data. 

```{r SCI pins}
VSCIresults <- pin_get("ejones/VSCIresults", board = "rsconnect")
VCPMI63results <- pin_get("ejones/VCPMI63results", board = "rsconnect")
VCPMI65results <- pin_get("ejones/VCPMI65results", board = "rsconnect")
```

Bring in correct BenSampID info for sites.

```{r}
x2021Sites <- filter(WQM_Stations_Filter, Year == 2021)$StationID
x2022Sites <- filter(WQM_Stations_Filter, Year == 2022)$StationID

bioData2021 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2021Sites & year(`Collection Date`) == 2021) %>% 
  filter(str_detect(BenSampID, 'R110')) %>% # only keep rarified data
  filter(RepNum == 1)  # only use Rep1
bioData2022 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2022Sites & year(`Collection Date`) == 2022) %>% 
  filter(str_detect(BenSampID, 'R110')) %>% # only keep rarified data
  filter(RepNum == 1) # only use Rep1

# make sure no sites are missing biological data (our key to using a site)
x2021Sites[! x2021Sites %in% bioData2021$StationID]
x2022Sites[! x2022Sites %in% bioData2022$StationID]

benSamps_Filter_fin <- bind_rows(bioData2021, bioData2022) %>% 
  left_join(ecoregion, by = 'StationID')
```

Get correct SCI information. Make sure to grab the VSCI metrics for all stations not run with VSCI and have that ready for final data smashing. We report the VSCI metrics for VCPMI sites for continuity.

```{r}
# bring in bioassessment decisions
IR2024bioassessment <- pin_get("ejones/CurrentIRbioassessmentDecisions", board = "rsconnect") %>% 
  filter(IRYear == 2024 & StationID %in% benSamps_Filter_fin$StationID)

benSamps_Filter_finDecision <- left_join(benSamps_Filter_fin, dplyr::select(IR2024bioassessment, StationID, AssessmentMethod)) %>% 
  mutate(AssessmentMethod = case_when(StationID == '2-MTC006.95' ~ 'VSCI', 
                                      # pin_get("ejones/PreviousIRbioassessmentDecisions", board = "rsconnect") all other 2-MTC stations use VSCI when Mike assessed
                                      TRUE ~ AssessmentMethod ))

# after speaking with PRO and NRO regional bios, the following SCI overrides are suggested
#c("1AXOR000.47", '8-POR015.70') to VSCI
```


Now pull the actual metric information according to the SCI the bios have chosen

```{r}
SCI_filter <- filter(VSCIresults, BenSampID %in% filter(benSamps_Filter_finDecision, AssessmentMethod == "VSCI")$BenSampID) %>%
  bind_rows(
    filter(VCPMI63results, BenSampID %in% filter(benSamps_Filter_finDecision, AssessmentMethod == "VCPMI + 63")$BenSampID)  ) %>%
  bind_rows(
    filter(VCPMI65results, BenSampID %in% filter(benSamps_Filter_finDecision, AssessmentMethod == "VCPMI - 65")$BenSampID)  ) %>%
  # pre bioassessment method based on ecoregion and basin, bioassessment method is better bc bio makes SCI decision
  # filter(VSCIresults, BenSampID %in% filter(benSamps_Filter_fin, ! US_L3CODE %in% c(63,65))$BenSampID) %>%
  # bind_rows(
  #   filter(VCPMI63results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(63) | str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  # bind_rows(
  #   filter(VCPMI65results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(65) & !str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  mutate(SeasonGradient = as.factor(paste0(Season, " (",Gradient,")"))) %>%
  left_join(dplyr::select(benSamps_Filter_fin, StationID, BenSampID, RepNum, `Collection Date`) ,
              by = c('StationID', 'BenSampID', 'RepNum', 'Collection Date')) %>%

  # make year variable to make review easier
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, Season, BenSampID, RepNum, SCI, `SCI Score`, `SCI Threshold`,
                Gradient, `Target Count`,  `Sample Comments`, `Collected By`:`Entered Date`) %>% 
  arrange(Year, StationID, `Collection Date`)


# We report VSCI metrics regardless of SCI used for continuity 
SCImetrics <- filter(VSCIresults, BenSampID %in% SCI_filter$BenSampID) %>% 
  dplyr::select(BenSampID:`Fam %MFBI Score`)

SCIfinal <- left_join(SCI_filter, SCImetrics, by = c('BenSampID'))
```


While we are pulling data, in IR2022 NRO has requested we ensure the Po River trend site is only using VCPMI65. For IR2024, they want all VSCI.  We will repull all the data for this site, average it, and smash it in to the final dataset later, overwriting previous benthic information.

```{r po river fix}
po <- filter(VSCIresults, StationID == '8-POR015.70'& `Target Count` == 110) %>% 
  mutate(Year = year(`Collection Date`)) %>% 
  filter(Year %in% c(2004,2011,2013,2015,2017,2019))# just the samples associated with prob
# but we only report the VSCI metrics so grab those for these bensamps
poMetrics <- filter(VSCIresults, BenSampID %in% po$BenSampID)


# clean up workspace
rm(VSCIresults, VCPMI63results,VCPMI65results,bioData2021, bioData2022,benSamps_Filter_fin, SCI_filter, ecoregion)
```


Now we can save both of these objects out and manually delete the erroneous rows in probData (things not close to benthic collections). We will average the station data in R after extra data is dropped.

```{r save for manual review}
#write.csv(probData, 'processedData/prob20212022data.csv', row.names = F, na = '')
#write.csv(SCIfinal, 'processedData/bio20212022data.csv', row.names = F, na = '')
```

### Habitat Data

Next we need to get the total habitat scores for each benthic sample.

```{r tot hab}
habSamps <- bind_rows(pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% x2021Sites & year(`Collection Date`) == 2021) ,
                      pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% x2022Sites & year(`Collection Date`) == 2022))  %>% 
  filter(HabSampID != 'EBG16724') # bad data point in db
# make sure this is close to nrow SCI_filter
habValues <- pin_get('ejones/habValues', board = 'rsconnect') %>% 
  filter(HabSampID %in% habSamps$HabSampID) %>% 
  group_by(HabSampID) %>% 
  summarise(TotHab = sum(HabValue)) %>% 
  filter(HabSampID != 'EBG16724') # bad data point in db

totHab <- left_join(habSamps, habValues, by = "HabSampID") %>% 
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, TotHab)
# we will do the join to benthic data and average by year farther down the script

#write.csv(totHab, 'processedData/totHab.csv', row.names = F, na = '')

# clean up workspace
rm(habSamps, habValues)
```

### LRBS data

This analysis is run using the LRBS R code (May2023run.R) (now run by Aerin) and the MasterSummary_2023-6-1.csv was copy/pasted into this project's processed Data directory.

Data is summarized and QAed below.

```{r lrbs qa}
#names(prob2022) # get the variables we need to match

lrbs <- read.csv('processedData/MasterSummary_2023-06-01.csv') %>% 
  dplyr::select(StationID, Date, LRBS = LRBS2, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, 
                BL_CB_GR_Embed_PCT = BL_CB_GRmeanEmbed,
                Embed_PCT = Xembed ) %>% 
  mutate(Year = year(Date)) %>% 
  group_by(StationID, Year) %>% 
  summarise_at(vars(c(LRBS:Embed_PCT)), mean, na.rm = TRUE) 

# see what stations are missing data from this latest two years
View(left_join(WQM_Stations_Filter, lrbs, by = c('StationID', 'Year')))

# all missing LRBS sites are known issues

#write.csv(lrbs, 'processedData/FinalLRBS.csv', row.names = F, na = '')
```


### Geospatial data 

Watershed-specific landcover information is provided for all wadeable probmon sites each IR publication. In order to expedite the process of organizing and analyzing this geospatial dataset, automated scripts have been developed. This project calls these scripts. 

#### Tiger roads organization

Each year a new tiger road file is required to best compare sample year to roads in the watershed. Follow instructions in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsWebScrapingScript.R in order to scrape the FTP and organize each year into a single shapefile for analyses.

*** Remember: you need to first create a new directory named the year you are scraping (e.g. 2019) in the local C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsPull directory for downloadTigerRoadsByYear() and in unzipped directory to work properly.

#### Delineate new watersheds

The C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\Organize20212022prob.Rmd walks users through delineating wadeable and boatable sites sampled in 2021-2022 using automated streamstats delineation methods. Manual review is conducted in either R or ArcGIS. 

#### Landcover analysis

Once all the necessary stations (wadeable) have QAed watershed information, landcover analysis scripts in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\landcoverAnalysis_20212022.R take users through the automated process of generating the ~100 landcover metrics the prob chapter reports out for wadeable dataset.



## QA data

From here, even though it is painful, the best QA method is to open up prop20212022data.csv and bio20212022data.csv and manually go through the chemistry and bug results to make sure all bug samples are there and consolidate/delete extra field/chem data (this result is stored in prop20212022data_QAed.csv). Splits, wonky field/chem collection compared to bugs, etc. can make this automation of that QA process not worth it.

### Summarize Field/Chem/Bio by Year

Bring in this clean field/chem/metals data, average by year, and output one more time for super QA. Note that mercury is separate bc we want the comment code. Hg collected once annually so the average is of 1. 

```{r}
probDataQAed <- read.csv('processedData/prob20212022data_QAed.csv') %>% 
  rename(`Ortho-P` = 'Ortho.P',
         `Hg-C` = 'Hg.C') %>% 
    # make sure only one row per date/time
  group_by(StationID, FDT_DATE_TIME) %>% 
  mutate(n = n()) %>% 
  dplyr::select(n , StationID: SSCTOTAL, MetalCCU, everything()) %>% ungroup()
  # if everything fine, proceed

  
# average by year
probDataQAedAvg <- probDataQAed %>% 
  dplyr::select(-c(n, FDT_DATE_TIME, Ana_Sam_Mrs_Container_Id_Desc, FDT_SPG_CODE, FDT_COMMENT, FDT_DEPTH)) %>% ungroup() %>% 
  # get one row per StationID
  group_by(StationID, Year) %>% 
  summarise_at(vars(DO:HARDNESS), mean, na.rm = TRUE) %>% 
  # change NaN to NA
  pivot_longer(cols =DO:HARDNESS,names_to = 'Parameter', values_to = 'Value')
probDataQAedAvg$Value[probDataQAedAvg$Value== 'NaN'] <- NA
probDataQAedAvg <- pivot_wider(probDataQAedAvg, names_from = 'Parameter', values_from = 'Value')


# have to do mercury separately bc we want the comment code
probDataQAedHg <- dplyr::select(probDataQAed, StationID, Year, MERCURY, `Hg-C`) %>%  
  mutate(`Hg-C` = as.character(`Hg-C`)) %>% 
  filter(!is.na(MERCURY) | !is.na(`Hg-C`)) %>%
  filter(MERCURY != "" | `Hg-C` != "") %>% 
  group_by(StationID, Year) %>% 
  summarise(MERCURY = mean(MERCURY, na.rm = T),
            HgCom = unique(`Hg-C`)) 
# change any NaN's back to NA and review manually to make sure no data lost
probDataQAedHg$MERCURY[probDataQAedHg$MERCURY== 'NaN'] <- NA


# smash lucyAvg and lucyHg together to a single, clean dataset
probDataQAedFin <- left_join(probDataQAedAvg, probDataQAedHg, by = c('StationID', 'Year'))



# save and manually review one last time
#write.csv(probDataQAedFin, 'processedData/FinalFieldAnalyteMetals.csv', row.names = F, na= '')
# went through and manually checked all blanks, 0's, and random points to make sure data looks good

# clean up workspace
rm(probDataQAedAvg, probDataQAed)
```

### Summarize Bugs

Now we need to convert the VCPMI scores to VSCI scale for analyses and make one row per bug record where more than one sample were collected per year to ensure data joins cleanly and CDF analysis can run.

```{r bugAvg and hab avg}
source('VCPMItoVSCIconversion.R')

bugAvg <- SCIfinal %>% 
  full_join(totHab, by = c('StationID', 'Collection Date', 'Year')) %>% 
  
  # # QA data
  # dplyr::select(-c(`Family Total Taxa`:`Fam %MFBI Score`)) %>%
  # group_by(StationID, `Collection Date`) %>%
  # mutate(n = n())

  filter(!is.na(SCI)) %>% # 2-HOU001.51 dropped bc bad sample
  
  group_by(StationID, Year, SCI) %>% 
  summarise_at(vars(c(`SCI Score`, `Family Total Taxa`:TotHab)), mean, na.rm = TRUE) %>% 
  mutate(VSCIVCPMI = case_when(SCI == 'VSCI' ~ `SCI Score`,
                               SCI != 'VSCI' ~ VCPMItoVSCIconversion(`SCI Score`, probRatio),
                               TRUE ~ as.numeric(NA))) %>% 
  dplyr::select(StationID, Year, TotHab,
                TotTaxa = `Family Total Taxa`,
                EPTTax = `Family EPT Taxa`,
                VEphem = `%Ephem`,
                VPTHydropsychidae = `%PT - Hydropsychidae`,
                VScrap = `%FamilyScraper`,
                VChiro = `%Chiro`,
                V2Dom = `Family %2 Dominant`,
                HBI = `Family HBI`,
                #VHapto = ????????? # dropped from this report bc could not make joins to old EDAS taxa info easily after EDAS-CEDS data updates
                EPTInd = `Family EPT Taxa`,
                VSCIVCPMI)

# no habitat 8-NAR006.75 2021
  


#write.csv(bugAvg, 'processedData/FinalBugHabitatMetrics.csv', row.names = F, na = '')
```


And back to the Po fix, run the VSCI conversion on those samples and combine the benthic metrics by year.

```{r po fix2}
# first get VSCI metrics associated with site and VCPMI65 score
poFix <- left_join(dplyr::select(poMetrics, BenSampID: `Fam %MFBI Score`, StationID),
                   dplyr::select(po, BenSampID, SCI, `SCI Score`, Year, StationID),
                   by = c('BenSampID',  'StationID')) %>% 
# then average everything by year
group_by(StationID, Year, SCI) %>% 
summarise_at(vars(c(`SCI Score`, `Family Total Taxa`:`Fam %MFBI Score`)), mean, na.rm = TRUE) %>% 
# then covert average VCPMI65 to VSCIVCPMI scale
  mutate( VSCIVCPMI = case_when(SCI == 'VSCI' ~ `SCI Score`,
                               SCI != 'VSCI' ~ VCPMItoVSCIconversion(`SCI Score`, probRatio),
                               TRUE ~ as.numeric(NA))) %>% 
  dplyr::select(StationID, Year,# TotHab,
                TotTaxa = `Family Total Taxa`,
                EPTTax = `Family EPT Taxa`,
                VEphem = `%Ephem`,
                VPTHydropsychidae = `%PT - Hydropsychidae`,
                VScrap = `%FamilyScraper`,
                VChiro = `%Chironomidae Score`,
                V2Dom = `Family %2 Dominant`,
                HBI = `Family HBI`,
                #VHapto = ????????? # dropped from this report bc could not make joins to old EDAS taxa info easily after EDAS-CEDS data updates
                EPTInd = `Family EPT Taxa`,
                VSCIVCPMI) 

rm(po); rm(poMetrics)
# clean up workspace
rm(probRatio); rm(SCIfinal); rm(bugAvg); rm(SCImetrics); rm(totHab)

```

Now overwrite these results for all previous Po results.

```{r Po Overwrite}
prob2022new <- filter(prob2022, StationID != '8-POR015.70')
prob2022Po <- filter(prob2022, StationID == '8-POR015.70') %>% 
  dplyr::select(-c(TotTaxa:VSCIVCPMI)) %>% 
  mutate(VHapto = NA) %>% # placeholder
  left_join(poFix, by = c('StationID', 'Year')) %>% 
  dplyr::select(names(prob2022new))

prob2022 <- bind_rows(prob2022new, prob2022Po) %>% 
  arrange(Year, StationID)

#write.csv(prob2022, 'processedData/prob2022PoFix.csv', row.names = F, na = '')

rm(prob2022new, prob2022Po, poFix, poMetrics, po)
```



# Combine data


Bring in corrected prob2022 with Po river fix.

```{r prob2020}
prob2022 <- read.csv('processedData/prob2022PoFix.csv') %>% 
   rename('Ortho-P' = 'Ortho.P',
         'Hg-C' = 'Hg.C', 
         '70331VFine' = 'X70331VFine')
```

There seem to be some mislabeled Ecoregions and bay/nonbay metadata in legacy data. Fix that now!

```{r}
prob2022ecoBay <- prob2022 %>% 
  left_join(pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect') %>%
              filter(StationID %in% prob2022$StationID) %>% 
              distinct(StationID, .keep_all = T) %>% 
              dplyr::select(StationID, Basin, US_L3NAME, Basin_Code), 
            by = 'StationID') %>% 
  mutate(Basin = coalesce(Basin.y, Basin.x),
         EcoRegion = coalesce(US_L3NAME, EcoRegion)) %>% 
  #dplyr::select(Basin.x, Basin.y, Basin, everything())
  mutate(Basin = case_when(Basin %in% c("James River Basin", "James" ) ~ "James",
                           Basin %in% c("Roanoke River Basin" , "Roanoke") ~ "Roanoke",
                           Basin %in% c( "New River Basin" , "New" ) ~ "New",
                           Basin %in% c( "Potomac River Basin" , "Shenandoah River Basin", "Potomac-Shenandoah") ~ "Potomac-Shenandoah",
                           Basin %in% c("Tennessee and Big Sandy River Basin", "Tennessee" ) ~  "Tennessee",
                           Basin %in% c("Rappahannock River Basin", "York River Basin",  "Rappahannock-York") ~ "Rappahannock-York",
                           Basin %in% c(  "Chowan and Dismal Swamp River Basin","Chowan"  ) ~   "Chowan" ,
                           Basin %in% c('Ches. Bay and Small Coastal Basin') ~ 'Chesapeake Bay',
                           TRUE ~ as.character(NA)),
         EcoRegion = case_when(US_L3NAME %in% c("Ridge and Valley" ) ~ "Central Appalachian Ridges and Valleys",
                               US_L3NAME %in% c("Blue Ridge" ) ~ "Blue Ridge Mountains",
                               TRUE ~ as.character(US_L3NAME)),
         SubBasin =  case_when(Basin_Code %in% c("James-Lower", "James-Middle", "James-Upper","Appomattox"  ) ~ "James",   
                               Basin_Code %in% c("Roanoke", "Yadkin") ~ "Roanoke",
                               Basin_Code %in% c( "New"  ) ~ "New",
                               Basin_Code %in% c(  "Potomac-Lower") ~  "Potomac" ,
                               Basin_Code%in% c("Potomac-Shenandoah") ~  "Shenandoah",
                               Basin_Code %in% c("Rappahannock") ~ "Rappahannock",
                               Basin_Code%in% c("York"  ) ~  "York" ,
                               Basin_Code %in% c("Chowan-Dismal" ) ~ "Chowan",
                               Basin_Code %in% c("Tennessee-Holston" ) ~  "Holston",
                               Basin_Code  %in% c( "Tennessee-Clinch"  ) ~   "Clinch-Powell" ,
                               Basin_Code  %in% c( "Tennessee-Big Sandy") ~  "Big Sandy",
                               # hardly ever used "Cheasapeake Bay"
                               TRUE ~ as.character(SubBasin)), # use old designation if the station doesnt have a record in teh pinned dataset(means it's not in CEDS)
         BayShed = case_when(Basin %in% c("James","Rappahannock-York","Potomac-Shenandoah", 'Chesapeake Bay') ~ "Bay",
                             TRUE ~ as.character("NonBay")),
         BioRegion = case_when(EcoRegion %in% c("Middle Atlantic Coastal Plain", "Southeastern Plains") ~ "Coast",
                               EcoRegion %in% c("Piedmont", "Northern Piedmont") ~ "Piedmont",
                               TRUE ~ as.character('Mountain')),
         BasinSize = case_when(AREA_SQ_MILES < 1 ~ 1,
                               between(AREA_SQ_MILES, 1, 9.99999999) ~ 2, #between is >= and <= 
                               between(AREA_SQ_MILES, 10, 49.9999999) ~ 3, #between is >= and <= 
                               AREA_SQ_MILES >= 50 ~ 4,
                               TRUE ~ as.numeric(NA)),
         StreamSizeCat = case_when(BasinSize %in% c(1,2) ~ 'Small',
                                   BasinSize == 3 ~ "Medium",
                                   BasinSize == 4 ~ "Large",
                                   TRUE ~ as.character(NA))) %>% 
  dplyr::select(names(prob2022))

prob2022 <- prob2022ecoBay
```




And the Jackson river site has issues. 2-JKS070.97

```{r}
prob2022 <- mutate(prob2022, StationID = case_when(StationID == '2-JKS070.97' ~ '2-JKS063.03',
                                                         TRUE ~ as.character(StationID)),
                      StationID_Trend = case_when(StationID_Trend == '2-JKS070.97' ~ '2-JKS063.03',
                                                         TRUE ~ as.character(StationID_Trend))) 
# fix landcover
prob2022Fine <- filter(prob2022, StationID != '2-JKS063.03')
prob2022Fix <- filter(prob2022, StationID == '2-JKS063.03') %>% 
  dplyr::select(DataSource:`Hg-C`) %>% 
  left_join(read.csv('C:/HardDriveBackup/R/GitHub/LandcoverAnalysis/Results/ProbWadeableAreaFixesLandcoverRerun/JacksonFix/Result10.csv'),
                     by = 'StationID')

prob2022 <- bind_rows(prob2022Fine, prob2022Fix) %>% 
  arrange(Year, StationID)
rm(prob2022Fine, prob2022Fix)
```



Bring in Field, Analyte, and Metals Data

```{r field}
field <- read.csv('processedData/FinalFieldAnalyteMetals.csv') %>%
  rename('Ortho-P' = 'Ortho.P',
         'Hg-C' = 'HgCom', 
         '70331VFine' = 'X70331VFine')

```

Bring in Bug and habitat data

```{r bug}
bugs <- read.csv('processedData/FinalBugHabitatMetrics.csv') 
```


Bring in LRBS data

```{r lrbs}
lrbs <- read.csv('processedData/FinalLRBS.csv')
```

Bring in metadata

```{r}
stationMetadata <- read_excel('C:/Users/wmu43954/OneDrive - Commonwealth of Virginia/Freshwater ProbMon/IR2024/Wadeable_ProbMon2021-2022.xlsx',
                                  #'originalData/Wadeable_ProbMon2021-2022.xlsx', # or from here for local archiving
                                  sheet = 'withMetadata') %>% 
    filter(status == 'TS') %>% # keep only wadeable sites that were sampled for data querying
  mutate(DataSource = 'State ProbMon') %>% 
  dplyr::select(DataSource, StationID, Year, StationID_Trend, LongitudeDD = Longitude, LatitudeDD = Latitude, stratum, designweight,
                weightcategory, station, state, status, comment, set) %>%
  mutate(station = as.numeric(NA)) %>% #drop this count bc confusing
  arrange(Year)
```




Bring in landcover data

```{r landcover}
landcover <- read.csv('C:/HardDriveBackup/R/GitHub/LandcoverAnalysis/Results/ProbWadeable20212022/Result10.csv') 
# fix buffalo trend issue
buff <- filter(landcover, StationID == '4ABNN002.17') %>% 
  mutate(YearSampled = c(2021,2022))
landcover <- landcover %>% 
  filter(StationID != '4ABNN002.17') %>% 
  bind_rows(buff) %>% 
  arrange(YearSampled)

rm(buff)
```

Smash data

```{r data smash}
prob2022fin <- prob2022 %>% 
  mutate(IR2024 = as.numeric(NA)) %>% #placeholder
  dplyr::select(DataSource:IR2022, IR2024, DO:STXRD ) 
#names(prob2020fin) # what to match

finalSmash <- stationMetadata %>% #dplyr::select(WQM_Stations_Filter, DataSource:set) %>% 
  # add in population information later
  left_join(field, by = c('StationID', 'Year')) %>% 
  left_join(lrbs, by = c('StationID', 'Year')) %>% 
  left_join(bugs, by = c('StationID', 'Year')) %>% 
  left_join(landcover %>%
              mutate(Year = YearSampled),
            by = c('StationID', 'Year'))



# clean up workspace
rm(bugs);rm(field);rm(landcover);rm(lrbs);rm(metalsCCUresults);rm(probData)
```




```{r}
names(prob2022fin)[! names(prob2022fin) %in% names(finalSmash)]

```



## Attach final metadata


First, where possible, get metadata from previous years.

```{r grab previous metdata}
# get what you can from previous metadat
previousInfo <- filter(prob2022fin, StationID %in% finalSmash$StationID) %>% 
  distinct(StationID, .keep_all = T)
previousInfoMetadata <-  filter(prob2022fin, StationID %in% previousInfo$StationID) %>%  
                                  dplyr::select(StationID, Basin, SubBasin, BayShed, EcoRegion, 
                                                BioRegion, Order, BasinSize, StreamSizeCat) %>% 
                                  distinct(StationID, .keep_all = T)
```

Use pinned data to get other basic metadata.

```{r new sites}
finalSmashMetadata <- filter(finalSmash, ! StationID %in% previousInfoMetadata$StationID) %>% 
  left_join(pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect') %>%
              filter(StationID %in% finalSmash$StationID) %>% 
              distinct(StationID, .keep_all = T) %>% 
              dplyr::select(StationID, Basin, US_L3NAME, Basin_Code), 
            by = 'StationID') %>% 
  mutate(Basin = case_when(Basin %in% c("James River Basin" ) ~ "James",
                           Basin %in% c("Roanoke River Basin" ) ~ "Roanoke",
                           Basin %in% c( "New River Basin"  ) ~ "New",
                           Basin %in% c( "Potomac River Basin" , "Shenandoah River Basin") ~ "Potomac-Shenandoah",
                           Basin %in% c("Tennessee and Big Sandy River Basin" ) ~  "Tennessee",
                           Basin %in% c("Rappahannock River Basin", "York River Basin" ) ~ "Rappahannock-York",
                           Basin %in% c(  "Chowan and Dismal Swamp River Basin"  ) ~   "Chowan" ,
                           Basin %in% c('Ches. Bay and Small Coastal Basin') ~ 'Chesapeake Bay',
                           TRUE ~ as.character(NA)),
         EcoRegion = case_when(US_L3NAME %in% c("Ridge and Valley" ) ~ "Central Appalachian Ridges and Valleys",
                               US_L3NAME %in% c("Blue Ridge" ) ~ "Blue Ridge Mountains",
                               TRUE ~ as.character(US_L3NAME)),
         SubBasin =  case_when(Basin_Code %in% c("James-Lower", "James-Middle", "James-Upper","Appomattox"  ) ~ "James",   
                               Basin_Code %in% c("Roanoke", "Yadkin") ~ "Roanoke",
                               Basin_Code %in% c( "New"  ) ~ "New",
                               Basin_Code %in% c(  "Potomac-Lower") ~  "Potomac" ,
                               Basin_Code%in% c("Potomac-Shenandoah") ~  "Shenandoah",
                               Basin_Code %in% c("Rappahannock") ~ "Rappahannock",
                               Basin_Code%in% c("York"  ) ~  "York" ,
                               Basin_Code %in% c("Chowan-Dismal" ) ~ "Chowan",
                               Basin_Code %in% c("Tennessee-Holston" ) ~  "Holston",
                               Basin_Code  %in% c( "Tennessee-Clinch"  ) ~   "Clinch-Powell" ,
                               Basin_Code  %in% c( "Tennessee-Big Sandy") ~  "Big Sandy",
                               # hardly ever used "Cheasapeake Bay"
                               TRUE ~ as.character(NA)),
         BayShed = case_when(Basin %in% c("James","Rappahannock-York","Potomac-Shenandoah", 'Chesapeake Bay') ~ "Bay",
                             TRUE ~ as.character("NonBay")),
         # BayShed = case_when(Basin %in% c("James River Basin","Potomac River Basin" , "Shenandoah River Basin",
         #                                  "Rappahannock River Basin", "York River Basin") ~ "Bay",
         #                     TRUE ~ as.character("NonBay")),
         BioRegion = case_when(EcoRegion %in% c("Middle Atlantic Coastal Plain", "Southeastern Plains") ~ "Coast",
                               EcoRegion %in% c("Piedmont", "Northern Piedmont") ~ "Piedmont",
                               TRUE ~ as.character('Mountain')),
         StationID_Trend = StationID,
         Order = weightcategory,
         AREA_SQ_MILES = totalArea_sqMile,
         BasinSize = case_when(AREA_SQ_MILES < 1 ~ 1,
                               between(AREA_SQ_MILES, 1, 9.99999999) ~ 2, #between is >= and <= 
                               between(AREA_SQ_MILES, 10, 49.9999999) ~ 3, #between is >= and <= 
                               AREA_SQ_MILES >= 50 ~ 4,
                               TRUE ~ as.numeric(NA)),
         StreamSizeCat = case_when(BasinSize %in% c(1,2) ~ 'Small',
                                   BasinSize == 3 ~ "Medium",
                                   BasinSize == 4 ~ "Large",
                                   TRUE ~ as.character(NA))) %>% 
  dplyr::select(DataSource:set, Basin, SubBasin, BayShed, EcoRegion, BioRegion, Order, BasinSize,
                StreamSizeCat,AREA_SQ_MILES, DO:STXRD) %>% 
  bind_rows(
     filter(finalSmash, StationID %in% previousInfoMetadata$StationID) %>% 
       left_join(previousInfoMetadata, by = 'StationID') %>% 
       mutate(AREA_SQ_MILES = totalArea_sqMile) %>% 
       dplyr::select(DataSource:set, Basin, SubBasin, BayShed, EcoRegion, BioRegion, Order, BasinSize,
                     StreamSizeCat, AREA_SQ_MILES, DO:STXRD)
     )


names(prob2022fin)[! names(prob2022fin) %in% names(finalSmashMetadata)]

```

These are the data windows we are looking at for 2024 IR:

* Full sample window (2001-2022)
* 2022 IR (2017-2022)
* 2022 IR (2015-2020)
* 2020 IR (2013-2018)
* 2018 IR (2011-2016)
* 2016 IR (2009-2014)
* 2014 IR (2007-2012)
* 2012 IR (2005-2010)
* 2010 IR (2003-2008)
* 2008 IR (2001-2007)
* Panels (Phase 1= 2001-2011; Phase 2= 2012-2022) 
* BioPanels (Phase 1 = 2001-2005; Phase 2 = 2006-2010; Phase 3 = 2011-2015; Phase 4 = 2016-2020; Phase 5 = 2021-2022) makes sense with draw

```{r}
# IR 2022 breakdown
#* Panels (Phase 1= 2001-2010; Phase 2= 2011-2020) Kept Phase 1 same as 2018IR to balance n samples in window
#* BioPanels (Phase 1 = 2001-2005; Phase 2 = 2006-2010; Phase 3 = 2011-2015; Phase 4 = 2016-2020)

```



```{r temporal metadata}
finalSmashMetadata <- mutate(finalSmashMetadata,
                            IR2008 = case_when(between(Year, 2001, 2007)~ 2008, TRUE ~ as.numeric(NA)),
                            IR2010 = case_when(between(Year, 2003, 2008)~ 2010, TRUE ~ as.numeric(NA)),
                            IR2012 = case_when(between(Year, 2005, 2010)~ 2012, TRUE ~ as.numeric(NA)),
                            IR2014 = case_when(between(Year, 2007, 2012)~ 2014, TRUE ~ as.numeric(NA)),
                            IR2016 = case_when(between(Year, 2009, 2014)~ 2016, TRUE ~ as.numeric(NA)),
                            IR2018 = case_when(between(Year, 2011, 2016)~ 2018, TRUE ~ as.numeric(NA)),
                            IR2020 = case_when(between(Year, 2013, 2018)~ 2020, TRUE ~ as.numeric(NA)),
                            IR2022 = case_when(between(Year, 2015, 2020)~ 2022, TRUE ~ as.numeric(NA)),
                            IR2024 = case_when(between(Year, 2017, 2022)~ 2022, TRUE ~ as.numeric(NA)),
                            # new IR here
                            Panel = case_when(between(Year, 2001, 2011)~ 'Phase1',
                                              between(Year, 2012, 2022)~ 'Phase2',
                                              TRUE ~ as.character(NA)),
                            BioPanel = case_when(between(Year, 2001, 2005)~ 'Phase1',
                                              between(Year, 2006, 2010)~ 'Phase2',
                                              between(Year, 2011, 2015)~ 'Phase3',
                                              between(Year, 2016, 2020)~ 'Phase4',
                                              between(Year, 2021, 2022)~ 'Phase5',
                                              TRUE ~ as.character(NA)),
                            BayPanel = paste0(BayShed, Panel),
                            StreamSizeCatPhase = paste0(Panel,StreamSizeCat ),
                            DataSource = "State ProbMon") %>% # change region info to State ProbMon 
  dplyr::select(any_of(names(prob2022fin)))


names(prob2022fin)[! names(prob2022fin) %in% names(finalSmashMetadata)]


```


Spatially join prob2022 to this VAHUSB dataset and 4th order watershed as well




```{r}
# how we made the vahusb dataset

# vahusb <- st_read('C:/HardDriveBackup/GIS/Assessment/AssessmentRegions_VA84_basins.shp') %>% 
#   group_by(VAHUSB) %>% 
#   summarise() %>% 
#   mutate(VAHUSB_NAME = case_when(VAHUSB == 'AO' ~ 'Atlantic Ocean Coastal',
#                                  VAHUSB == 'AS' ~ 'Albemarle Sound',
#                                  VAHUSB == 'BS' ~ 'Big Sandy River',
#                                  VAHUSB == 'CB' ~ 'Chesapeake Bay/Chesapeake Bay Coastal',
#                                  VAHUSB == 'CL' ~ 'Chowan River, Lower',
#                                  VAHUSB == 'CM' ~ 'Chowan River-Meherrin River',
#                                  VAHUSB == 'CU' ~ 'Chowan River, Upper',
#                                  VAHUSB == 'JA' ~ 'James River- Appomattox River',
#                                  VAHUSB == 'JL' ~ 'James River, Lower (Tidal)',
#                                  VAHUSB == 'JM' ~ 'James River, Middle (Piedmont)',
#                                  VAHUSB == 'JR' ~ 'James River- Rivanna River',
#                                  VAHUSB == 'JU' ~ 'James River, Upper (Mountain)',
#                                  VAHUSB == 'NE' ~ 'New River',
#                                  VAHUSB == 'PL' ~ 'Potomac River, Lower',
#                                  VAHUSB == 'PS' ~ 'Potomac River-Shenandoah River',
#                                  VAHUSB == 'PU' ~ 'Potomac River, Upper',
#                                  VAHUSB == 'RA' ~ 'Rappahannock River',
#                                  VAHUSB == 'RD' ~ 'Roanoke River- Dan River',
#                                  VAHUSB == 'RL' ~ 'Roanoke River, Lower',
#                                  VAHUSB == 'RU' ~ 'Roanoke River, Upper',
#                                  VAHUSB == 'TC' ~ 'Tennessee-Clinch River',
#                                  VAHUSB == 'TH' ~ 'Tennessee-Holston River',
#                                  VAHUSB == 'TP' ~ 'Tennessee-Powell River',
#                                  VAHUSB == 'YA' ~ 'Yadkin River-Ararat River',
#                                  VAHUSB == 'YO' ~ 'York River',
#                                  TRUE ~ as.character(NA)))
# 
# View(vahusb %>% st_drop_geometry())
# 
# #st_write(vahusb, 'C:/HardDriveBackup/GIS/Assessment/VAHUSB.shp')
``` 

```{r new basin analyses}
library(sf)


vahusb <- st_read('C:/HardDriveBackup/GIS/Assessment/VAHUSB.shp') %>% 
  rename(VAHUSB_NAME = VAHUSB_)
#wshd4Order <- st_read("C:/HardDriveBackup/GIS/GIS_BaseLayer_Datasets.gdb",'VA_SUBBASINS_4TH_ORDER_STG')

finalSmashMetadatasf <- finalSmashMetadata %>% 
    st_as_sf(coords = c("LongitudeDD", "LatitudeDD"), 
               remove = F, # don't remove these lat/lon cols from df
               crs = 4326) %>% # add projection, needs to be geographic for now bc entering lat/lng
  st_intersection(vahusb) 


# after careful review, this step was dropped because a station was lost in this intersection and the n in each category didn't prove useful for further analysis
#%>% 
  #st_intersection(dplyr::select(wshd4Order, Basin4thOrder = NAME)) 


prob2024fin <- finalSmashMetadatasf %>% 
  dplyr::select(DataSource:SubBasin, VAHUSB, VAHUSB_NAME,# Basin4thOrder, 
                BayShed:STXRD) %>% 
  # sf doesn't like modern names in this version of R
  rename('Ortho-P' = 'Ortho.P',
         'Hg-C' = 'Hg.C', 
         '70331VFine' = 'X70331VFine') %>% 
  arrange(Year, StationID) %>% 
  st_drop_geometry()

names(prob2022fin) == names(prob2024fin)
```


## Smash old and new data

```{r old and new}
prob2024 <- bind_rows(prob2022fin, prob2024fin ) %>% 
    arrange(Year, StationID) %>% 
  # fix station field
  mutate(station = 1:n()) 
```


Remove the 1's in the IR columns and IR years for full dataset

```{r}
prob2024fin <- prob2024 %>% 
  mutate(IR2008 = case_when(between(Year, 2001, 2007)~ 2008, TRUE ~ as.numeric(NA)),
         IR2010 = case_when(between(Year, 2003, 2008)~ 2010, TRUE ~ as.numeric(NA)),
         IR2012 = case_when(between(Year, 2005, 2010)~ 2012, TRUE ~ as.numeric(NA)),
         IR2014 = case_when(between(Year, 2007, 2012)~ 2014, TRUE ~ as.numeric(NA)),
         IR2016 = case_when(between(Year, 2009, 2014)~ 2016, TRUE ~ as.numeric(NA)),
         IR2018 = case_when(between(Year, 2011, 2016)~ 2018, TRUE ~ as.numeric(NA)),
         IR2020 = case_when(between(Year, 2013, 2018)~ 2020, TRUE ~ as.numeric(NA)),
         IR2022 = case_when(between(Year, 2015, 2020)~ 2022, TRUE ~ as.numeric(NA)),
         IR2024 = case_when(between(Year, 2017, 2022)~ 2024, TRUE ~ as.numeric(NA)),

         # and fix some basin size mislabels in previous data
          BasinSize = case_when(AREA_SQ_MILES < 1 ~ 1,
                               between(AREA_SQ_MILES, 1, 9.99999999) ~ 2, #between is >= and <= 
                               between(AREA_SQ_MILES, 10, 49.9999999) ~ 3, #between is >= and <= 
                               AREA_SQ_MILES >= 50 ~ 4,
                               TRUE ~ as.numeric(NA)),
         StreamSizeCat = case_when(BasinSize %in% c(1,2) ~ 'Small',
                                   BasinSize == 3 ~ "Medium",
                                   BasinSize == 4 ~ "Large",
                                   TRUE ~ as.character(NA)),
                            Panel = case_when(between(Year, 2001, 2011)~ 'Phase1',
                                              between(Year, 2012, 2022)~ 'Phase2',
                                              TRUE ~ as.character(NA)),
                            BioPanel = case_when(between(Year, 2001, 2005)~ 'Phase1',
                                              between(Year, 2006, 2010)~ 'Phase2',
                                              between(Year, 2011, 2015)~ 'Phase3',
                                              between(Year, 2016, 2020)~ 'Phase4',
                                              between(Year, 2021, 2022)~ 'Phase5',

                                              TRUE ~ as.character(NA)),
                            BayPanel = paste0(BayShed, Panel),
                            StreamSizeCatPhase = paste0(Panel,StreamSizeCat )
         )
```


# Fix Lat/Lng

There are a number of trend sites taht have different lat/lng depending on the year. We will join the CEDS coordinates and be done with that issue.




```{r}
multiStationGIS_ViewAll <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(prob2024fin$StationID)) %>%
  as_tibble() %>% 
  dplyr::select(Station_Id, Longitude, Latitude)

sitesNotInCEDS <- tribble(
  ~Station_Id,  ~Longitude, ~Latitude,
  '5ANTW045.12',	-77.190038, 	36.866562,
  '6CSLM002.11',	-81.407864,	36.809949,
  '2-RVN035.67',	-78.4155,	38.008844,
  '2-STH000.50',	-79.378135,	37.773018,
  '2-WLN006.90',	-79.80374,	37.897411,
  '3-MTN003.31',	-77.796595,	38.455726,
  '9-ELK013.81',	-81.164599,	36.710784,
  '9-LFK005.39',	-81.171301,	37.243087,
  '4AXUY000.58', -78.8769987,	36.64839836,
  '4AROC010.68',	-78.56410796,	37.1086978,
  '9-BST069.21',	-81.28267087,	37.2601764,
  '3-JOA002.68', -77.90597222,	38.494275,
  '2BFSP001.09', -78.654028,	37.366806,
  '1AXLN000.27', -77.520753,	38.490531,
  '2BMBB000.60', -78.7752,	37.519419
)

multiStationGIS_ViewAll <- bind_rows(multiStationGIS_ViewAll, sitesNotInCEDS)

prob2024fin2 <- left_join(prob2024fin, multiStationGIS_ViewAll, by = c('StationID' = 'Station_Id')) %>% 
  mutate(LatitudeDD = Latitude,
         LongitudeDD = Longitude) %>% 
  #dplyr::select(Latitude, Longitude, everything())
  dplyr::select(-c(Latitude, Longitude))
  
names(prob2024fin2) ==names(prob2024fin)


prob2024fin <- prob2024fin2
```




```{r save data for review}
write.csv(prob2024fin, 'processedData/Wadeable_ProbMon_2001-2022.csv', row.names = F, na  ='')

# write.csv(prob2024fin %>% 
#   group_by(Basin) %>% 
#   count(), 'processedData/summaries/BasinSummary.csv', row.names = F, na  ='')
# 
# 
# write.csv(prob2024fin %>% 
#   group_by(SubBasin) %>% 
#   count, 'processedData/summaries/SubBasinSummary.csv', row.names = F, na  ='')
# 
# write.csv(prob2024fin %>% 
#   group_by(VAHUSB) %>% 
#   count(), 'processedData/summaries/VAHUSBSummary.csv', row.names = F, na  ='')

# prob2024fin %>% 
#   group_by(VAHUSB) %>% 
#   count() %>% 
#   arrange(desc(n)) %>% View() # this is helpful to update which subbasins we want to run populations estimates on. we want about 30 sites before we run these. Check in CDFanalysis.R to make sure all populations with that criteria are being analyzed

# write.csv(prob2024fin %>% 
#   group_by(Basin4thOrder) %>% 
#   count(), 'processedData/summaries/Basin4OrderSummary.csv', row.names = F, na  ='')
```









# Design Status update

We also want to add back in the full design status sites for an updated design status dataset.

Note the sampleID field uses the StationID_Trend information to ensure each station has a unique name for CDF analysis purposes. 

We also need to update the design status window information to match the new window breakdowns.

**Make sure you have each weight as the original design weight in this spreadsheet. Subsquent steps in CDFanalysis.R recalculate the weight according to numerous window scenarios based on the original weight (and handle all trend weight adjustments).**

`1`="3790.5165999999999",  
`2`="947.62919999999997",
`3`="541.50239999999997",
`4`="315.87639999999999", 
`5`="140.3895", 
`6`="140.3895"

```{r}
designStatusOld <- read.csv('originalData/designStatusIR2022_EVJyearFixes.csv') %>% 
  mutate(sampleID = case_when(sampleID == '1AQUA004.14'~'1AQUA004.35',
                              sampleID == '2-JKS070.97' ~ '2-JKS063.03',
                              TRUE ~ as.character(sampleID)))
  
designStatusOld <- designStatusOld %>% 
  # use fixed metadata from above
  left_join(dplyr::select(prob2024fin, StationID_Trend, Basin:BioRegion),
            by = c('sampleID'='StationID_Trend')) %>% 
  mutate(Basin = coalesce(Basin.y, Basin.x),
         SubBasin = coalesce(SubBasin.y, SubBasin.x), 
         VAHUSB = coalesce(VAHUSB.y, VAHUSB.x),
         VAHUSB_NAME = coalesce(VAHUSB_NAME.y, VAHUSB_NAME.x),
         BayShed = coalesce(BayShed.y, BayShed.x),
         BayPanel = coalesce(BayPanel.y, BayPanel.x), 
         EcoRegion = coalesce(EcoRegion.y, EcoRegion.x), 
         BioRegion = coalesce(BioRegion.y, BioRegion.x)) %>% 
  dplyr::select(names(designStatusOld)) %>% 
         
  
  mutate(Station_Id = as.character(Station_Id),
         Station_Id = na_if(Station_Id, "")) %>%  
  left_join(dplyr::select(prob2024fin, StationID_Trend, AREA_SQ_MILES),
            by = c('sampleID' = 'StationID_Trend')) %>% 
  mutate(IR2008 = case_when(between(Year, 2001, 2007)~ 2008, TRUE ~ as.numeric(NA)),
         IR2010 = case_when(between(Year, 2003, 2008)~ 2010, TRUE ~ as.numeric(NA)),
         IR2012 = case_when(between(Year, 2005, 2010)~ 2012, TRUE ~ as.numeric(NA)),
         IR2014 = case_when(between(Year, 2007, 2012)~ 2014, TRUE ~ as.numeric(NA)),
         IR2016 = case_when(between(Year, 2009, 2014)~ 2016, TRUE ~ as.numeric(NA)),
         IR2018 = case_when(between(Year, 2011, 2016)~ 2018, TRUE ~ as.numeric(NA)),
         IR2020 = case_when(between(Year, 2013, 2018)~ 2020, TRUE ~ as.numeric(NA)),
         IR2022 = case_when(between(Year, 2015, 2020)~ 2022, TRUE ~ as.numeric(NA)),
         # new IR here
         IR2024 = case_when(between(Year, 2017, 2022) ~ 2024, TRUE ~ as.numeric(NA)), 
         Panel = case_when(between(Year, 2001, 2011)~ 'Phase1',
                           between(Year, 2012, 2022)~ 'Phase2',
                           TRUE ~ as.character(NA)),
         BioPanel = case_when(between(Year, 2001, 2005)~ 'Phase1',
                              between(Year, 2006, 2010)~ 'Phase2',
                              between(Year, 2011, 2015)~ 'Phase3',
                              between(Year, 2016, 2020)~ 'Phase4',
                              between(Year, 2021, 2022)~ 'Phase5',
                              TRUE ~ as.character(NA)),
         BayPanel = paste0(BayShed, Panel),
         StreamSizeCat = case_when(BasinSize %in% c(1,2) ~ 'Small',
                                   BasinSize == 3 ~ "Medium",
                                   BasinSize == 4 ~ "Large",
                                   TRUE ~ as.character(NA)),
         StreamSizeCatPhase = paste0(Panel,StreamSizeCat ),
         BasinSize = case_when(AREA_SQ_MILES < 1 ~ 1,
                               between(AREA_SQ_MILES, 1, 9.99999999) ~ 2, #between is >= and <= 
                               between(AREA_SQ_MILES, 10, 49.9999999) ~ 3, #between is >= and <= 
                               AREA_SQ_MILES >= 50 ~ 4,
                               TRUE ~ as.numeric(NA))) %>%
  dplyr::select(-c(AREA_SQ_MILES)) %>% 
  dplyr::select(station, sampleID, Station_Id, everything()) %>% 
  rename(`strahler order` = 'strahler.order',
         `Latitude-DD` = 'Latitude.DD',
         `Longitude-DD` = 'Longitude.DD') %>% 
  #fix known issue
  mutate(designweight = case_when(sampleID %in% c('1AXMJ000.42_2012', '1AXMJ000.42_2014', '1AXMJ000.42_2016')~ 947.62919999999997,
                                  TRUE ~ as.numeric(designweight))) %>% 
  mutate(sampleID = case_when(sampleID == '2-JKS070.97' ~ '2-JKS063.03',
                              TRUE ~ as.character(sampleID)))




designStatusNew <- read_excel('C:/Users/wmu43954/OneDrive - Commonwealth of Virginia/Freshwater ProbMon/IR2024/Wadeable_ProbMon2021-2022.xlsx',
                                  #'originalData/Wadeable_ProbMon2021-2022.xlsx', # or from here for local archiving
                                  sheet = 'withMetadata') %>% 
  
  # if no siteID, assign one
  mutate(StationID = coalesce(StationID, siteID)) %>% 
  
  dplyr::select(Year, StationID:set, Station_Id = siteID) %>% 
  
  #will use CEDS lat/lngs just in case people messed with coordinates
  left_join(dplyr::select(prob2024fin, StationID, Year, #`strahler order` = Order, 
                          `Longitude-DD` =  LongitudeDD, `Latitude-DD` = LatitudeDD, 
                          Basin:IR2024, Order, weight.category = weightcategory) %>% 
              mutate(`strahler order` = Order),
            by = c('StationID', 'Year')) %>%
  
  mutate(sampleID = coalesce(StationID_Trend, StationID), # use StationID_Trend where available
         `Latitude-DD` = coalesce(`Latitude-DD`, Latitude),
         `Longitude-DD` = coalesce(`Longitude-DD`, Longitude),
         station = as.numeric(NA)) %>% # placeholder for now
  dplyr::select(names(designStatusOld))



# smash together in this order so the station count is correct
designStatusFin <- bind_rows(filter(designStatusOld, status == 'TS') %>% arrange(Year),
                             filter(designStatusNew, status == 'TS') %>% arrange(Year),
                             filter(designStatusOld, status != 'TS') %>% arrange(Year),
                             filter(designStatusNew, status != 'TS') %>% arrange(Year)) %>% 
  mutate(station = 1:n()) %>% 
  dplyr::select(names(designStatusOld), IR2024)

#write.csv(designStatusFin, 'processedData/designStatusIR2024.csv', row.names = F, na = '')

```


