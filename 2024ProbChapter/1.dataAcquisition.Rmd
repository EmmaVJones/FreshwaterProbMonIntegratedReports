---
title: "Data Acquisition"
author: "Emma Jones"
date: "2/6/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(config)
library(sf)
library(lubridate)
library(pool)
library(pins)
library(sqldf)
library(dbplyr)
library(readxl)

```



## Background

This script connects to the ODS environment, pulls data for assessment window, and summarizes results. Historically, field and analyte information has been reduced to one record per sample year, even if the station was sampled more than once (e.g. spring and fall). The summary statistic used is the median for this report.

## Connect to ODS

Make sure pulling from production environment and not test since those are not exact copies.

```{r connect to ODS}

# Server connection things
conn <- config::get("connectionSettings") # get configuration settings


board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

## Connect to ODS production
pool <- dbPool(
 drv = odbc::odbc(),
 Driver = "ODBC Driver 11 for SQL Server",#"SQL Server Native Client 11.0",
 Server= "DEQ-SQLODS-PROD,50000",
 dbname = "ODS",
 trusted_connection = "yes"
)
```

## Bring in final data format to match

Below is the final dataset for the 2022 IR that we will aim to match for 2024 IR.

This version (_EVJ) was used because it includes the transition of the NRO site naming issue 1AQUA004.14 to 1AQUA004.35. The original data published online for IR2022 is still available in 'originalData/originalOriginal/Wadeable_ProbMon_2001-2020.csv' but this version represents the best data to move forward with for IR2024. Since Lucy is awesome, she combined these data in previous reports and kept the ProbMon name 1AQUA004.14 to represent the station. Emma updated that station in the _EVJ spreadsheet on 2/6/2023 so when carried forward, this station will me known as 1AQUA004.35.

```{r prob 2022}
prob2022 <- read_csv('originalData/Wadeable_ProbMon_2001-2020_EVJ.csv') 
```

Generally speaking, we need to hit a few data sources to acquire all the data we need to publish for final chapter:
 - Field Data (Wqm_Field_Data_View) for DO, pH, SpCond
 - Analyte Data (Wqm_Analyte_Data_View) for TN, TP, TDS, NH4, NO3, TKN, Ortho-P, Turb, TSS, Na, K, Cl, Sf, 70331VFine, SSCCOARSE, SSCFINE, SSCTOTAL
 - PHAB database (still in Access) for LRBS, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, BL_CB_GR_Embed_PCT, Embed_PCT
 - EDAS (pinned (pre calculated) habitat data on server) for TotHab
 - EDAS (pinned (pre calculated) benthic data on server) for TotTaxa, EPTTax, VEphem, VPTHydropsychidae, VScrap, VChiro, V2Dom, HBI, VHapto, EPTInd, VSCIVCPMI (calculated conversion)
 - Analyte Data (Wqm_Analyte_Data_View) for dissolved metals MetalCCU (calculated), ARSENICppm, BERYLLIUMppm, CADMIUMppm, CHROMIUMppm, COPPERppm, LEADppm, MANGppm, NICKELppm, SILVERppm, ZINCppm, ANTIMONYppm, ALUMINUMppm, SELENIUMppm, IRONppm, MERCURYppm, THALLIUMppm, CALCIUM, MAGNESIUM, ARSENIC, BARIUM, BERYLLIUM, CADMIUM, CHROMIUM, COPPER, IRON, LEAD, MANGANESE, THALLIUM, NICKEL, SILVER, ZINC, ANTIMONY, ALUMINUM, SELENIUM, HARDNESS, MERCURY, Hg-C
 - Special GIS dataset for watershed spatial info 


## Bring in sites

Now we need to bring in the two years worth of new sites to pull the data

```{r new sites}
WQM_Stations_Filter <- read_excel('C:/Users/wmu43954/OneDrive - Commonwealth of Virginia/Freshwater ProbMon/IR2024/Wadeable_ProbMon2021-2022.xlsx',
                                  #'originalData/Wadeable_ProbMon2021-2022.xlsx', # or from here for local archiving
                                  sheet = 'Sheet1') %>% 
  filter(status == 'TS') # keep only wadeable sites that were sampled for data querying
```




## Query data

Using the current build of the conventionals function to query and organize field, analyte, and metals data consistently. Field and analyte methods pulled from CEDS WQM data query tool.

### Query Terms

These presets allow correct data retrieval. 

```{r query terms}

# Basic station info for conventionals
multiStationInfo <- pool %>% tbl(in_schema("wqm",  "Wqm_Stations_View")) %>%
  filter(Sta_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()
multiStationGIS_View <-  pool %>% tbl(in_schema("wqm",  "Wqm_Sta_GIS_View")) %>%
  filter(Station_Id %in% !! toupper(WQM_Stations_Filter$StationID)) %>%
  as_tibble()

# make sure all stations are in CEDS
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationInfo$Sta_Id]
WQM_Stations_Filter$StationID[! WQM_Stations_Filter$StationID %in% multiStationGIS_View$Station_Id]
```




### Field data

```{r field data}
#dateRange_multistation <- c(as.Date('2020-01-01'), as.Date('2020-12-31'))# c(as.Date('2019-02-05'), as.Date('2019-02-08'))# 
x2021Sites <- filter(WQM_Stations_Filter, Year == 2021)$StationID
x2022Sites <- filter(WQM_Stations_Filter, Year == 2022)$StationID

# do as two steps to make sure only bring back data from desired window for each site
multistationFieldData <- bind_rows(
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2021Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2021-01-01'), as.Date('2021-12-31'))) %>% # & # x >= left & x <= right
    as_tibble(),
  pool %>% tbl(in_schema("wqm", "Wqm_Field_Data_View")) %>%
     filter(Fdt_Sta_Id %in% !! x2022Sites &
           between(as.Date(Fdt_Date_Time), as.Date('2022-01-01'), as.Date('2022-12-31'))) %>% # & # x >= left & x <= right
    as_tibble()) %>% 
  filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")

  # filter(Fdt_Sta_Id %in% !! WQM_Stations_Filter$StationID &
  #          between(as.Date(Fdt_Date_Time), !! dateRange_multistation[1], !! dateRange_multistation[2])) %>% # & # x >= left & x <= right
           #Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE") %>%  # don't drop QA failure on SQL part bc also drops any is.na(Ssc_Description)
  # as_tibble() %>% 
  # filter(Ssc_Description != "INVALID DATA SET QUALITY ASSURANCE FAILURE")
```

### Analyte Data

```{r analyte data}
# can do this in one step since we are searching for Fdt_Id's from appropriate field data windows
multistationAnalyteData <- pool %>% tbl(in_schema("wqm", "Wqm_Analytes_View")) %>%
  filter(Ana_Sam_Fdt_Id %in% !! multistationFieldData$Fdt_Id &
           #between(as.Date(Ana_Received_Date), !! dateRange_multistation[1], !! dateRange_multistation[2]) & # x >= left & x <= right
           Pg_Parm_Name != "STORET STORAGE TRANSACTION DATE YR/MO/DAY") %>% 
  as_tibble() %>%
  left_join(dplyr::select(multistationFieldData, Fdt_Id, Fdt_Sta_Id, Fdt_Date_Time), by = c("Ana_Sam_Fdt_Id" = "Fdt_Id"))

analyteNames <- multistationAnalyteData %>% 
  dplyr::select(Me_Meth_Short_Name, Pg_Storet_Code) %>% 
  distinct(Pg_Storet_Code, .keep_all = T)
```




### Organize by Conventionals logic

To consistently organize field, analyte, and metals data it is prudent to use the "conventionals" data rules.

```{r conventionals organization of raw data}
source('C:/HardDriveBackup/R/GitHub/WQMdataQueryTool/conventionalsFunction10212022.R')

conventionalsList <- conventionalsSummary(conventionals= pin_get("conventionals2022IRfinalWithSecchi", board = "rsconnect")[0,],
                                          stationFieldDataUserFilter= multistationFieldData, 
                                          stationAnalyteDataUserFilter = multistationAnalyteData, 
                                          stationInfo = multiStationInfo,
                                          stationGIS_View = multiStationGIS_View,
                                          dropCodes = c('QF'),
                                          assessmentUse = F,
                                          overwriteUncensoredZeros = TRUE) 
conventionals <- conventionalsList$More %>% 
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)

#write.csv(conventionals, 'processedData/2024conventionals.csv', row.names = F, na = "")
```

### QA data

In previous cycles, we had a Logi-proficient staff member pull the pre-R queries for the same data using Logi and then ran analyses to double check both methods resulted in the same answers. For IR2024, we will subsample 5% of all CEDS field and analyte data points and manually review to ensure the data are correct. 

```{r QA data}
conventionalsLong <- conventionals %>% 
  dplyr::select(-c(STA_DESC:Data_Source, FDT_DEPTH_DESC:FDT_COMMENT, Ana_Sam_Mrs_Container_Id_Desc, ENTEROCOCCI )) %>% 
  dplyr::select(! contains(c('LEVEL_', 'RMK'))) %>% # drop all level columns since all DEQ data
  pivot_longer(-c('FDT_STA_ID', 'FDT_DATE_TIME', 'FDT_DEPTH'), 
               names_to = 'Parameter', values_to = 'Value',  values_drop_na = TRUE)  # drop NA values so QAing 10% of real data

# how many rows do we need to QA?
nrow(conventionalsLong) * .05
  
conventionalsLongQA <- conventionalsLong %>% 
  sample_n(331, replace = FALSE) %>%  #randomly selects 335 rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)

#331 is still a lot, subsample again to 5% and see if we need to dig in more
conventionalsLongQAminiEmma <- conventionalsLongQA %>% 
  sample_n(17, replace = FALSE) %>%  #randomly selects 5% of rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)
# write.csv(conventionalsLongQAminiEmma, 'processedData/QA/conventionalsLongQAminiEmma.csv')
# This dataset checked out 100%


conventionalsLongQAminiJason <- conventionalsLongQA %>% 
  sample_n(17, replace = FALSE) %>%  #randomly selects 5% of rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)
# write.csv(conventionalsLongQAminiJason, 'processedData/QA/conventionalsLongQAminiJason.csv')


conventionalsLongQAminiScott <- conventionalsLongQA %>% 
  sample_n(17, replace = FALSE) %>%  #randomly selects 5% of rows for QA
  arrange(FDT_STA_ID, FDT_DATE_TIME, FDT_DEPTH)
# write.csv(conventionalsLongQAminiScott, 'processedData/QA/conventionalsLongQAminiScott.csv')
rm(conventionalsLongQA, conventionalsLongQAmini, conventionalsLongQAminiEmma, conventionalsLongQAminiJason, conventionalsLongQAminiScott) # clean up workspace
```



#### QA Equipment Blank Data


Now quick QA test for EB information. What we want is to make sure that none of the blanks have too high values, indicating too much uncertainty with respect to units and potentially exceeding standards.

```{r QA conventionals EB}
QAconventionals <- conventionals %>% 
  filter(Ana_Sam_Mrs_Container_Id_Desc == 'EB') %>% 
  dplyr::select(FDT_STA_ID, FDT_DATE_TIME,STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, NITROGEN_mg_L:RMK_82079) %>% 
  dplyr::select(!contains('LEVEL_'))

# No SWRO QA in 2021-2022?

# make a lookuptable quick
allNames = c(names(QAconventionals %>% dplyr::select(-c(FDT_STA_ID:Ana_Sam_Mrs_Container_Id_Desc))))
parameters = allNames[seq(1, length(allNames), 2)]  
remarks = allNames[! allNames %in% parameters]  

lookupTable <- tibble(Parameter = parameters,
       RemarkName = remarks) %>% 
  mutate(link = 1:n())

# two steps bc can't combine character and numeric entries in a pivot longer
QAconventionals <- left_join(
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE,FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc, ! contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols = -c(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'Parameter', values_to = 'Measure', values_drop_na = T) %>% 
    left_join(dplyr::select(lookupTable, -RemarkName), by = 'Parameter'),
  QAconventionals %>% 
    dplyr::select(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc,  contains('RMK')) %>% 
    #group_by(FDT_STA_ID, FDT_DATE_TIME, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc) %>% 
    pivot_longer(cols = -c(FDT_STA_ID, FDT_DATE_TIME, STA_REC_CODE, FDT_SPG_CODE, Ana_Sam_Mrs_Container_Id_Desc),
                 names_to = 'RemarkName', values_to = 'Remark', values_drop_na = T) %>% 
    left_join(dplyr::select(lookupTable, -Parameter), by = 'RemarkName'),
  by = c("FDT_STA_ID", "FDT_DATE_TIME", "STA_REC_CODE", "FDT_SPG_CODE", "Ana_Sam_Mrs_Container_Id_Desc", "link")) %>% 
  dplyr::select(-link)
  

#write.csv(QAconventionals, 'test.csv', row.names = F, na="")
```


T	= Value reported is less than the criteria of detection
QQ =	Analyte detected above the MDL but below the method quantification limit.

Since all equipment blanks have those two flags, our sample data are deemed within spec and we are good to proceed.


For reporting purposes, we don't want to use the raw data (even though Roger has confidence in those numbers). We want to back lab reported values back to 0.001 if they fall below that. This is the second report we are pulling uncensored data. We need to repull and organize all years of data with uncensored and censored and rerun statistics for both (some programs e.g. permitting) may prefer censored data but uncensored could offer smoother CDF curves (remove blocky detection limits that have improved over last 20+ years).

Jason and I have determined that using uncensored this round is okay because we are only cleaning up the lower end of the dataset and are not coming close to potential standards violations.

```{r back to positive}
below0 <- function(x)(if(all(!is.na(x) & x < 0 & is.numeric(x))){return(0.001)}else{x})
#below0(conventionals$`CADMIUM, DISSOLVED (UG/L AS CD)`)

conventionals2 <- conventionals %>% 
  rowwise() %>% 
 # mutate_if(is.numeric, ~0.001 * (. < 0))
  mutate_at(vars(FDT_FIELD_PH:SSC_TOTAL), below0)

# double check things worked
summary(conventionals)
summary(conventionals2) # make sure columns with negative mins have gone to 0.001
```




Next we need to change the data for manual QA to the probmon format. We will also drop EB's at this point.


```{r prob data format}

probData <- conventionals2 %>%
  filter(Ana_Sam_Mrs_Container_Id_Desc != 'EB') %>% 
  mutate(StationID = FDT_STA_ID, 
         Year = year(FDT_DATE_TIME),
              DO = DO_mg_L, 
              pH = FDT_FIELD_PH,
              SpCond = FDT_SPECIFIC_CONDUCTANCE, 
              TN = NITROGEN_TOTAL_00600_mg_L,
              TP = PHOSPHORUS_TOTAL_00665_mg_L, 
              TDS = TDS_mg_L, #this is fine, just a recode of storet 70300
              NH4 = AMMONIA_mg_L, 
              NO3 = NITRATE_mg_L, 
              TKN = NITROGEN_KJELDAHL_TOTAL_00625_mg_L,
              `Ortho-P` = PHOSPHORUS_TOTAL_ORTHOPHOSPHATE_70507_mg_L,
              Turb = `TURBIDITY,LAB NEPHELOMETRIC TURBIDITY UNITS, NTU`,
              TSS = TSS_mg_L, 
              Na = `SODIUM, DISSOLVED (MG/L AS NA)`, 
              K = `POTASSIUM, DISSOLVED (MG/L AS K)`,
              Cl = CHLORIDE_DISSOLVED_00941_mg_L, #CHLORIDE_mg_L,
              Sf = SULFATE_DISS_mg_L, #SULFATE_mg_L,
              `70331VFine` = `SSC%Finer`,
              SSCCOARSE = SSC_COARSE,
              SSCFINE =  SSC_FINE, 
              SSCTOTAL = SSC_TOTAL,
              # sediment ppm data hasn't been collected since 2011
              ARSENICppm = as.numeric(NA),
              BERYLLIUMppm = as.numeric(NA),
              CADMIUMppm = as.numeric(NA),
              CHROMIUMppm = as.numeric(NA),
              COPPERppm = as.numeric(NA),
              LEADppm = as.numeric(NA),
              MANGppm = as.numeric(NA),
              NICKELppm = as.numeric(NA),
              SILVERppm = as.numeric(NA),
              ZINCppm = as.numeric(NA),
              ANTIMONYppm = as.numeric(NA),
              ALUMINUMppm = as.numeric(NA),
              SELENIUMppm = as.numeric(NA),
              IRONppm = as.numeric(NA),
              MERCURYppm = as.numeric(NA),
              THALLIUMppm = as.numeric(NA),
              CALCIUM = `CALCIUM, DISSOLVED (MG/L AS CA)`,
              MAGNESIUM = `MAGNESIUM, DISSOLVED (MG/L AS MG)`,
              ARSENIC = `ARSENIC, DISSOLVED  (UG/L AS AS)`,
              BARIUM = `BARIUM, DISSOLVED (UG/L AS BA)`,
              BERYLLIUM = `BERYLLIUM, DISSOLVED (UG/L AS BE)`,
              CADMIUM = `CADMIUM, DISSOLVED (UG/L AS CD)`,
              CHROMIUM = `CHROMIUM, DISSOLVED (UG/L AS CR)`,
              COPPER = `COPPER, DISSOLVED (UG/L AS CU)`,
              IRON = `IRON, DISSOLVED (UG/L AS FE)`,
              LEAD = `LEAD, DISSOLVED (UG/L AS PB)`,
              MANGANESE = `MANGANESE, DISSOLVED (UG/L AS MN)`,
              THALLIUM = `THALLIUM, DISSOLVED (UG/L AS TL)`,
              NICKEL = `NICKEL, DISSOLVED (UG/L AS NI)`,
              SILVER = `SILVER, DISSOLVED (UG/L AS AG)`,
              ZINC = `ZINC, DISSOLVED (UG/L AS ZN)`,
              ANTIMONY = `ANTIMONY, DISSOLVED (UG/L AS SB)`,
              ALUMINUM = `ALUMINUM, DISSOLVED (UG/L AS AL)`,
              SELENIUM = `SELENIUM, DISSOLVED (UG/L AS SE)`,
              HARDNESS = `HARDNESS, CA MG CALCULATED (MG/L AS CACO3) AS DISSOLVED`,
              MERCURY = `MERCURY-TL,FILTERED WATER,ULTRATRACE METHOD NG/L`,
              `Hg-C` = `RMK_50091`) %>% 
  dplyr::select(FDT_DATE_TIME, FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, Ana_Sam_Mrs_Container_Id_Desc, #other helpful info
                any_of(names(prob2022))) %>% 
  dplyr::select(StationID, FDT_DATE_TIME, Year, FDT_DATE_TIME, Year, FDT_DEPTH, FDT_SPG_CODE, FDT_COMMENT, 
                Ana_Sam_Mrs_Container_Id_Desc, everything()) %>% 
  arrange(Year, StationID, FDT_DATE_TIME)

#clean up workspace
rm(multistationAnalyteData);rm(multistationFieldData);rm(multiStationGIS_View);rm(multiStationInfo)
rm(conventionalsList);rm(conventionals);rm(conventionals2)
```


At this point we know there are still too many samples at certain stations that get repeat visits for other sample program codes. We will correct for this in future steps once we have the benthic data to choose the chem results that occurred closest to the bug data.


### Metals CCU

This is a good point to run the Metals CCU analysis before QA and yearly consolidation.

MetalsCCU calculation function from BSA tool. This version is the correct version with Larry's bonus metals commented out.

```{r metalsCCUfunction}

# Metals CCU Calculation, with Larry Bonus metals commented out
metalsCCUcalc <- function(Hardness,Aluminum,Arsenic,Cadmium,Chromium,Copper,Lead,Nickel,Selenium,Zinc){
  criteriaHardness <- ifelse(Hardness<25,25,ifelse(Hardness>400,400,Hardness))
  #AluminumEPAChronic <- Aluminum/150 # Larry Bonus Metal
  ArsenicChronic <- Arsenic/150
  #CadmiumChronic <- Cadmium/(exp(0.7852*(log(criteriaHardness))-3.49)) # Larry Bonus Metal
  ChromiumChronic <- Chromium/((exp(0.819*(log(criteriaHardness))+0.6848))*0.86)
  CopperChronic <- Copper/((exp(0.8545*(log(criteriaHardness))-1.702))*0.96)
  LeadChronic <- Lead/((exp(1.273*(log(criteriaHardness))-3.259))*(1.46203-(log(criteriaHardness)*(0.145712))))
  NickelChronic <- Nickel/((exp(0.846*(log(criteriaHardness))-0.884))*0.997)
  #SeleniumChronic <- Selenium/5 # Larry Bonus Metal
  ZincChronic <- Zinc/((exp(0.8473*(log(criteriaHardness))+0.884))*0.986)
  #return(sum(AluminumEPAChronic,ArsenicChronic,CadmiumChronic,ChromiumChronic,CopperChronic,LeadChronic,
  #           NickelChronic,SeleniumChronic,ZincChronic))
  return(sum(ArsenicChronic,ChromiumChronic,CopperChronic,LeadChronic,
             NickelChronic,ZincChronic))
}


```

Just get necessary data to run function.

```{r metalsCCUcalculation}
metalsCCUresults <- select(probData,StationID, Year,  FDT_DATE_TIME, Ana_Sam_Mrs_Container_Id_Desc,
                           HARDNESS, ALUMINUM, ARSENIC, CADMIUM, 
                           CHROMIUM, COPPER, LEAD, NICKEL, SELENIUM, ZINC)

metalsCCUresults <- metalsCCUresults %>% 
  rowwise() %>% 
  mutate( MetalCCU = round(metalsCCUcalc(Hardness = HARDNESS, Aluminum = ALUMINUM, 
                               Arsenic = ARSENIC,Cadmium = CADMIUM,
                               Chromium = CHROMIUM,Copper = COPPER, 
                               Lead = LEAD,Nickel =NICKEL,
                               Selenium = SELENIUM,Zinc = ZINC), 
                            digits=4))  %>% 
  dplyr::select(StationID:Ana_Sam_Mrs_Container_Id_Desc, MetalCCU)


probData <- left_join(probData, metalsCCUresults,
                       by = c("StationID", "Year", "FDT_DATE_TIME", "Ana_Sam_Mrs_Container_Id_Desc"))

``` 




Last, we need to remove extra samples not associated with FP/bio sampling events but not drop too much in case other programs grabbed samples on a different date. This is still best done manually. For 2022IR, Lucy manually screened data for just spring/fall sample information (using excel).

To do this, we need benthic data.



### Benthic Data

Use pinned benthic data to pull appropriate SCI information for sites.

```{r get ecoregion for SCI}
ecoregion <- pin_get("ejones/WQM-Stations-Spatial", board = "rsconnect") %>% 
        filter(StationID %in% WQM_Stations_Filter$StationID) %>% 
  dplyr::select(StationID, US_L3CODE, Basin) %>% 
  distinct(StationID, .keep_all = T)
```

Get all SCI information from pinned data. 

```{r SCI pins}
VSCIresults <- pin_get("ejones/VSCIresults", board = "rsconnect")
VCPMI63results <- pin_get("ejones/VCPMI63results", board = "rsconnect")
VCPMI65results <- pin_get("ejones/VCPMI65results", board = "rsconnect")
```

Bring in correct BenSampID info for sites.

```{r}
x2021Sites <- filter(WQM_Stations_Filter, Year == 2021)$StationID
x2022Sites <- filter(WQM_Stations_Filter, Year == 2022)$StationID

bioData2021 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2021Sites & year(`Collection Date`) == 2021) %>% 
  filter(str_detect(BenSampID, 'R110')) %>% # only keep rarified data
  filter(RepNum == 1)  # only use Rep1
bioData2022 <- pin_get("ejones/benSamps", board = "rsconnect") %>% 
  filter(StationID %in% x2022Sites & year(`Collection Date`) == 2022) %>% 
  filter(str_detect(BenSampID, 'R110')) %>% # only keep rarified data
  filter(RepNum == 1) # only use Rep1

# make sure no sites are missing biological data (our key to using a site)
x2021Sites[! x2021Sites %in% bioData2021$StationID]
x2022Sites[! x2022Sites %in% bioData2022$StationID]

benSamps_Filter_fin <- bind_rows(bioData2021, bioData2022) %>% 
  left_join(ecoregion, by = 'StationID')
```

Get correct SCI information. Make sure to grab the VSCI metrics for all stations not run with VSCI and have that ready for final data smashing. We report the VSCI metrics for VCPMI sites for continuity.

```{r}
# bring in bioassessment decisions
IR2024bioassessment <- pin_get("ejones/CurrentIRbioassessmentDecisions", board = "rsconnect") %>% 
  filter(IRYear == 2024 & StationID %in% benSamps_Filter_fin$StationID)

benSamps_Filter_finDecision <- left_join(benSamps_Filter_fin, dplyr::select(IR2024bioassessment, StationID, AssessmentMethod)) %>% 
  mutate(AssessmentMethod = case_when(StationID = '2-MTC006.95' ~ 'VSCI', # pin_get("ejones/PreviousIRbioassessmentDecisions", board = "rsconnect") all other 2-MTC stations use VSCI whne Mike assessed
                                      is.na(AssessmentMethod) & 
                                      ))

IR20bioassessment <- pin_get("ejones/PreviousIRbioassessmentDecisions", board = "rsconnect") %>% 
  filter(IRYear == 2024 & StationID %in% benSamps_Filter_fin$StationID)
```



**This is also where we do any SCI overrides. Maybe we can use the bioassessment information from these sites in the future pin_get("ejones/CurrentIRbioassessmentDecisions", board = "rsconnect")**

```{r}
SCI_filter <- filter(VSCIresults, BenSampID %in% filter(benSamps_Filter_fin, ! US_L3CODE %in% c(63,65))$BenSampID) %>%
  bind_rows(
    filter(VCPMI63results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(63) | str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  bind_rows(
    filter(VCPMI65results, BenSampID %in% filter(benSamps_Filter_fin,  US_L3CODE %in% c(65) & !str_detect(Basin, "Chowan"))$BenSampID)  ) %>%
  mutate(SeasonGradient = as.factor(paste0(Season, " (",Gradient,")"))) %>%
  left_join(dplyr::select(benSamps_Filter_fin, StationID, BenSampID, RepNum, `Collection Date`) ,
              by = c('StationID', 'BenSampID', 'RepNum', 'Collection Date')) %>%

  # make year variable to make review easier
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, Season, BenSampID, RepNum, SCI, `SCI Score`, `SCI Threshold`,
                Gradient, `Target Count`,  `Sample Comments`, `Collected By`:`Entered Date`, everything()) %>% 
  arrange(Year, StationID, `Collection Date`)


# after speaking with PRO and NRO regional bios, the following SCI overrides are suggested
SCI_filter <- filter(SCI_filter, ! StationID %in% c("1AXOR000.47", '8-POR015.70')) %>% 
  bind_rows(
    bind_rows(
      filter(VSCIresults, BenSampID %in% filter(benSamps_Filter_fin, StationID == "1AXOR000.47")$BenSampID),
      filter(VCPMI65results, BenSampID %in% filter(benSamps_Filter_fin, StationID == '8-POR015.70')$BenSampID)) %>%
      mutate(SeasonGradient = as.factor(paste0(Season, " (",Gradient,")"))) %>%
      left_join(dplyr::select(benSamps_Filter_fin, StationID, BenSampID, RepNum, `Collection Date`) ,
                by = c('StationID', 'BenSampID', 'RepNum', 'Collection Date')) %>%
      
      # make year variable to make review easier
      mutate(Year = year(`Collection Date`)) %>% 
      dplyr::select(StationID, Year, `Collection Date`, Season, BenSampID, RepNum, SCI, `SCI Score`, `SCI Threshold`,
                    Gradient, `Target Count`,  `Sample Comments`, `Collected By`:`Entered Date`, everything()) %>% 
      arrange(Year, StationID, `Collection Date`)
  ) %>% 
  dplyr::select(Year, StationID, `Collection Date`, BenSampID, SCI, `SCI Score`) %>% # only actually need these metrics
  arrange(Year, StationID, `Collection Date`) 


# We report VSCI metrics regardless of SCI used for continuity 
SCImetrics <- filter(VSCIresults, BenSampID %in% SCI_filter$BenSampID) %>% 
  dplyr::select(BenSampID:`Fam %MFBI Score`)

SCIfinal <- left_join(SCI_filter, SCImetrics, by = c('BenSampID'))
```


While we are pulling data, NRO has requested we ensure the Po River trend site is only using VCPMI65. We will repull all the data for this site, average it, and smash it in to the final dataset later, overwriting previous benthic information.

```{r po river fix}

po <- filter(VCPMI65results, StationID == '8-POR015.70'& `Target Count` == 110) %>% 
  mutate(Year = year(`Collection Date`)) %>% 
  filter(Year %in% c(2004,2011,2013,2015,2017))#,2019))# just the samples associated with prob
# but we only report the VSCI metrics so grab those for these bensamps
poMetrics <- filter(VSCIresults, BenSampID %in% po$BenSampID)


# clean up workspace
rm(VSCIresults);rm(VCPMI63results);rm(VCPMI65results); rm(bioData2019);rm(bioData2020); rm(benSamps_Filter_fin)
rm(SCI_filter)
```


Now we can save both of these objects out and manually delete the erroneous rows in probData (things not close to benthic collections). We will average the station data in R after extra data is dropped.

```{r save for manual review}
#write.csv(probData, 'processedData/prob20192020dataFin.csv', row.names = F, na = '')
#write.csv(SCIfinal, 'processedData/bio20192020data.csv', row.names = F, na = '')
```

### Habitat Data

Next we need to get the total habitat scores for each benthic sample.

```{r tot hab}
habSamps <- bind_rows(pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% x2019Sites & year(`Collection Date`) == 2019) ,
                      pin_get('ejones/habSamps', board = 'rsconnect') %>% 
                        filter(StationID %in% x2020Sites & year(`Collection Date`) == 2020)) 
# make sure this is close to nrow SCI_filter
habValues <- pin_get('ejones/habValues', board = 'rsconnect') %>% 
  filter(HabSampID %in% habSamps$HabSampID) %>% 
  group_by(HabSampID) %>% 
  summarise(TotHab = sum(HabValue))

totHab <- left_join(habSamps, habValues, by = "HabSampID") %>% 
  mutate(Year = year(`Collection Date`)) %>% 
  dplyr::select(StationID, Year, `Collection Date`, TotHab)
# we will do the join to benthic data and average by year farther down the script

#write.csv(totHab, 'processedData/totHab.csv', row.names = F, na = '')

# clean up workspace
rm(habSamps); rm(habValues)
```

### LRBS data

This analysis is run using C:\HardDriveBackup\R\GitHub\PhysicalHabitat\PHAB2021\Dec2021run.R and the TMDLsummary_2021-12-09.csv was copy/pasted into this project's processed Data directory.

Data is summarized and QAed below.

```{r lrbs qa}
names(prob2020) # get the variables we need to match

lrbs <- read.csv('processedData/MasterSummary_2021-12-09.csv') %>% #TMDLsummary_2021-12-09.csv') %>% 
  dplyr::select(StationID, Date, LRBS = LRBS2, Slope, FN_PCT, SA_PCT, SA_FN_PCT, LSUB_DMM, 
                BL_CB_GR_Embed_PCT = BL_CB_GRmeanEmbed,
                Embed_PCT = Xembed ) %>% 
  mutate(Year = year(Date)) %>% 
  group_by(StationID, Year) %>% 
  summarise_at(vars(c(LRBS:Embed_PCT)), mean, na.rm = TRUE) 

# see what stations are missing data from this latest two years
View(left_join(WQM_Stations_Filter, lrbs, by = c('StationID', 'Year')))
# cool only the expected 2019 sites don't have lrbs

#write.csv(lrbs, 'processedData/FinalLRBS.csv', row.names = F, na = '')
```


### Geospatial data 

Watershed-specific landcover information is provided for all wadeable probmon sites each IR publication. In order to expedite the process of organizing and analyzing this geospatial dataset, automated scripts have been developed. This project calls these scripts. 

#### Tiger roads organization

Each year a new tiger road file is required to best compare sample year to roads in the watershed. Follow instructions in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsWebScrapingScript.R in order to scrape the FTP and organize each year into a single shapefile for analyses.

*** Remember: you need to first create a new directory named the year you are scraping (e.g. 2019) in the local C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\tigerRoadsPull directory for downloadTigerRoadsByYear() and in unzipped directory to work properly.

#### Delineate new watersheds

The C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\Organize20192020prob.Rmd walks users through delineating wadeable and boatable sites sampled in 2019-2020 using automated streamstats delineation methods. Manual review is conducted in either R or ArcGIS. 

#### Landcover analysis

Once all the necessary stations (wadeable) have QAed watershed information, landcover analysis scripts in C:\HardDriveBackup\R\GitHub\LandcoverAnalysis\ProbMonOrganization\landcoverAnalysis_20192020.R take users through the automated process of generating the ~100 landcover metrics the prob chapter reports out for wadeable dataset.



## QA data

